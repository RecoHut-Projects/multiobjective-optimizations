
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Exploring Multi-Objective Hyperparameter Optimization &#8212; multiobjective-optimizations</title>
    
  <link href="_static/css/theme.css" rel="stylesheet" />
  <link href="_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="_static/js/index.1c5a1a01449ed65a7b51.js">

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/togglebutton.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script>
        const thebe_selector = ".thebe,.cell"
        const thebe_selector_input = "pre,.cell_input div.highlight"
        const thebe_selector_output = ".output,.cell_output"
    </script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Implicit Hybrid Movie Recommender using Collie Library" href="T660394_Implicit_Hybrid_Movie_Recommender_using_Collie_Library.html" />
    <link rel="prev" title="Multi-objective Optimization using Pymoo Library" href="T145475_Multi_objective_Optimization_using_Pymoo_Library.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="index.html">
      
      
      
      <h1 class="site-logo" id="site-title">multiobjective-optimizations</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="L069335_Multi_Objective_Optimization.html">
   Multi-Objective Optimization
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Concepts
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="L492548_Efficient_Frontier.html">
   Efficient Frontier
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="L839342_Pareto_Optimality.html">
   Pareto Optimality
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="L483593_MOO_Decision_Maker.html">
   MOO Decision Maker
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="L336869_Multi_Task_Learning.html">
   Multi-Task Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="L553498_MOO_in_Recommender_Systems.html">
   MOO in Recommender Systems
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="L480379_Marketplace_Recommenders.html">
   Marketplace Recommenders
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="L556513_Multi_Objective_Hyperparameter_Optimization.html">
   Multi-Objective Hyperparameter Optimization
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Methods
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="L005003_Scalarization.html">
   Scalarization
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="L762719_Shared_Bottom.html">
   Shared Bottom
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="L915313_Expected_Hypervolume_Improvement_%28EHVI%29.html">
   Expected Hypervolume Improvement (EHVI)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="L587138_Multi_gradient_Descent_%28MGDRec%29.html">
   Multi-gradient Descent (MGDRec)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="L861465_Mixture_of_Experts_%28MoE%29.html">
   Mixture of Experts (MoE)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="L485744_Multi_gate_Mixture_of_Experts_%28MMoE%29.html">
   Multi-gate Mixture-of-Experts (MMoE)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="L519476_Entire_Space_Multi_Task_Model_%28ESSM%29.html">
   Entire Space Multi-Task Model (ESSM)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="L718205_Progressive_Layered_Extraction_%28PLE%29.html">
   Progressive Layered Extraction (PLE)
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Cases
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="L749932_MTL_for_Related_Products_Recommendations_at_Pinterest.html">
   MTL for Related Products Recommendations at Pinterest
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="L792752_UberEats.html">
   UberEats
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="L106645_Etsy.html">
   Etsy
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="L149938_Airbnb_Experiences.html">
   Airbnb Experiences
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Tutorials
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="T929652_Efficient_Frontier.html">
   Efficient Frontier
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T854417_Linear_Optimization_with_OR_Tools.html">
   Linear Optimization with OR-Tools
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T209908_Stein%27s_Paradox.html">
   Stein’s Paradox
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T423887_Solving_MOO_with_SMT_Toolkit.html">
   Solving MOO with SMT Toolkit
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T277640_Efficient_Continuous_Pareto_Exploration_in_MTL_on_UCI_Census.html">
   Efficient Continuous Pareto Exploration in MTL on UCI Census
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T851476_Multi_Task_Learning.html">
   Multi-Task Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T307666_Pareto_Efficient_algorithm_for_MOO.html">
   Pareto-Efficient algorithm for MOO
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T465017_Shared_Bottom_in_Tensorflow.html">
   Shared Bottom in Tensorflow
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T175823_MoE_in_Tensorflow.html">
   MoE in Tensorflow
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T580632_MMoE_on_Census_income_data_in_Tensorflow.html">
   MMoE on Census income data in Tensorflow
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T948705_Sparsely_gated_Mixture_of_Experts.html">
   Sparsely-gated Mixture-of-Experts
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T458949_Multi_Objective_Ranking_using_Constrained_Optimization_in_GBTs.html">
   Multi-Objective Ranking using Constrained Optimization in GBTs
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T145475_Multi_objective_Optimization_using_Pymoo_Library.html">
   Multi-objective Optimization using Pymoo Library
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Exploring Multi-Objective Hyperparameter Optimization
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T660394_Implicit_Hybrid_Movie_Recommender_using_Collie_Library.html">
   Implicit Hybrid Movie Recommender using Collie Library
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T559084_Multi_task_Learning_on_ML_100k_in_TFRS.html">
   Multi-task Learning on ML-100k in TFRS
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T368830_Multi_Task_Recommender_on_Olist_dataset_using_TFRS_Library.html">
   Multi-Task Recommender on Olist dataset using TFRS Library
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T671443_MAMO_Framework.html">
   MAMO Framework
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="T952247_MKR.html">
   MKR
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="_sources/T892775_Exploring_Multi_Objective_Hyperparameter_Optimization.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/sparsh-ai/multiobjective-optimizations/main?urlpath=tree/docs/T892775_Exploring_Multi_Objective_Hyperparameter_Optimization.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        <a class="colab-button" href="https://colab.research.google.com/github/sparsh-ai/multiobjective-optimizations/blob/main/docs/T892775_Exploring_Multi_Objective_Hyperparameter_Optimization.ipynb"><button type="button" class="btn btn-secondary topbarbtn"
                title="Launch Colab" data-toggle="tooltip" data-placement="left"><img class="colab-button-logo"
                    src="_static/images/logo_colab.png"
                    alt="Interact on Colab">Colab</button></a>
        
        <button type="button" class="btn btn-secondary topbarbtn"
            onclick="initThebeSBT()" title="Launch Thebe" data-toggle="tooltip" data-placement="left"><i
                class="fas fa-play"></i><span style="margin-left: .4em;">Live Code</span></button>
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#setup">
   Setup
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#multi-objective-hyperparameter-optimization-with-ax">
   Multi-objective hyperparameter optimization with Ax
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#data">
     Data
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#optimization-objectives">
     Optimization objectives
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#search-space">
     Search space
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id1">
     Optimization objectives
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#experiment-setup">
     Experiment setup
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#mobo-experiment-with-ehvi">
     MOBO Experiment (with EHVI)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#random-experiment-with-sobol">
     Random Experiment (with Sobol)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#visualize-results">
     Visualize Results
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#multi-objective-hyperparameter-optimization-with-optuna">
   Multi-objective hyperparameter optimization with Optuna
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id2">
     Data
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id3">
     Optimization objectives
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id4">
     Experiment setup
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id5">
     Visualize Results
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#multi-objective-hyperparameter-optimization-for-credit-card-fraud">
   Multi-objective hyperparameter optimization for credit card fraud
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#multi-objective-hyperparameter-optimization-for-credit-card-fraud-with-optuna">
   Multi-objective hyperparameter optimization for Credit Card Fraud with Optuna
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#a-tale-of-two-tradeoffs">
     A tale of two tradeoffs
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id6">
     Data
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#hyperparameter-optimization-with-optuna">
     Hyperparameter optimization with Optuna
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#training-routine">
     Training routine
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id7">
     Experiment setup
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id8">
     Visualize Results
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#conclusion-or-something">
     Conclusion or something
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#multi-objective-bayesian-optimization-for-approximate-nearest-neighbors-search-annoy-hnsw-lsh">
   Multi-objective Bayesian optimization for Approximate Nearest Neighbors search (Annoy, HNSW, LSH)
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#nearest-neighbor-search">
     Nearest Neighbor Search
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#ann-primer">
       ANN Primer
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#all-in-the-fannmily">
       All in the fANNmily
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#the-game-plann">
     The game plANN
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#approximate-nearest-neighbors-oh-yeah">
     Approximate Nearest Neighbors, Oh Yeah!
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#hyperparameters">
     Hyperparameters
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#code-it-up">
     Code it up
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#discussion-time">
     Discussion time
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#hierarchical-navigable-small-worlds">
     Hierarchical Navigable Small Worlds
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id9">
     Hyperparameters
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id10">
     Code it up
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id11">
     Discussion time
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#locality-sensitive-hashing">
     Locality-sensitive Hashing
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id12">
     Hyperparameters
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id13">
     Code it up
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id14">
     Discussion time
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#optimizing-anns-without-mobo">
     Optimizing ANNs without MOBO
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#the-big-take-away">
   The big take-away
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="exploring-multi-objective-hyperparameter-optimization">
<h1>Exploring Multi-Objective Hyperparameter Optimization<a class="headerlink" href="#exploring-multi-objective-hyperparameter-optimization" title="Permalink to this headline">¶</a></h1>
<p>To get to grips with Ax and see multi-output Bayesian optimization in action, in <code class="docutils literal notranslate"><span class="pre">section</span> <span class="pre">1</span></code>, we will tackle a totally synthetic problem <code class="docutils literal notranslate"><span class="pre">Ax</span> <span class="pre">on</span> <span class="pre">synthetic</span> <span class="pre">data</span></code>.</p>
<p>While we wanted to demonstrate how to use the Ax Developer API to do this, we’d actually recommend using Optuna for this kind of problem. This trades off the flexibility of Ax for a much smaller and simpler API. So in <code class="docutils literal notranslate"><span class="pre">section</span> <span class="pre">2</span></code>, we will do the same demo with Optuna.</p>
<p>Section 1 and 2 demonstrate how to use the libraries on a completely toy problem. What happens when we up the problem complexity a little and try a real machine learning problem? In <code class="docutils literal notranslate"><span class="pre">section</span> <span class="pre">3</span></code>, we attacked credit card fraud detection using a dataset released by the machine learning group at Université Libre de Bruxelles. Credit card fraud comes with some interesting facets, such as a highly imbalanced class distribution. The same group recently released a handbook about the problem, for those interested in diving deep.</p>
<p>We put quite some effort into prototyping with xgboost and multi-layer perceptron algorithms on this dataset, with both Ax and Optuna. While we could cherry-pick results that show EHVI outperforming random search in finding hyperparameter combinations resulting in good, high AUPRC, low-latency algorithms, we couldn’t reliably reproduce these results, and often random would win.</p>
<p>Our working hypothesis is that the problem is essentially too easy, by which we mean xgboost could find a good solution under a wide range of hyperparameters. If only one or two hyperparameters are important to a problem, we can get good coverage of those with random sampling, or even a parameter sweep (tantamount to grid search). While Bayesian optimization might sample slightly more densely from the good region, it doesn’t provide a reliable substantial improvement (and indeed, it appears to trade off some exploration for that). We expect Bayesian optimization to shine when more than one hyperparameter is important and good coverage of the hyperparameter space is not possible.</p>
<p>Our final <code class="docutils literal notranslate"><span class="pre">section</span> <span class="pre">4</span></code> dives deep into the world of approximate nearest neighbor (ANN) search. Approximate nearest neighbor search has all the elements we’re looking for: a real-world use case, a substantial and non-trivial dataset, and algorithms that have inherent tradeoffs between competing objectives like recall, inference time, and memory consumption. In this section we apply Optuna HPO to three different ANN algorithms on over one million GloVe word embeddings.</p>
<p>Again, we put a ton of effort into prototyping with these algorithms but the results weren’t impressive. In nearly all cases, random search performed about as well as Optuna’s MO HPO strategy. We surmise that the reason lies in the relationship of these algorithms’ hyperparameters with the output objective space. Many ANN algorithms have hyperparameters that are designed to exert influence on only one or two objectives, constraining the range of possible output realizations. For example, we may think we have a wide two-dimensional plane of (speed, accuracy) points to explore, like we’ve discussed above. But for ANNs, the reality is that the actual possible range of realizable values is quite narrow because ANN hyperparameters tightly control single objectives. When this happens, it becomes much easier for random search to perform about as well as a more sophisticated HPO strategy, because there’s simply less output objective space to explore.</p>
<div class="section" id="setup">
<h2>Setup<a class="headerlink" href="#setup" title="Permalink to this headline">¶</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">%%</span><span class="n">writefile</span> <span class="n">requirements</span><span class="o">.</span><span class="n">txt</span>
<span class="n">ax</span><span class="o">-</span><span class="n">platform</span><span class="o">==</span><span class="mf">0.1.20</span>
<span class="n">optuna</span><span class="o">==</span><span class="mf">2.8.0</span>
<span class="n">pandas</span><span class="o">==</span><span class="mf">1.2.4</span>
<span class="n">scikit</span><span class="o">-</span><span class="n">learn</span><span class="o">==</span><span class="mf">0.23.2</span>
<span class="n">matplotlib</span><span class="o">==</span><span class="mf">3.4.2</span>
<span class="n">seaborn</span><span class="o">==</span><span class="mf">0.11.1</span>
<span class="n">xgboost</span><span class="o">==</span><span class="mf">1.4.1</span>
<span class="n">h5py</span><span class="o">==</span><span class="mf">3.2.1</span>
<span class="n">pyyaml</span><span class="o">==</span><span class="mf">5.4.1</span> 
<span class="n">psutil</span><span class="o">==</span><span class="mf">5.6.6</span> 
<span class="n">jinja2</span><span class="o">==</span><span class="mf">3.0.0</span>
<span class="n">annoy</span><span class="o">==</span><span class="mf">1.17.0</span>
<span class="n">hnswlib</span><span class="o">==</span><span class="mf">0.5.1</span>
<span class="n">FALCONN</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Writing requirements.txt
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>!pip install -q -r requirements.txt
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="multi-objective-hyperparameter-optimization-with-ax">
<h2>Multi-objective hyperparameter optimization with Ax<a class="headerlink" href="#multi-objective-hyperparameter-optimization-with-ax" title="Permalink to this headline">¶</a></h2>
<p>This section demonstrates using Ax for multi-objective hyperparameter optimization synthetic binary classification task.</p>
<p>In multi-objective hyperparameter optimization, we seek to find the “Pareto frontier” of points representing optimal tradeoffs between multiple objectives. In this example, we trade off prediction latency with predictive performance (area under the ROC curve). We will compare a Bayesian optimization algorithm, maximizing the expected hypervolume increase (EHVI) with Gaussian Process models of the objective functions, against quasi-random Sobol search.</p>
<p>First, we need something to learn, so we’ll generate a small, toy classification problem. We’ll define the metrics we care about: the area under the ROC curve, and prediction latency. Then, we’ll use the expected hypervolume improvement (EHVI) to find the Pareto frontier of points that maximize these objectives.</p>
<p>This tutorial largely follows the official <a class="reference external" href="https://ax.dev/tutorials/multiobjective_optimization.html">Multi-Objective Optimization Ax API</a> tutorial, but whereas the originial optimizes a known mathematical function, we have made some special considerations for hyperparameter optimization of machine learning models.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">time</span>
<span class="kn">import</span> <span class="nn">warnings</span>
<span class="kn">from</span> <span class="nn">functools</span> <span class="kn">import</span> <span class="n">lru_cache</span>

<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="kn">from</span> <span class="nn">ax</span> <span class="kn">import</span> <span class="n">Data</span><span class="p">,</span> <span class="n">Models</span><span class="p">,</span> <span class="n">MultiObjective</span><span class="p">,</span> <span class="n">MultiObjectiveOptimizationConfig</span><span class="p">,</span> <span class="n">ObjectiveThreshold</span>
<span class="kn">from</span> <span class="nn">ax.core.experiment</span> <span class="kn">import</span> <span class="n">Experiment</span>
<span class="kn">from</span> <span class="nn">ax.core.parameter</span> <span class="kn">import</span> <span class="n">RangeParameter</span><span class="p">,</span> <span class="n">ParameterType</span>
<span class="kn">from</span> <span class="nn">ax.core.search_space</span> <span class="kn">import</span> <span class="n">SearchSpace</span>
<span class="kn">from</span> <span class="nn">ax.metrics.noisy_function</span> <span class="kn">import</span> <span class="n">NoisyFunctionMetric</span>
<span class="kn">from</span> <span class="nn">ax.modelbridge.factory</span> <span class="kn">import</span> <span class="n">get_MOO_EHVI</span>
<span class="kn">from</span> <span class="nn">ax.modelbridge.modelbridge_utils</span> <span class="kn">import</span> <span class="n">observed_hypervolume</span>
<span class="kn">from</span> <span class="nn">ax.plot.exp_utils</span> <span class="kn">import</span> <span class="n">exp_to_df</span>
<span class="kn">from</span> <span class="nn">ax.plot.pareto_frontier</span> <span class="kn">import</span> <span class="n">plot_pareto_frontier</span>
<span class="kn">from</span> <span class="nn">ax.plot.pareto_utils</span> <span class="kn">import</span> <span class="n">compute_pareto_frontier</span>
<span class="kn">from</span> <span class="nn">ax.runners.synthetic</span> <span class="kn">import</span> <span class="n">SyntheticRunner</span>
<span class="kn">from</span> <span class="nn">ax.utils.notebook.plotting</span> <span class="kn">import</span> <span class="n">render</span><span class="p">,</span> <span class="n">init_notebook_plotting</span>

<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_classification</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">roc_auc_score</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">cross_validate</span><span class="p">,</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.neural_network</span> <span class="kn">import</span> <span class="n">MLPClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.pipeline</span> <span class="kn">import</span> <span class="n">Pipeline</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>
</pre></div>
</div>
</div>
</div>
<p>There are some (safe!) warnings we anticipate, so we’ll turn off the noise.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s1">&#39;ignore&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="data">
<h3>Data<a class="headerlink" href="#data" title="Permalink to this headline">¶</a></h3>
<p>Let’s generate a classification problem. <code class="docutils literal notranslate"><span class="pre">sklearn</span></code> has a helper for that. The exact parameters aren’t too important! In this notebook, we really just want to explore how to use Ax for multi-objective hyperparameter optimization.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_classification</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">n_features</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span> <span class="n">n_informative</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>We’ll make an 80/20 split and use the larger portion for training, and the smaller portion for evaluation. If the algorithm we trained was going to be used, we’d want a third split to report our final metrics on. As is, we might risk overfitting the hyperparameters to the dev set across many trials. To avoid tripling (or more) the run time, we’ll skip cross-validation and simply train each algorithm on the train set, and evaluate on dev. We could get a more robust measure of the metrics associated with each hyperparameter configuration by cross-validating each.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_dev</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_dev</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">stratify</span><span class="o">=</span><span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="optimization-objectives">
<h3>Optimization objectives<a class="headerlink" href="#optimization-objectives" title="Permalink to this headline">¶</a></h3>
<p>Let’s say we care about two objectives: the area under the ROC curve (AUC), and latency. AUC captures the quality of a classifier in a threshold-independent way, and is appropriate for the balanced class distribution we have in our toy problem. By optimizing for this, we’re finding a good classifier without making a decision about how bad each flavour of misclassification (false positives and false negatives) are, whereas optimizing for a point metric like accuracy directly would be making that call. Latency measures how long a prediction takes. As it turns out, with the scikit-learn multi-layer perceptron model we’ll use, not very long. To help smooth out noise in the latency measurement, we’ll measure the time required to predict on a batch of a million points.</p>
<p>This will be a small model, and no GPU is used. If we were using a GPU-enabled framework (like PyTorch, TensorFlow, or Jax), we’d need to be a little more careful about how we measure latency. Also note that in most applications, we’d probably be more interested in the total response time of a model end point, or the total processing time for a large batch of offline predictions, and it’s not necessarily the case that the actual model prediction time is the dominant bottleneck to those things at all!</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">million_points</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="mi">1000</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">roc</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">roc_auc_score</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">latency</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
    <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
    <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="n">end</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">end</span> <span class="o">-</span> <span class="n">start</span>
</pre></div>
</div>
</div>
</div>
<p>We define a training routine, and compute the objectives. We also wrap the routine in a <code class="docutils literal notranslate"><span class="pre">lru_cache</span></code> decorator. Ax optimizes over black box functions, and each objective is evaluated seperately. This makes it very flexible (and applicable to many more optimization problems than hyperparameter optimization), but means that no special consideration is given for hyperparameter optimization, like how expensive it is to repeatedly train a model.</p>
<p>Since we are using the Developer API to Ax, we are afforded low level control over how the results of trials are handled. We could write some code in the Ax optimization loop to avoid requiring multiple evaluations. A simpler fix, since we’re running only locally, is to cache the function, so that the second and subsequent evaluations are near instant, and we avoid retraining the model. A good description of both options is given in an Ax GitHub issue addressing exactly this question: <a class="reference external" href="https://github.com/facebook/Ax/issues/584">Ax/issues/584</a>.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">train_wrapper</span></code> function is simply to enable caching - mutable objects cannot be cached, so we must unpack and repack some arguments.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nd">@lru_cache</span><span class="p">()</span> <span class="c1">#maxsize=maxsize</span>
<span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="o">**</span><span class="n">param_dict</span><span class="p">):</span>
    <span class="n">hidden_layer_sizes</span> <span class="o">=</span> <span class="p">[</span>
        <span class="nb">round</span><span class="p">(</span><span class="n">param_dict</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;layer_</span><span class="si">{</span><span class="n">layer</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">_size&quot;</span><span class="p">])</span>
        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">round</span><span class="p">(</span><span class="n">param_dict</span><span class="p">[</span><span class="s2">&quot;n_layers&quot;</span><span class="p">]))</span>
    <span class="p">]</span>

    <span class="n">pipeline</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">([</span>
        <span class="p">(</span><span class="s2">&quot;scaler&quot;</span><span class="p">,</span> <span class="n">StandardScaler</span><span class="p">()),</span>
        <span class="p">(</span><span class="s2">&quot;mlp&quot;</span><span class="p">,</span> <span class="n">MLPClassifier</span><span class="p">(</span>
            <span class="n">solver</span><span class="o">=</span><span class="s2">&quot;adam&quot;</span><span class="p">,</span>
            <span class="n">hidden_layer_sizes</span><span class="o">=</span><span class="n">hidden_layer_sizes</span><span class="p">,</span>
            <span class="n">alpha</span><span class="o">=</span><span class="n">param_dict</span><span class="p">[</span><span class="s2">&quot;alpha&quot;</span><span class="p">],</span>
            <span class="n">tol</span><span class="o">=</span><span class="n">param_dict</span><span class="p">[</span><span class="s2">&quot;tol&quot;</span><span class="p">],</span>
            <span class="n">beta_1</span><span class="o">=</span><span class="n">param_dict</span><span class="p">[</span><span class="s2">&quot;beta_1&quot;</span><span class="p">],</span>
            <span class="n">beta_2</span><span class="o">=</span><span class="n">param_dict</span><span class="p">[</span><span class="s2">&quot;beta_2&quot;</span><span class="p">],</span>
            <span class="n">learning_rate_init</span><span class="o">=</span><span class="n">param_dict</span><span class="p">[</span><span class="s2">&quot;learning_rate_init&quot;</span><span class="p">]</span>
        <span class="p">))</span>
    <span class="p">])</span>

    <span class="c1"># expedient for testing</span>
    <span class="n">pipeline</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
    <span class="n">metrics</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;auc&quot;</span><span class="p">:</span> <span class="n">roc</span><span class="p">(</span><span class="n">pipeline</span><span class="p">,</span> <span class="n">X_dev</span><span class="p">,</span> <span class="n">y_dev</span><span class="p">),</span>
        <span class="s2">&quot;latency&quot;</span><span class="p">:</span> <span class="n">latency</span><span class="p">(</span><span class="n">pipeline</span><span class="p">,</span> <span class="n">million_points</span><span class="p">)</span>
    <span class="p">}</span>

    <span class="k">return</span> <span class="n">metrics</span>


<span class="k">def</span> <span class="nf">train_wrapper</span><span class="p">(</span><span class="n">param_dict</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">train</span><span class="p">(</span><span class="o">**</span><span class="n">param_dict</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="search-space">
<h3>Search space<a class="headerlink" href="#search-space" title="Permalink to this headline">¶</a></h3>
<p>Search space for scikit-learn’s <code class="docutils literal notranslate"><span class="pre">MLPClassifier</span></code> roughly taken from <a class="reference external" href="https://www.amazon.science/publications/multi-objective-multi-fidelity-hyperparameter-optimization-with-application-to-fairness">Multi-objective multi-fidelity hyperparameter optimization with application to fairness</a>.</p>
<p>Notice that this is a tricky search space! We allow up to three layers in the network, and we must always sample a layer size for each layer, even those we do not use. That means when we vary both <code class="docutils literal notranslate"><span class="pre">layer_3_size</span></code> and <code class="docutils literal notranslate"><span class="pre">n_layers</span></code>, the information about the effect of <code class="docutils literal notranslate"><span class="pre">layer_3_size</span></code> will be noisy, since it may not even be used, and thus not affect the HP search space at all. Hierarchical search spaces are on the long-term wishlist for Ax. As of right now, it works without special consideration, but it makes the problem harder.</p>
<p>You may notice all the parameters are given type float. This seems strange, since the number of layers in a network is definitely an integer. We found that using integer <code class="docutils literal notranslate"><span class="pre">ParameterType</span></code>s resulted in the qEHVI algorithm we use below getting stuck on repeated hyperparameter combinations. The Gaussian Process models under the hood require that all parameters are presented as floats, and so we posit something goes wrong in the internal integer -&gt; float -&gt; integer conversion. Empirically, setting the <code class="docutils literal notranslate"><span class="pre">ParameterType</span></code> to <code class="docutils literal notranslate"><span class="pre">FLOAT</span></code> and casting to an integer on use in the <code class="docutils literal notranslate"><span class="pre">train</span></code> function above avoided the problem of repeated hyperparameter configurations.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">parameter_space</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;n_layers&quot;</span><span class="p">:</span> <span class="n">RangeParameter</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">&quot;n_layers&quot;</span><span class="p">,</span> <span class="n">lower</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">upper</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">parameter_type</span><span class="o">=</span><span class="n">ParameterType</span><span class="o">.</span><span class="n">FLOAT</span><span class="p">),</span>
    <span class="s2">&quot;layer_1_size&quot;</span><span class="p">:</span> <span class="n">RangeParameter</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">&quot;layer_1_size&quot;</span><span class="p">,</span> <span class="n">lower</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">upper</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">parameter_type</span><span class="o">=</span><span class="n">ParameterType</span><span class="o">.</span><span class="n">FLOAT</span><span class="p">),</span>
    <span class="s2">&quot;layer_2_size&quot;</span><span class="p">:</span> <span class="n">RangeParameter</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">&quot;layer_2_size&quot;</span><span class="p">,</span> <span class="n">lower</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">upper</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">parameter_type</span><span class="o">=</span><span class="n">ParameterType</span><span class="o">.</span><span class="n">FLOAT</span><span class="p">),</span>
    <span class="s2">&quot;layer_3_size&quot;</span><span class="p">:</span> <span class="n">RangeParameter</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">&quot;layer_3_size&quot;</span><span class="p">,</span> <span class="n">lower</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">upper</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">parameter_type</span><span class="o">=</span><span class="n">ParameterType</span><span class="o">.</span><span class="n">FLOAT</span><span class="p">),</span>
    <span class="s2">&quot;alpha&quot;</span><span class="p">:</span> <span class="n">RangeParameter</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">&quot;alpha&quot;</span><span class="p">,</span> <span class="n">lower</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">,</span> <span class="n">upper</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">parameter_type</span><span class="o">=</span><span class="n">ParameterType</span><span class="o">.</span><span class="n">FLOAT</span><span class="p">,</span> <span class="n">log_scale</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
    <span class="s2">&quot;tol&quot;</span><span class="p">:</span> <span class="n">RangeParameter</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">&quot;tol&quot;</span><span class="p">,</span> <span class="n">lower</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">,</span> <span class="n">upper</span><span class="o">=</span><span class="mf">1e-1</span><span class="p">,</span> <span class="n">parameter_type</span><span class="o">=</span><span class="n">ParameterType</span><span class="o">.</span><span class="n">FLOAT</span><span class="p">,</span> <span class="n">log_scale</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
    <span class="s2">&quot;beta_1&quot;</span><span class="p">:</span> <span class="n">RangeParameter</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">&quot;beta_1&quot;</span><span class="p">,</span> <span class="n">lower</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span> <span class="n">upper</span><span class="o">=</span><span class="mf">0.99</span><span class="p">,</span> <span class="n">parameter_type</span><span class="o">=</span><span class="n">ParameterType</span><span class="o">.</span><span class="n">FLOAT</span><span class="p">),</span>
    <span class="s2">&quot;beta_2&quot;</span><span class="p">:</span> <span class="n">RangeParameter</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">&quot;beta_2&quot;</span><span class="p">,</span> <span class="n">lower</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span> <span class="n">upper</span><span class="o">=</span><span class="mf">0.99</span><span class="p">,</span> <span class="n">parameter_type</span><span class="o">=</span><span class="n">ParameterType</span><span class="o">.</span><span class="n">FLOAT</span><span class="p">),</span>
    <span class="s2">&quot;learning_rate_init&quot;</span><span class="p">:</span> <span class="n">RangeParameter</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">&quot;learning_rate_init&quot;</span><span class="p">,</span> <span class="n">lower</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">,</span> <span class="n">upper</span><span class="o">=</span><span class="mf">1e-1</span><span class="p">,</span> <span class="n">parameter_type</span><span class="o">=</span><span class="n">ParameterType</span><span class="o">.</span><span class="n">FLOAT</span><span class="p">,</span> <span class="n">log_scale</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="p">}</span>
</pre></div>
</div>
</div>
</div>
<p>From the individual paramater spaces, we construct a search space.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">search_space</span> <span class="o">=</span> <span class="n">SearchSpace</span><span class="p">(</span>
    <span class="n">parameters</span> <span class="o">=</span> <span class="n">parameter_space</span><span class="o">.</span><span class="n">values</span><span class="p">(),</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># This is a helper - the function we optimize must accept an array of arguments,</span>
<span class="c1"># but it is clearer to use named entries in a dict, so we manually convert</span>

<span class="k">def</span> <span class="nf">cast_params_as_dict</span><span class="p">(</span><span class="n">current_params</span><span class="p">):</span>
    <span class="n">current_param_dict</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">key</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">parameter_space</span><span class="o">.</span><span class="n">keys</span><span class="p">())):</span>
        <span class="n">current_param_dict</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">current_params</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">current_param_dict</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="id1">
<h3>Optimization objectives<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h3>
<p>We must define the objectives we care about in a way Ax understands. To do this, we write a small custom class for each metric, inheriting from an Ax base class. Note that we can specify whether lower is better or worse for each objective individually when instantiating the class, making it possible to jointly optimize for maximimum AUC and minimum latency.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">MetricAUC</span><span class="p">(</span><span class="n">NoisyFunctionMetric</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">):</span>
        <span class="n">param_dict</span> <span class="o">=</span> <span class="n">cast_params_as_dict</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
        <span class="k">return</span> <span class="nb">float</span><span class="p">(</span><span class="n">train_wrapper</span><span class="p">(</span><span class="n">param_dict</span><span class="p">)[</span><span class="s2">&quot;auc&quot;</span><span class="p">])</span>
    
<span class="k">class</span> <span class="nc">MetricLatency</span><span class="p">(</span><span class="n">NoisyFunctionMetric</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">):</span>
        <span class="n">param_dict</span> <span class="o">=</span> <span class="n">cast_params_as_dict</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
        <span class="k">return</span> <span class="nb">float</span><span class="p">(</span><span class="n">train_wrapper</span><span class="p">(</span><span class="n">param_dict</span><span class="p">)[</span><span class="s2">&quot;latency&quot;</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">metric_auc</span> <span class="o">=</span> <span class="n">MetricAUC</span><span class="p">(</span><span class="s2">&quot;auc&quot;</span><span class="p">,</span> <span class="n">parameter_space</span><span class="o">.</span><span class="n">keys</span><span class="p">(),</span> <span class="n">noise_sd</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">lower_is_better</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">metric_latency</span> <span class="o">=</span> <span class="n">MetricLatency</span><span class="p">(</span><span class="s2">&quot;latency&quot;</span><span class="p">,</span> <span class="n">parameter_space</span><span class="o">.</span><span class="n">keys</span><span class="p">(),</span> <span class="n">noise_sd</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">lower_is_better</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">mo</span> <span class="o">=</span> <span class="n">MultiObjective</span><span class="p">(</span><span class="n">metrics</span><span class="o">=</span><span class="p">[</span>
    <span class="n">metric_auc</span><span class="p">,</span>
    <span class="n">metric_latency</span>
<span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<p>We must set some <em>objective thresholds</em>. These determine the point from which the hypervolume is measured. The Ax docs advise picking a point around 10% lower (or higher, if the objective is to be minimized) than the lowest acceptable value for the ultimate purpose of the thing we’re optimizing. So, if an algorithm must predict a million points in less than a second, we’d want our latency metric bound at 1.1s.</p>
<p>When first exploring a problem, we may have no idea what accuracy or latency is achievable. In this case we recommend running manually with a few random hyperparameter combinations to get ballpark estimates. Over time, the objective thresholds can be refined to focus the search towards a particular part of the Pareto frontier.</p>
<p>Choosing these values carefully is important. If we choose an objective threshold for a maximization target that we never reach, we won’t be able to calculate a hypervolume at all (it’s zero!) and the optimization process will fail. A little bit of experimentation is recommended here, perhaps with a sample of the dataset you ultimately want to train on for speed.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">relative</span></code> flag allows the bound to be a percentage of the current best trial for that objective, but we’ll stick to absolute thresholds.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">objective_thresholds</span> <span class="o">=</span> <span class="p">[</span>
    <span class="n">ObjectiveThreshold</span><span class="p">(</span><span class="n">metric</span><span class="o">=</span><span class="n">metric_auc</span><span class="p">,</span> <span class="n">bound</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">relative</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
    <span class="n">ObjectiveThreshold</span><span class="p">(</span><span class="n">metric</span><span class="o">=</span><span class="n">metric_latency</span><span class="p">,</span> <span class="n">bound</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">relative</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
<span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<p>We bundle up the objectives and thresholds into a config that will be passed to the experiment.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">optimization_config</span> <span class="o">=</span> <span class="n">MultiObjectiveOptimizationConfig</span><span class="p">(</span>
    <span class="n">objective</span><span class="o">=</span><span class="n">mo</span><span class="p">,</span>
    <span class="n">objective_thresholds</span><span class="o">=</span><span class="n">objective_thresholds</span>   
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="experiment-setup">
<h3>Experiment setup<a class="headerlink" href="#experiment-setup" title="Permalink to this headline">¶</a></h3>
<p>We’ll run two hyperparameter searches: one using the expected hypervolume improvement, and another with quasi-random Sobol sampling for comparison. In both cases, we initialize our model for generating new hyperparameter configs with a few random points, before performing a fixed number of trials. (In the Sobol case, there’s no difference between the random initial points and the rest of the points).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">N_INIT</span> <span class="o">=</span> <span class="mi">5</span>    <span class="c1"># number of initial (SOBOL) points</span>
<span class="n">N_TRIAL</span> <span class="o">=</span> <span class="mi">50</span>  <span class="c1"># number of trials to run after initialization</span>
</pre></div>
</div>
</div>
</div>
<p>A couple of helper functions to avoid repeating ourselves. An <code class="docutils literal notranslate"><span class="pre">Experiment</span></code> is the abstraction for a complete hyperparameter search in Ax. The <code class="docutils literal notranslate"><span class="pre">runner</span></code> determines how the trials of the experiment are deployed. For instance, here, they’re run locally using the <code class="docutils literal notranslate"><span class="pre">SyntheticRunner</span></code> with no changes. Ax allows custom runners, which (the docs tell us) can control things like deploying the experiment to production (to collect data there, useful for A/B testing-like optimization).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">build_experiment</span><span class="p">(</span><span class="n">name</span><span class="p">):</span>
    <span class="n">experiment</span> <span class="o">=</span> <span class="n">Experiment</span><span class="p">(</span>
        <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span>
        <span class="n">search_space</span><span class="o">=</span><span class="n">search_space</span><span class="p">,</span>
        <span class="n">optimization_config</span><span class="o">=</span><span class="n">optimization_config</span><span class="p">,</span>
        <span class="n">runner</span><span class="o">=</span><span class="n">SyntheticRunner</span><span class="p">(),</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">experiment</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">initialize_experiment</span><span class="p">(</span><span class="n">experiment</span><span class="p">):</span>
    <span class="n">sobol</span> <span class="o">=</span> <span class="n">Models</span><span class="o">.</span><span class="n">SOBOL</span><span class="p">(</span><span class="n">search_space</span><span class="o">=</span><span class="n">experiment</span><span class="o">.</span><span class="n">search_space</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">N_INIT</span><span class="p">):</span>
        <span class="n">experiment</span><span class="o">.</span><span class="n">new_trial</span><span class="p">(</span><span class="n">sobol</span><span class="o">.</span><span class="n">gen</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>

    <span class="k">return</span> <span class="n">experiment</span><span class="o">.</span><span class="n">fetch_data</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="mobo-experiment-with-ehvi">
<h3>MOBO Experiment (with EHVI)<a class="headerlink" href="#mobo-experiment-with-ehvi" title="Permalink to this headline">¶</a></h3>
<p>For the first experiment, we’ll use the qEHVI algorithm detailed in the paper <a class="reference external" href="https://arxiv.org/abs/2006.05078">Differentiable Expected Hypervolume Improvement for Parallel Multi-Objective Bayesian Optimization</a>. The “q” in “qEHVI” represents the number of parallel trials, and since we’re not parallelizing anything, q=1 here. We explained how the expected hypervolume improvement (EHVI) acquisition function works in this blog post <a class="reference external" href="https://blog.fastforwardlabs.com/2021/07/07/exploring-multi-objective-hyperparameter-optimization.html">Exploring Multi-Objective Hyperparameter Optimization</a>. In a nutshell, we create a model that takes hyperparameters as inputs and predicts the resultant optimization objectives as outputs. We use this model to find the input hyperparameter combination that will most improve the hypervolume spanned between the Pareto frontier and the objective thresholds we set earlier. Then, we try that hyperparameter config, generating a new data point with which to improve our model, and loop this whole procedure <code class="docutils literal notranslate"><span class="pre">N_TRIAL</span></code> times.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">ehvi_experiment</span><span class="p">(</span><span class="n">experiment</span><span class="p">,</span> <span class="n">data</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Takes in an initialized experiment (with SOBOL, for example)</span>
<span class="sd">    along with data generated from that initial experiment. </span>
<span class="sd">    These provide the basis of the MOO_EHVI model</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">hv_list</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">model</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">N_TRIAL</span><span class="p">):</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Iteration: </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="c1"># create an optimization model</span>
        <span class="n">model</span> <span class="o">=</span> <span class="n">get_MOO_EHVI</span><span class="p">(</span>
            <span class="n">experiment</span><span class="o">=</span><span class="n">experiment</span><span class="p">,</span> 
            <span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="c1"># generate a candidate hp configuration</span>
        <span class="n">generator_run</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">gen</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># turn that config into a trial and run it</span>
        <span class="n">trial</span> <span class="o">=</span> <span class="n">experiment</span><span class="o">.</span><span class="n">new_trial</span><span class="p">(</span><span class="n">generator_run</span><span class="o">=</span><span class="n">generator_run</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Trial parameters: </span><span class="si">{</span><span class="n">trial</span><span class="o">.</span><span class="n">arm</span><span class="o">.</span><span class="n">parameters</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="n">trial</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>

        <span class="c1"># get the results of that trial and append it to existing results</span>
        <span class="n">data</span> <span class="o">=</span> <span class="n">Data</span><span class="o">.</span><span class="n">from_multiple_data</span><span class="p">([</span><span class="n">data</span><span class="p">,</span> <span class="n">trial</span><span class="o">.</span><span class="n">fetch_data</span><span class="p">()])</span>

        <span class="k">try</span><span class="p">:</span>
            <span class="n">hv</span> <span class="o">=</span> <span class="n">observed_hypervolume</span><span class="p">(</span><span class="n">modelbridge</span><span class="o">=</span><span class="n">model</span><span class="p">)</span>
        <span class="k">except</span><span class="p">:</span>
            <span class="n">hv</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Failed to compute hv&quot;</span><span class="p">)</span>
        <span class="n">hv_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">hv</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Hypervolume: </span><span class="si">{</span><span class="n">hv</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">()</span>

    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">exp_to_df</span><span class="p">(</span><span class="n">experiment</span><span class="p">)[[</span><span class="s2">&quot;auc&quot;</span><span class="p">,</span> <span class="s2">&quot;latency&quot;</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">double</span><span class="p">),</span> <span class="n">model</span><span class="p">,</span> <span class="n">data</span>
</pre></div>
</div>
</div>
</div>
<p>Initialize the experiment with <code class="docutils literal notranslate"><span class="pre">N_INIT</span></code> random points, and fit a model to those points.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">mobo_experiment</span> <span class="o">=</span> <span class="n">build_experiment</span><span class="p">(</span><span class="s2">&quot;mo_mlp&quot;</span><span class="p">)</span>
<span class="n">mobo_data</span> <span class="o">=</span> <span class="n">initialize_experiment</span><span class="p">(</span><span class="n">mobo_experiment</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Begin sampling hyperparameter configurations smartly here. We output the outcomes, model and raw data. We’ll only actually use the outcomes below, but Ax has a host of neat visualizations that can be built from the model and data, and it’s handy to have them around if you’d like to have a play!</p>
<p>On each trial we also output the hyperparameter combination. Note that everything is a float due to our earlier computational trick. Notice also that we generate all the <code class="docutils literal notranslate"><span class="pre">layer_x_size</span></code> hyperparameters on each trial, even if <code class="docutils literal notranslate"><span class="pre">n_layers</span></code> means that they won’t all be used.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">mobo_outcomes</span><span class="p">,</span> <span class="n">mobo_model</span><span class="p">,</span> <span class="n">mobo_post_data</span> <span class="o">=</span> <span class="n">ehvi_experiment</span><span class="p">(</span><span class="n">mobo_experiment</span><span class="p">,</span> <span class="n">mobo_data</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Iteration: 0
Trial parameters: {&#39;n_layers&#39;: 2.171482669035623, &#39;layer_1_size&#39;: 30.08647830534666, &#39;layer_2_size&#39;: 2.071838398216976, &#39;layer_3_size&#39;: 2.0, &#39;alpha&#39;: 1.1461861985117928e-05, &#39;tol&#39;: 0.0015148752326422294, &#39;beta_1&#39;: 0.2823774422300207, &#39;beta_2&#39;: 0.3235873399832296, &#39;learning_rate_init&#39;: 0.1}
Hypervolume: 0.4542651975154876

Iteration: 1
Trial parameters: {&#39;n_layers&#39;: 2.1559662174735656, &#39;layer_1_size&#39;: 34.392685004951716, &#39;layer_2_size&#39;: 2.0, &#39;layer_3_size&#39;: 2.0000000000000004, &#39;alpha&#39;: 4.885909407587005e-06, &#39;tol&#39;: 0.001581289807264481, &#39;beta_1&#39;: 0.4486767992040468, &#39;beta_2&#39;: 0.24576658413799793, &#39;learning_rate_init&#39;: 0.1}
Hypervolume: 0.4969227111339569

Iteration: 2
Trial parameters: {&#39;n_layers&#39;: 2.168520731591188, &#39;layer_1_size&#39;: 32.788328555101394, &#39;layer_2_size&#39;: 2.0, &#39;layer_3_size&#39;: 2.0, &#39;alpha&#39;: 7.70695609181423e-05, &#39;tol&#39;: 0.0018821611128834747, &#39;beta_1&#39;: 0.32901484388947333, &#39;beta_2&#39;: 0.17551068504044162, &#39;learning_rate_init&#39;: 0.1}
Hypervolume: 0.4969227111339569

Iteration: 3
Trial parameters: {&#39;n_layers&#39;: 2.1965441163266024, &#39;layer_1_size&#39;: 38.878539082805126, &#39;layer_2_size&#39;: 2.0, &#39;layer_3_size&#39;: 2.000000000000001, &#39;alpha&#39;: 1.1693167177245632e-05, &#39;tol&#39;: 0.0032980252422236574, &#39;beta_1&#39;: 0.24440184158634584, &#39;beta_2&#39;: 0.30631586777782577, &#39;learning_rate_init&#39;: 0.1}
Hypervolume: 0.4969227111339569

Iteration: 4
Trial parameters: {&#39;n_layers&#39;: 2.1554462401856114, &#39;layer_1_size&#39;: 27.876108720090528, &#39;layer_2_size&#39;: 2.0, &#39;layer_3_size&#39;: 2.0, &#39;alpha&#39;: 9.285826395574598e-06, &#39;tol&#39;: 0.007915046766495017, &#39;beta_1&#39;: 0.3393730911921644, &#39;beta_2&#39;: 0.23572099775207456, &#39;learning_rate_init&#39;: 0.1}
Hypervolume: 0.5018305158615112

Iteration: 5
Trial parameters: {&#39;n_layers&#39;: 2.504480657269585, &#39;layer_1_size&#39;: 31.543215389630127, &#39;layer_2_size&#39;: 2.0, &#39;layer_3_size&#39;: 2.0, &#39;alpha&#39;: 1.2450609638958142e-05, &#39;tol&#39;: 0.0016577129894726156, &#39;beta_1&#39;: 0.32804614370209617, &#39;beta_2&#39;: 0.24114523691423684, &#39;learning_rate_init&#39;: 0.07324881520582602}
Hypervolume: 0.5077590203285217

Iteration: 6
Trial parameters: {&#39;n_layers&#39;: 2.1407625350324024, &#39;layer_1_size&#39;: 32.975575935025184, &#39;layer_2_size&#39;: 7.5641726314122355, &#39;layer_3_size&#39;: 2.000000000000002, &#39;alpha&#39;: 5.263330295525675e-06, &#39;tol&#39;: 0.0011806415752733932, &#39;beta_1&#39;: 0.24051611665607558, &#39;beta_2&#39;: 0.1648346820716327, &#39;learning_rate_init&#39;: 0.07149941692102016}
Hypervolume: 0.5077590203285217

Iteration: 7
Trial parameters: {&#39;n_layers&#39;: 2.2844628346428317, &#39;layer_1_size&#39;: 35.6128305312202, &#39;layer_2_size&#39;: 2.0, &#39;layer_3_size&#39;: 2.0, &#39;alpha&#39;: 4.521445246774359e-05, &#39;tol&#39;: 0.0019857893588151065, &#39;beta_1&#39;: 0.38858326786146824, &#39;beta_2&#39;: 0.3569262215709754, &#39;learning_rate_init&#39;: 0.1}
Hypervolume: 0.5077590203285217

Iteration: 8
Trial parameters: {&#39;n_layers&#39;: 2.181785332592603, &#39;layer_1_size&#39;: 32.092949056359785, &#39;layer_2_size&#39;: 2.0000000000000138, &#39;layer_3_size&#39;: 8.018564742883298, &#39;alpha&#39;: 1.762033690693942e-05, &#39;tol&#39;: 0.0021589475480211498, &#39;beta_1&#39;: 0.33056563088506113, &#39;beta_2&#39;: 0.28647770878039147, &#39;learning_rate_init&#39;: 0.053369993520097275}
Hypervolume: 0.5077590203285217

Iteration: 9
Trial parameters: {&#39;n_layers&#39;: 2.1229704596322803, &#39;layer_1_size&#39;: 33.379802585996465, &#39;layer_2_size&#39;: 2.000000005201619, &#39;layer_3_size&#39;: 2.0, &#39;alpha&#39;: 1.9470067154406234e-05, &#39;tol&#39;: 0.002212618601943917, &#39;beta_1&#39;: 0.33384352135660883, &#39;beta_2&#39;: 0.2883312944327689, &#39;learning_rate_init&#39;: 0.02161882611679547}
Hypervolume: 0.528127644062042

Iteration: 10
Trial parameters: {&#39;n_layers&#39;: 2.2761734072992796, &#39;layer_1_size&#39;: 21.24786952110524, &#39;layer_2_size&#39;: 2.0, &#39;layer_3_size&#39;: 9.77580267266817, &#39;alpha&#39;: 2.2782447436130996e-05, &#39;tol&#39;: 0.0013196204781981532, &#39;beta_1&#39;: 0.41803511519533454, &#39;beta_2&#39;: 0.2476367755743143, &#39;learning_rate_init&#39;: 0.09999999993081568}
Hypervolume: 0.528127644062042

Iteration: 11
Trial parameters: {&#39;n_layers&#39;: 2.3708642794808106, &#39;layer_1_size&#39;: 15.961970335777423, &#39;layer_2_size&#39;: 2.0000000000000466, &#39;layer_3_size&#39;: 11.30367195097513, &#39;alpha&#39;: 5.296683778565724e-05, &#39;tol&#39;: 0.0026748010966388294, &#39;beta_1&#39;: 0.19669488713593217, &#39;beta_2&#39;: 0.28400806993714833, &#39;learning_rate_init&#39;: 0.0999999999999957}
Hypervolume: 0.5423303472995756

Iteration: 12
Trial parameters: {&#39;n_layers&#39;: 2.3879304584355525, &#39;layer_1_size&#39;: 12.110614309508819, &#39;layer_2_size&#39;: 2.000000000000383, &#39;layer_3_size&#39;: 12.090141099811415, &#39;alpha&#39;: 3.2485296835728523e-06, &#39;tol&#39;: 0.0026534332055059328, &#39;beta_1&#39;: 0.25368824794074213, &#39;beta_2&#39;: 0.3648850806720361, &#39;learning_rate_init&#39;: 0.09999999999989959}
Hypervolume: 0.552562699317932

Iteration: 13
Trial parameters: {&#39;n_layers&#39;: 2.3008292557503456, &#39;layer_1_size&#39;: 5.931034087172928, &#39;layer_2_size&#39;: 2.0, &#39;layer_3_size&#39;: 5.531588035603183, &#39;alpha&#39;: 4.4114821778808025e-06, &#39;tol&#39;: 0.0026032083097342466, &#39;beta_1&#39;: 0.28990616132537833, &#39;beta_2&#39;: 0.2839499769192946, &#39;learning_rate_init&#39;: 0.1}
Hypervolume: 0.5786638271808623

Iteration: 14
Trial parameters: {&#39;n_layers&#39;: 2.4358822658457693, &#39;layer_1_size&#39;: 2.0, &#39;layer_2_size&#39;: 2.0, &#39;layer_3_size&#39;: 2.40868687631494, &#39;alpha&#39;: 2.1746395739235583e-06, &#39;tol&#39;: 0.0023827308697848474, &#39;beta_1&#39;: 0.26786634367069173, &#39;beta_2&#39;: 0.21043247132917572, &#39;learning_rate_init&#39;: 0.1}
Hypervolume: 0.6390115344524382

Iteration: 15
Trial parameters: {&#39;n_layers&#39;: 2.2977471124946436, &#39;layer_1_size&#39;: 29.647383247239524, &#39;layer_2_size&#39;: 2.0000000000817857, &#39;layer_3_size&#39;: 13.377488415458469, &#39;alpha&#39;: 7.894079277531108e-06, &#39;tol&#39;: 0.002655347185384139, &#39;beta_1&#39;: 0.2927377671795356, &#39;beta_2&#39;: 0.3326066685098693, &#39;learning_rate_init&#39;: 0.09365821870574463}
Hypervolume: 0.6743603813648222

Iteration: 16
Trial parameters: {&#39;n_layers&#39;: 2.4198308960816055, &#39;layer_1_size&#39;: 17.71988872165018, &#39;layer_2_size&#39;: 2.0, &#39;layer_3_size&#39;: 11.098336461857787, &#39;alpha&#39;: 1.390103035708873e-05, &#39;tol&#39;: 0.0018082427622846322, &#39;beta_1&#39;: 0.262680807696098, &#39;beta_2&#39;: 0.3729265674811088, &#39;learning_rate_init&#39;: 0.03670215196731105}
Hypervolume: 0.6747909152507781

Iteration: 17
Trial parameters: {&#39;n_layers&#39;: 2.455302931995293, &#39;layer_1_size&#39;: 2.0, &#39;layer_2_size&#39;: 2.0, &#39;layer_3_size&#39;: 2.000000002616093, &#39;alpha&#39;: 1.1934609500280115e-06, &#39;tol&#39;: 0.0006977356479936777, &#39;beta_1&#39;: 0.26402431567435675, &#39;beta_2&#39;: 0.18131391161165974, &#39;learning_rate_init&#39;: 0.1}
Hypervolume: 0.6747909152507781

Iteration: 18
Trial parameters: {&#39;n_layers&#39;: 2.7246090963433707, &#39;layer_1_size&#39;: 2.00000000787909, &#39;layer_2_size&#39;: 2.0000000120430705, &#39;layer_3_size&#39;: 2.000000073127251, &#39;alpha&#39;: 2.9801204533115163e-06, &#39;tol&#39;: 0.0011829638364021704, &#39;beta_1&#39;: 0.32294203970911844, &#39;beta_2&#39;: 0.26455338645478965, &#39;learning_rate_init&#39;: 0.09999999972422231}
Hypervolume: 0.6753506302833556

Iteration: 19
Trial parameters: {&#39;n_layers&#39;: 2.511806025975865, &#39;layer_1_size&#39;: 2.000000000000165, &#39;layer_2_size&#39;: 2.0000000000003606, &#39;layer_3_size&#39;: 2.000000000001316, &#39;alpha&#39;: 1.0000000000001185e-06, &#39;tol&#39;: 0.0017893153320823576, &#39;beta_1&#39;: 0.37947419521767267, &#39;beta_2&#39;: 0.10969514552380613, &#39;learning_rate_init&#39;: 0.1}
Hypervolume: 0.6753506302833556

Iteration: 20
Trial parameters: {&#39;n_layers&#39;: 2.5668846185546323, &#39;layer_1_size&#39;: 2.0000000000000098, &#39;layer_2_size&#39;: 2.0000000000000187, &#39;layer_3_size&#39;: 2.0000000000000187, &#39;alpha&#39;: 2.2983096452556754e-06, &#39;tol&#39;: 0.0008368413669434144, &#39;beta_1&#39;: 0.18750360351542125, &#39;beta_2&#39;: 0.2722287921883868, &#39;learning_rate_init&#39;: 0.09999999999999959}
Hypervolume: 0.6753506302833556

Iteration: 21
Trial parameters: {&#39;n_layers&#39;: 2.175409661713549, &#39;layer_1_size&#39;: 30.96061600468984, &#39;layer_2_size&#39;: 4.725067706896717, &#39;layer_3_size&#39;: 11.584705354692389, &#39;alpha&#39;: 5.523780001275952e-05, &#39;tol&#39;: 0.0008281034363167859, &#39;beta_1&#39;: 0.3857629606631202, &#39;beta_2&#39;: 0.2754608293485975, &#39;learning_rate_init&#39;: 0.09999999956225132}
Hypervolume: 0.6753506302833556

Iteration: 22
Trial parameters: {&#39;n_layers&#39;: 2.579249473582747, &#39;layer_1_size&#39;: 14.107374786788643, &#39;layer_2_size&#39;: 2.000000000000044, &#39;layer_3_size&#39;: 9.853311565321544, &#39;alpha&#39;: 8.98716770781367e-06, &#39;tol&#39;: 0.0012688358433801683, &#39;beta_1&#39;: 0.2486651132010794, &#39;beta_2&#39;: 0.2532521322179295, &#39;learning_rate_init&#39;: 0.1}
Hypervolume: 0.6753506302833556

Iteration: 23
Trial parameters: {&#39;n_layers&#39;: 2.0769455423026284, &#39;layer_1_size&#39;: 21.32043127246135, &#39;layer_2_size&#39;: 2.0000324928404685, &#39;layer_3_size&#39;: 5.542660610274344, &#39;alpha&#39;: 1.8832645533399574e-05, &#39;tol&#39;: 0.0006735630400330491, &#39;beta_1&#39;: 0.16868690493511163, &#39;beta_2&#39;: 0.21299442768194718, &#39;learning_rate_init&#39;: 0.09999957895958349}
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Hypervolume: 0.6753506302833556

Iteration: 24
Trial parameters: {&#39;n_layers&#39;: 2.33898454049863, &#39;layer_1_size&#39;: 2.0, &#39;layer_2_size&#39;: 2.0, &#39;layer_3_size&#39;: 2.000000001451734, &#39;alpha&#39;: 1.0000000002168774e-06, &#39;tol&#39;: 0.0017696101934131195, &#39;beta_1&#39;: 0.40236014283387966, &#39;beta_2&#39;: 0.3849995251040405, &#39;learning_rate_init&#39;: 0.1}
Hypervolume: 0.6753506302833556

Iteration: 25
Trial parameters: {&#39;n_layers&#39;: 2.0998638534663705, &#39;layer_1_size&#39;: 16.221529986826575, &#39;layer_2_size&#39;: 4.216120793925986, &#39;layer_3_size&#39;: 8.18961306754179, &#39;alpha&#39;: 4.153461509004716e-05, &#39;tol&#39;: 0.00823856122117489, &#39;beta_1&#39;: 0.2550077074035585, &#39;beta_2&#39;: 0.36015722570253444, &#39;learning_rate_init&#39;: 0.1}
Hypervolume: 0.6753506302833556

Iteration: 26
Trial parameters: {&#39;n_layers&#39;: 2.3690197106053503, &#39;layer_1_size&#39;: 2.000000000001144, &#39;layer_2_size&#39;: 2.0000000000012297, &#39;layer_3_size&#39;: 2.000000000001774, &#39;alpha&#39;: 1.1074317504396192e-06, &#39;tol&#39;: 0.0004962911348616523, &#39;beta_1&#39;: 0.33693812845081966, &#39;beta_2&#39;: 0.22181933448306582, &#39;learning_rate_init&#39;: 0.027404057764938254}
Hypervolume: 0.6753506302833556

Iteration: 27
Trial parameters: {&#39;n_layers&#39;: 2.2907800929747935, &#39;layer_1_size&#39;: 28.025427409474666, &#39;layer_2_size&#39;: 2.0, &#39;layer_3_size&#39;: 11.319119104383475, &#39;alpha&#39;: 2.847524866552166e-06, &#39;tol&#39;: 0.002912666172343327, &#39;beta_1&#39;: 0.2679797247421904, &#39;beta_2&#39;: 0.28236319216160854, &#39;learning_rate_init&#39;: 0.1}
Hypervolume: 0.6767765033245084

Iteration: 28
Trial parameters: {&#39;n_layers&#39;: 2.259476975678476, &#39;layer_1_size&#39;: 27.56570138818317, &#39;layer_2_size&#39;: 2.0000000000001186, &#39;layer_3_size&#39;: 7.406192447839585, &#39;alpha&#39;: 5.8156337545378865e-05, &#39;tol&#39;: 0.003581516697766077, &#39;beta_1&#39;: 0.37020848608648477, &#39;beta_2&#39;: 0.3046460563373836, &#39;learning_rate_init&#39;: 0.09999999999999938}
Hypervolume: 0.6767765033245084

Iteration: 29
Trial parameters: {&#39;n_layers&#39;: 2.3617568674505836, &#39;layer_1_size&#39;: 2.0, &#39;layer_2_size&#39;: 2.0, &#39;layer_3_size&#39;: 2.0, &#39;alpha&#39;: 1.0183003235982578e-05, &#39;tol&#39;: 0.0004016631805430063, &#39;beta_1&#39;: 0.34970091693446326, &#39;beta_2&#39;: 0.22008698980346006, &#39;learning_rate_init&#39;: 0.09470149839864953}
Hypervolume: 0.6767765033245084

Iteration: 30
Trial parameters: {&#39;n_layers&#39;: 2.0880496859648394, &#39;layer_1_size&#39;: 2.0000000440604184, &#39;layer_2_size&#39;: 2.000000000114252, &#39;layer_3_size&#39;: 15.075828925559662, &#39;alpha&#39;: 9.893983594078947e-06, &#39;tol&#39;: 0.0018839212529773062, &#39;beta_1&#39;: 0.16542865507266338, &#39;beta_2&#39;: 0.3373743623511453, &#39;learning_rate_init&#39;: 0.07408994735976521}
Hypervolume: 0.6767765033245084

Iteration: 31
Trial parameters: {&#39;n_layers&#39;: 2.352931728672978, &#39;layer_1_size&#39;: 27.13170525586469, &#39;layer_2_size&#39;: 3.590962896407267, &#39;layer_3_size&#39;: 6.521828584272605, &#39;alpha&#39;: 7.596596299851156e-06, &#39;tol&#39;: 0.005204493146422204, &#39;beta_1&#39;: 0.2549968087560718, &#39;beta_2&#39;: 0.37968984045903487, &#39;learning_rate_init&#39;: 0.09999999999998384}
Hypervolume: 0.6767765033245084

Iteration: 32
Trial parameters: {&#39;n_layers&#39;: 2.2955540989566177, &#39;layer_1_size&#39;: 2.000000649759022, &#39;layer_2_size&#39;: 2.000000243315218, &#39;layer_3_size&#39;: 9.878974090376314, &#39;alpha&#39;: 1.000000676232105e-06, &#39;tol&#39;: 0.0009031454606763224, &#39;beta_1&#39;: 0.25408420038110113, &#39;beta_2&#39;: 0.2262085111078986, &#39;learning_rate_init&#39;: 0.052124754641595245}
Hypervolume: 0.677192803621292

Iteration: 33
Trial parameters: {&#39;n_layers&#39;: 2.4149049199187242, &#39;layer_1_size&#39;: 2.000000000181535, &#39;layer_2_size&#39;: 2.0000000019973263, &#39;layer_3_size&#39;: 5.882747003652283, &#39;alpha&#39;: 5.737428125076955e-06, &#39;tol&#39;: 0.010178097552905557, &#39;beta_1&#39;: 0.22544160060416105, &#39;beta_2&#39;: 0.3505036704064393, &#39;learning_rate_init&#39;: 0.024243137848720956}
Hypervolume: 0.677192803621292

Iteration: 34
Trial parameters: {&#39;n_layers&#39;: 2.4678263688509934, &#39;layer_1_size&#39;: 21.697644122237072, &#39;layer_2_size&#39;: 7.563284500820823, &#39;layer_3_size&#39;: 7.4579621057293455, &#39;alpha&#39;: 9.079379776888422e-06, &#39;tol&#39;: 0.004510449265109881, &#39;beta_1&#39;: 0.27434494259025083, &#39;beta_2&#39;: 0.34449538455090056, &#39;learning_rate_init&#39;: 0.08377645457746859}
Hypervolume: 0.6801311779022214

Iteration: 35
Trial parameters: {&#39;n_layers&#39;: 2.006482158747981, &#39;layer_1_size&#39;: 32.93672039503974, &#39;layer_2_size&#39;: 2.0000000000014766, &#39;layer_3_size&#39;: 7.518012395572798, &#39;alpha&#39;: 9.052510679289635e-06, &#39;tol&#39;: 0.003933767392095681, &#39;beta_1&#39;: 0.3358597230209689, &#39;beta_2&#39;: 0.3767985508681723, &#39;learning_rate_init&#39;: 0.09999999999998528}
Hypervolume: 0.6801311779022214

Iteration: 36
Trial parameters: {&#39;n_layers&#39;: 2.349710133978586, &#39;layer_1_size&#39;: 2.0000000678741903, &#39;layer_2_size&#39;: 2.000000058386465, &#39;layer_3_size&#39;: 10.357118795659213, &#39;alpha&#39;: 8.168960636911883e-06, &#39;tol&#39;: 0.020465459096204893, &#39;beta_1&#39;: 0.2510726162481965, &#39;beta_2&#39;: 0.3987139372379151, &#39;learning_rate_init&#39;: 0.09999999932335508}
Hypervolume: 0.6801311779022214

Iteration: 37
Trial parameters: {&#39;n_layers&#39;: 2.2357994263303764, &#39;layer_1_size&#39;: 46.14269100155069, &#39;layer_2_size&#39;: 2.0, &#39;layer_3_size&#39;: 6.93141556387207, &#39;alpha&#39;: 3.708630016435144e-05, &#39;tol&#39;: 0.0009668094858794671, &#39;beta_1&#39;: 0.3059108542210651, &#39;beta_2&#39;: 0.21861364996603727, &#39;learning_rate_init&#39;: 0.1}
Hypervolume: 0.6801311779022214

Iteration: 38
Trial parameters: {&#39;n_layers&#39;: 2.184197114686457, &#39;layer_1_size&#39;: 2.0, &#39;layer_2_size&#39;: 2.0, &#39;layer_3_size&#39;: 3.7283172384029086, &#39;alpha&#39;: 5.169955887654654e-06, &#39;tol&#39;: 0.007387845725779292, &#39;beta_1&#39;: 0.06269954077957314, &#39;beta_2&#39;: 0.37077648557029524, &#39;learning_rate_init&#39;: 0.1}
Hypervolume: 0.6801311779022214

Iteration: 39
Trial parameters: {&#39;n_layers&#39;: 2.3263929203099343, &#39;layer_1_size&#39;: 29.68348438392572, &#39;layer_2_size&#39;: 2.0, &#39;layer_3_size&#39;: 7.7463465326862115, &#39;alpha&#39;: 2.1483380298530478e-05, &#39;tol&#39;: 0.001032589282398367, &#39;beta_1&#39;: 0.2445609769907558, &#39;beta_2&#39;: 0.27632860476868254, &#39;learning_rate_init&#39;: 0.1}
Hypervolume: 0.6801311779022214

Iteration: 40
Trial parameters: {&#39;n_layers&#39;: 2.2798317963232746, &#39;layer_1_size&#39;: 18.71262709991881, &#39;layer_2_size&#39;: 2.000000001819187, &#39;layer_3_size&#39;: 8.451682992783047, &#39;alpha&#39;: 6.622882524204553e-06, &#39;tol&#39;: 0.0082636922516075, &#39;beta_1&#39;: 0.3435557939636251, &#39;beta_2&#39;: 0.3618395643286626, &#39;learning_rate_init&#39;: 0.04623236360930396}
Hypervolume: 0.6801311779022214

Iteration: 41
Trial parameters: {&#39;n_layers&#39;: 2.2136337114318314, &#39;layer_1_size&#39;: 38.955223653694446, &#39;layer_2_size&#39;: 2.000000000017822, &#39;layer_3_size&#39;: 5.68149005894366, &#39;alpha&#39;: 3.4270994681317695e-05, &#39;tol&#39;: 0.0030102534687364937, &#39;beta_1&#39;: 0.20868182508050467, &#39;beta_2&#39;: 0.3732647779303089, &#39;learning_rate_init&#39;: 0.09999999999972985}
Hypervolume: 0.6801311779022214

Iteration: 42
Trial parameters: {&#39;n_layers&#39;: 2.216707352843848, &#39;layer_1_size&#39;: 25.069550956338375, &#39;layer_2_size&#39;: 2.0000000000000067, &#39;layer_3_size&#39;: 7.432740718517184, &#39;alpha&#39;: 4.527609371630962e-05, &#39;tol&#39;: 0.0022895467378352854, &#39;beta_1&#39;: 0.284063350070402, &#39;beta_2&#39;: 0.23670920686675387, &#39;learning_rate_init&#39;: 0.1}
Hypervolume: 0.6801311779022214

Iteration: 43
Trial parameters: {&#39;n_layers&#39;: 2.3454902720276714, &#39;layer_1_size&#39;: 30.65477749815171, &#39;layer_2_size&#39;: 2.000000000060351, &#39;layer_3_size&#39;: 2.0000000001119997, &#39;alpha&#39;: 5.714022114131987e-06, &#39;tol&#39;: 0.00023635449987376156, &#39;beta_1&#39;: 0.30952532702200886, &#39;beta_2&#39;: 0.20231028247553035, &#39;learning_rate_init&#39;: 0.042187511793780934}
Hypervolume: 0.6801311779022214

Iteration: 44
Trial parameters: {&#39;n_layers&#39;: 2.5493636759698317, &#39;layer_1_size&#39;: 2.0000000000001195, &#39;layer_2_size&#39;: 2.0000000000002562, &#39;layer_3_size&#39;: 2.0000000000000515, &#39;alpha&#39;: 3.337160492526422e-05, &#39;tol&#39;: 0.008055640281554123, &#39;beta_1&#39;: 0.264499875570493, &#39;beta_2&#39;: 0.36481315258112285, &#39;learning_rate_init&#39;: 0.09999999999999734}
Hypervolume: 0.6801311779022214

Iteration: 45
Trial parameters: {&#39;n_layers&#39;: 2.2912920718855343, &#39;layer_1_size&#39;: 38.939932330146995, &#39;layer_2_size&#39;: 2.000000000036352, &#39;layer_3_size&#39;: 9.077504772173233, &#39;alpha&#39;: 1.2275211933729696e-05, &#39;tol&#39;: 0.001218185311188117, &#39;beta_1&#39;: 0.44054052414950007, &#39;beta_2&#39;: 0.29173638248149475, &#39;learning_rate_init&#39;: 0.09999999999900935}
Hypervolume: 0.6801311779022214

Iteration: 46
Trial parameters: {&#39;n_layers&#39;: 2.42665466917612, &#39;layer_1_size&#39;: 18.2870757134025, &#39;layer_2_size&#39;: 2.000000058947996, &#39;layer_3_size&#39;: 12.890261731007357, &#39;alpha&#39;: 2.059246538284955e-05, &#39;tol&#39;: 0.002046482844228231, &#39;beta_1&#39;: 0.29707952637039114, &#39;beta_2&#39;: 0.28456789206766875, &#39;learning_rate_init&#39;: 0.020527063683686758}
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Hypervolume: 0.6801311779022214

Iteration: 47
Trial parameters: {&#39;n_layers&#39;: 2.1740833359780547, &#39;layer_1_size&#39;: 2.00000000722151, &#39;layer_2_size&#39;: 2.0000003910139785, &#39;layer_3_size&#39;: 17.72250574389064, &#39;alpha&#39;: 9.625332225111807e-05, &#39;tol&#39;: 0.05984304155379833, &#39;beta_1&#39;: 0.159189341037692, &#39;beta_2&#39;: 0.3310724166482909, &#39;learning_rate_init&#39;: 0.09999999566833469}
Hypervolume: 0.6801311779022214

Iteration: 48
Trial parameters: {&#39;n_layers&#39;: 2.387685932472402, &#39;layer_1_size&#39;: 2.0, &#39;layer_2_size&#39;: 2.0, &#39;layer_3_size&#39;: 12.46606763938873, &#39;alpha&#39;: 2.2725143880376773e-05, &#39;tol&#39;: 0.06319847041938564, &#39;beta_1&#39;: 0.0920470780206449, &#39;beta_2&#39;: 0.31482440098010545, &#39;learning_rate_init&#39;: 0.1}
Hypervolume: 0.6845613408088683

Iteration: 49
Trial parameters: {&#39;n_layers&#39;: 2.0815076900282987, &#39;layer_1_size&#39;: 10.89255115911259, &#39;layer_2_size&#39;: 2.0, &#39;layer_3_size&#39;: 21.358310305745476, &#39;alpha&#39;: 0.0002325608788495778, &#39;tol&#39;: 0.02548872435612815, &#39;beta_1&#39;: 0.2381769461749815, &#39;beta_2&#39;: 0.3335624922305656, &#39;learning_rate_init&#39;: 0.09999999999998814}
Hypervolume: 0.6845613408088683
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="random-experiment-with-sobol">
<h3>Random Experiment (with Sobol)<a class="headerlink" href="#random-experiment-with-sobol" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">sobol_experiment</span><span class="p">(</span><span class="n">experiment</span><span class="p">,</span> <span class="n">data</span><span class="p">):</span>
    <span class="n">sobol_model</span> <span class="o">=</span> <span class="n">Models</span><span class="o">.</span><span class="n">SOBOL</span><span class="p">(</span>
        <span class="n">experiment</span><span class="o">=</span><span class="n">experiment</span><span class="p">,</span> 
        <span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">sobol_hv_list</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">N_TRIAL</span><span class="p">):</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Iteration: </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        
        <span class="c1"># generate a random hp configuration</span>
        <span class="n">generator_run</span> <span class="o">=</span> <span class="n">sobol_model</span><span class="o">.</span><span class="n">gen</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
                
        <span class="c1"># turn that config into a trial and run it</span>
        <span class="n">trial</span> <span class="o">=</span> <span class="n">experiment</span><span class="o">.</span><span class="n">new_trial</span><span class="p">(</span><span class="n">generator_run</span><span class="o">=</span><span class="n">generator_run</span><span class="p">)</span>
        <span class="n">trial</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>

        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Trial parameters: </span><span class="si">{</span><span class="n">trial</span><span class="o">.</span><span class="n">arm</span><span class="o">.</span><span class="n">parameters</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="c1"># Fit a GP-based model in order to compute a hypervolume</span>
        <span class="c1"># NOT used to generate new hp configuration points</span>
        <span class="n">dummy_model</span> <span class="o">=</span> <span class="n">get_MOO_EHVI</span><span class="p">(</span>
            <span class="n">experiment</span><span class="o">=</span><span class="n">experiment</span><span class="p">,</span> 
            <span class="n">data</span><span class="o">=</span><span class="n">experiment</span><span class="o">.</span><span class="n">fetch_data</span><span class="p">(),</span>
        <span class="p">)</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">hv</span> <span class="o">=</span> <span class="n">observed_hypervolume</span><span class="p">(</span><span class="n">modelbridge</span><span class="o">=</span><span class="n">dummy_model</span><span class="p">)</span>
        <span class="k">except</span><span class="p">:</span>
            <span class="n">hv</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Failed to compute hv&quot;</span><span class="p">)</span>
        <span class="n">sobol_hv_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">hv</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Hypervolume: </span><span class="si">{</span><span class="n">hv</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">()</span>

    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">exp_to_df</span><span class="p">(</span><span class="n">experiment</span><span class="p">)[[</span><span class="s2">&quot;auc&quot;</span><span class="p">,</span> <span class="s2">&quot;latency&quot;</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">double</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">random_experiment</span> <span class="o">=</span> <span class="n">build_experiment</span><span class="p">(</span><span class="s2">&quot;sobol_mlp&quot;</span><span class="p">)</span>
<span class="n">random_data</span> <span class="o">=</span> <span class="n">initialize_experiment</span><span class="p">(</span><span class="n">random_experiment</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">random_outcomes</span> <span class="o">=</span> <span class="n">sobol_experiment</span><span class="p">(</span><span class="n">random_experiment</span><span class="p">,</span> <span class="n">random_data</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Iteration: 0
Trial parameters: {&#39;n_layers&#39;: 2.3876266479492188, &#39;layer_1_size&#39;: 32.793450355529785, &#39;layer_2_size&#39;: 29.24840259552002, &#39;layer_3_size&#39;: 24.738411903381348, &#39;alpha&#39;: 0.7005566319366558, &#39;tol&#39;: 0.012106192465260531, &#39;beta_1&#39;: 0.02480564844235778, &#39;beta_2&#39;: 0.8933606113791466, &#39;learning_rate_init&#39;: 2.367064363405651e-06}
Hypervolume: 0.5037194621562957

Iteration: 1
Trial parameters: {&#39;n_layers&#39;: 1.626617981120944, &#39;layer_1_size&#39;: 19.20220473408699, &#39;layer_2_size&#39;: 7.705499842762947, &#39;layer_3_size&#39;: 30.610647603869438, &#39;alpha&#39;: 0.0006718020342532956, &#39;tol&#39;: 0.00015653476454037594, &#39;beta_1&#39;: 0.8733604594357312, &#39;beta_2&#39;: 0.11035005169361829, &#39;learning_rate_init&#39;: 0.043907645241162696}
Hypervolume: 0.5067433512210845

Iteration: 2
Trial parameters: {&#39;n_layers&#39;: 1.331133909523487, &#39;layer_1_size&#39;: 45.251627668738365, &#39;layer_2_size&#39;: 44.46834337711334, &#39;layer_3_size&#39;: 11.565945342183113, &#39;alpha&#39;: 4.882724323426376e-06, &#39;tol&#39;: 1.948534770172022e-05, &#39;beta_1&#39;: 0.469042191199027, &#39;beta_2&#39;: 0.4463693572785705, &#39;learning_rate_init&#39;: 0.003326329390861609}
Hypervolume: 0.5251318323612213

Iteration: 3
Trial parameters: {&#39;n_layers&#39;: 2.592106305062771, &#39;layer_1_size&#39;: 6.717572033405304, &#39;layer_2_size&#39;: 22.48565535247326, &#39;layer_3_size&#39;: 41.531844675540924, &#39;alpha&#39;: 0.004691798757360244, &#39;tol&#39;: 0.002708534513507292, &#39;beta_1&#39;: 0.6143161879442632, &#39;beta_2&#39;: 0.534049687971361, &#39;learning_rate_init&#39;: 2.888559484874865e-05}
Hypervolume: 0.5251318323612213

Iteration: 4
Trial parameters: {&#39;n_layers&#39;: 2.857993047684431, &#39;layer_1_size&#39;: 40.58149953186512, &#39;layer_2_size&#39;: 9.031672313809395, &#39;layer_3_size&#39;: 48.54681473970413, &#39;alpha&#39;: 0.00989952917357636, &#39;tol&#39;: 0.050468348544478736, &#39;beta_1&#39;: 0.3559965386679396, &#39;beta_2&#39;: 0.3077685929397121, &#39;learning_rate_init&#39;: 0.0006366144037249331}
Hypervolume: 0.5251318323612213

Iteration: 5
Trial parameters: {&#39;n_layers&#39;: 1.097474630922079, &#39;layer_1_size&#39;: 11.42294529080391, &#39;layer_2_size&#39;: 34.01442900300026, &#39;layer_3_size&#39;: 6.8614387065172195, &#39;alpha&#39;: 1.0593306242400134e-05, &#39;tol&#39;: 0.0003891914630408717, &#39;beta_1&#39;: 0.7273618404753506, &#39;beta_2&#39;: 0.7034814686570316, &#39;learning_rate_init&#39;: 0.00016165942910395683}
Hypervolume: 0.565696746110916

Iteration: 6
Trial parameters: {&#39;n_layers&#39;: 1.9232655111700296, &#39;layer_1_size&#39;: 28.873416408896446, &#39;layer_2_size&#39;: 18.251539155840874, &#39;layer_3_size&#39;: 35.46791999042034, &#39;alpha&#39;: 6.914495897549351e-05, &#39;tol&#39;: 8.423367861632198e-05, &#39;beta_1&#39;: 0.14364598613139243, &#39;beta_2&#39;: 0.8463880526311696, &#39;learning_rate_init&#39;: 6.308985444824775e-06}
Hypervolume: 0.565696746110916

Iteration: 7
Trial parameters: {&#39;n_layers&#39;: 2.183751503005624, &#39;layer_1_size&#39;: 23.157306730747223, &#39;layer_2_size&#39;: 42.794464349746704, &#39;layer_3_size&#39;: 17.68869659304619, &#39;alpha&#39;: 0.07388710481080317, &#39;tol&#39;: 0.006043253202244398, &#39;beta_1&#39;: 0.7545201208256185, &#39;beta_2&#39;: 0.1260992139345035, &#39;learning_rate_init&#39;: 0.015423412289538084}
Hypervolume: 0.565696746110916

Iteration: 8
Trial parameters: {&#39;n_layers&#39;: 2.1074771489948034, &#39;layer_1_size&#39;: 48.96616117656231, &#39;layer_2_size&#39;: 14.466326043009758, &#39;layer_3_size&#39;: 4.4527901113033295, &#39;alpha&#39;: 0.0002834437119808365, &#39;tol&#39;: 0.00137252160113304, &#39;beta_1&#39;: 0.3082725239386782, &#39;beta_2&#39;: 0.32228515935502944, &#39;learning_rate_init&#39;: 0.0006943252577063182}
Hypervolume: 0.565696746110916

Iteration: 9
Trial parameters: {&#39;n_layers&#39;: 1.8469682391732931, &#39;layer_1_size&#39;: 3.120314359664917, &#39;layer_2_size&#39;: 40.486202120780945, &#39;layer_3_size&#39;: 46.39534358680248, &#39;alpha&#39;: 0.29528842601515487, &#39;tol&#39;: 1.0731568719090511e-05, &#39;beta_1&#39;: 0.65170620091632, &#39;beta_2&#39;: 0.6562458675252274, &#39;learning_rate_init&#39;: 0.00015330481131706574}
Hypervolume: 0.565696746110916

Iteration: 10
Trial parameters: {&#39;n_layers&#39;: 1.1737948060035706, &#39;layer_1_size&#39;: 36.67269004881382, &#39;layer_2_size&#39;: 11.246757462620735, &#39;layer_3_size&#39;: 15.375372007489204, &#39;alpha&#39;: 0.001979284411249378, &#39;tol&#39;: 0.0003054157684031657, &#39;beta_1&#39;: 0.18847283466812223, &#39;beta_2&#39;: 0.7683065984277054, &#39;learning_rate_init&#39;: 1.1356257081356907e-05}
Hypervolume: 0.565696746110916

Iteration: 11
Trial parameters: {&#39;n_layers&#39;: 2.9342751055955887, &#39;layer_1_size&#39;: 15.393189162015915, &#39;layer_2_size&#39;: 37.705685406923294, &#39;layer_3_size&#39;: 33.22407811880112, &#39;alpha&#39;: 2.058362883832593e-06, &#39;tol&#39;: 0.02222619409760319, &#39;beta_1&#39;: 0.8330731623731553, &#39;beta_2&#39;: 0.23351558849960566, &#39;learning_rate_init&#39;: 0.008284174248869179}
Hypervolume: 0.565696746110916

Iteration: 12
Trial parameters: {&#39;n_layers&#39;: 2.6371697522699833, &#39;layer_1_size&#39;: 29.65899483859539, &#39;layer_2_size&#39;: 48.25318956375122, &#39;layer_3_size&#39;: 28.330959975719452, &#39;alpha&#39;: 2.5133065415858905e-05, &#39;tol&#39;: 0.003336045580102628, &#39;beta_1&#39;: 0.07156201451923698, &#39;beta_2&#39;: 0.9695085818804801, &#39;learning_rate_init&#39;: 1.8646625123594012e-06}
Hypervolume: 0.565696746110916

Iteration: 13
Trial parameters: {&#39;n_layers&#39;: 1.3761744983494282, &#39;layer_1_size&#39;: 22.254629135131836, &#39;layer_2_size&#39;: 24.794284507632256, &#39;layer_3_size&#39;: 22.57526572048664, &#39;alpha&#39;: 0.023465470180976344, &#39;tol&#39;: 4.2597736046679686e-05, &#39;beta_1&#39;: 0.9499839825220406, &#39;beta_2&#39;: 0.0010001206612214445, &#39;learning_rate_init&#39;: 0.056367098551043195}
Hypervolume: 0.565696746110916

Iteration: 14
Trial parameters: {&#39;n_layers&#39;: 1.581600235775113, &#39;layer_1_size&#39;: 41.2021936327219, &#39;layer_2_size&#39;: 27.03368392586708, &#39;layer_3_size&#39;: 39.15996889770031, &#39;alpha&#39;: 0.17528677283985655, &#39;tol&#39;: 0.000716291520103208, &#39;beta_1&#39;: 0.4232514676665887, &#39;beta_2&#39;: 0.42991929334495216, &#39;learning_rate_init&#39;: 0.0021507987891938697}
Hypervolume: 0.565696746110916

Iteration: 15
Trial parameters: {&#39;n_layers&#39;: 2.3425706941634417, &#39;layer_1_size&#39;: 10.731849640607834, &#39;layer_2_size&#39;: 4.013876512646675, &#39;layer_3_size&#39;: 9.497938722372055, &#39;alpha&#39;: 0.0001639112198265041, &#39;tol&#39;: 0.09825456201970183, &#39;beta_1&#39;: 0.5367272571884095, &#39;beta_2&#39;: 0.5793517326358706, &#39;learning_rate_init&#39;: 4.4172988585201856e-05}
Hypervolume: 0.565696746110916

Iteration: 16
Trial parameters: {&#39;n_layers&#39;: 2.286590538918972, &#39;layer_1_size&#39;: 42.65602780878544, &#39;layer_2_size&#39;: 39.08290049433708, &#39;layer_3_size&#39;: 36.55237178504467, &#39;alpha&#39;: 0.0024440496432114843, &#39;tol&#39;: 3.802152405469434e-05, &#39;beta_1&#39;: 0.9133116063093767, &#39;beta_2&#39;: 0.39607808716315773, &#39;learning_rate_init&#39;: 8.693081858953526e-05}
Hypervolume: 0.565696746110916

Iteration: 17
Trial parameters: {&#39;n_layers&#39;: 1.5392537787556648, &#39;layer_1_size&#39;: 9.371765464544296, &#39;layer_2_size&#39;: 16.05685605108738, &#39;layer_3_size&#39;: 18.796677440404892, &#39;alpha&#39;: 2.5421605533368903e-06, &#39;tol&#39;: 0.004894578727668136, &#39;beta_1&#39;: 0.04642177283391356, &#39;beta_2&#39;: 0.6073230338227004, &#39;learning_rate_init&#39;: 0.001209276916182206}
Hypervolume: 0.565696746110916

Iteration: 18
Trial parameters: {&#39;n_layers&#39;: 1.4946852456778288, &#39;layer_1_size&#39;: 30.92386345565319, &#39;layer_2_size&#39;: 36.29698771238327, &#39;layer_3_size&#39;: 47.37991613149643, &#39;alpha&#39;: 0.0005388461134912532, &#39;tol&#39;: 0.062312958818119046, &#39;beta_1&#39;: 0.5695359376659617, &#39;beta_2&#39;: 0.9435160805135965, &#39;learning_rate_init&#39;: 0.006792144334123189}
Hypervolume: 0.565696746110916

Iteration: 19
Trial parameters: {&#39;n_layers&#39;: 2.7419857662171125, &#39;layer_1_size&#39;: 21.083510518074036, &#39;layer_2_size&#39;: 12.842854842543602, &#39;layer_3_size&#39;: 5.717886045575142, &#39;alpha&#39;: 0.5612613521875438, &#39;tol&#39;: 0.0008622153383270953, &#39;beta_1&#39;: 0.45225516929104925, &#39;beta_2&#39;: 0.03682012666296214, &#39;learning_rate_init&#39;: 1.3985283679902212e-05}
Hypervolume: 0.565696746110916

Iteration: 20
Trial parameters: {&#39;n_layers&#39;: 2.944395938888192, &#39;layer_1_size&#39;: 35.21885658800602, &#39;layer_2_size&#39;: 23.20301939547062, &#39;layer_3_size&#39;: 12.732843235135078, &#39;alpha&#39;: 0.03834346815251047, &#39;tol&#39;: 1.5781659899747944e-05, &#39;beta_1&#39;: 0.6883760404800996, &#39;beta_2&#39;: 0.803960596694611, &#39;learning_rate_init&#39;: 0.09970206921368036}
Hypervolume: 0.565696746110916

Iteration: 21
Trial parameters: {&#39;n_layers&#39;: 1.1975493971258402, &#39;layer_1_size&#39;: 16.753272622823715, &#39;layer_2_size&#39;: 49.65722092986107, &#39;layer_3_size&#39;: 42.675396621227264, &#39;alpha&#39;: 3.586355931679589e-05, &#39;tol&#39;: 0.0012225823876705346, &#39;beta_1&#39;: 0.3334150664769113, &#39;beta_2&#39;: 0.20735709413513542, &#39;learning_rate_init&#39;: 1.067797218000312e-06}
Hypervolume: 0.565696746110916

Iteration: 22
Trial parameters: {&#39;n_layers&#39;: 1.774377878755331, &#39;layer_1_size&#39;: 47.70129184424877, &#39;layer_2_size&#39;: 2.4170436710119247, &#39;layer_3_size&#39;: 23.653959840536118, &#39;alpha&#39;: 8.471752785869621e-06, &#39;tol&#39;: 0.02682008230380691, &#39;beta_1&#39;: 0.8002666611662135, &#39;beta_2&#39;: 0.3503319078851491, &#39;learning_rate_init&#39;: 5.3715429215193894e-05}
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Hypervolume: 0.565696746110916

Iteration: 23
Trial parameters: {&#39;n_layers&#39;: 2.021191995590925, &#39;layer_1_size&#39;: 4.291433691978455, &#39;layer_2_size&#39;: 28.44311100244522, &#39;layer_3_size&#39;: 29.502666041254997, &#39;alpha&#39;: 0.007907780625795788, &#39;tol&#39;: 0.00019327251871192847, &#39;beta_1&#39;: 0.15946671797707676, &#39;beta_2&#39;: 0.6224801234239712, &#39;learning_rate_init&#39;: 0.0017510817195338577}
Hypervolume: 0.565696746110916

Iteration: 24
Trial parameters: {&#39;n_layers&#39;: 2.1949023753404617, &#39;layer_1_size&#39;: 27.232035741209984, &#39;layer_2_size&#39;: 6.295709237456322, &#39;layer_3_size&#39;: 40.266760274767876, &#39;alpha&#39;: 1.0724075626030375e-06, &#39;tol&#39;: 0.0004311551282011866, &#39;beta_1&#39;: 0.6404688712274655, &#39;beta_2&#39;: 0.8184167514443398, &#39;learning_rate_init&#39;: 0.024931592714360348}
Hypervolume: 0.565696746110916

Iteration: 25
Trial parameters: {&#39;n_layers&#39;: 1.9480653405189514, &#39;layer_1_size&#39;: 24.704937398433685, &#39;layer_2_size&#39;: 30.84560388326645, &#39;layer_3_size&#39;: 10.581384152173996, &#39;alpha&#39;: 0.0010302343098922426, &#39;tol&#39;: 0.0335580919014286, &#39;beta_1&#39;: 0.257452125813812, &#39;beta_2&#39;: 0.1599404201162979, &#39;learning_rate_init&#39;: 4.1286346571670175e-06}
Hypervolume: 0.565696746110916

Iteration: 26
Trial parameters: {&#39;n_layers&#39;: 1.0238543208688498, &#39;layer_1_size&#39;: 38.75408299267292, &#39;layer_2_size&#39;: 21.081981793045998, &#39;layer_3_size&#39;: 27.189330637454987, &#39;alpha&#39;: 0.23678647887404494, &#39;tol&#39;: 0.009767676394539519, &#39;beta_1&#39;: 0.8452763105323539, &#39;beta_2&#39;: 0.2719485869379714, &#39;learning_rate_init&#39;: 2.37155539481684e-05}
Hypervolume: 0.565696746110916

Iteration: 27
Trial parameters: {&#39;n_layers&#39;: 2.770662745460868, &#39;layer_1_size&#39;: 13.156611829996109, &#39;layer_2_size&#39;: 46.059233874082565, &#39;layer_3_size&#39;: 21.4101073294878, &#39;alpha&#39;: 0.00022712025764174622, &#39;tol&#39;: 7.076833041549904e-05, &#39;beta_1&#39;: 0.23832741432264448, &#39;beta_2&#39;: 0.7294739700239151, &#39;learning_rate_init&#39;: 0.004102348130371032}
Hypervolume: 0.565696746110916

Iteration: 28
Trial parameters: {&#39;n_layers&#39;: 2.537033947184682, &#39;layer_1_size&#39;: 46.893009051680565, &#39;layer_2_size&#39;: 32.41795665025711, &#39;layer_3_size&#39;: 16.517002061009407, &#39;alpha&#39;: 8.507434502950944e-05, &#39;tol&#39;: 0.00010431240734320164, &#39;beta_1&#39;: 0.9602550184642896, &#39;beta_2&#39;: 0.4725278516430408, &#39;learning_rate_init&#39;: 0.000285554383538666}
Hypervolume: 0.565696746110916

Iteration: 29
Trial parameters: {&#39;n_layers&#39;: 1.2897105682641268, &#39;layer_1_size&#39;: 5.169940650463104, &#39;layer_2_size&#39;: 10.440727457404137, &#39;layer_3_size&#39;: 34.389237225055695, &#39;alpha&#39;: 0.09089268993566091, &#39;tol&#39;: 0.013378544614626753, &#39;beta_1&#39;: 0.12334870639070869, &#39;beta_2&#39;: 0.4983956897044554, &#39;learning_rate_init&#39;: 0.00036403217603283997}
Hypervolume: 0.5795015656948088

Iteration: 30
Trial parameters: {&#39;n_layers&#39;: 1.744220782071352, &#39;layer_1_size&#39;: 34.6208668500185, &#39;layer_2_size&#39;: 41.204303592443466, &#39;layer_3_size&#39;: 3.345999449491501, &#39;alpha&#39;: 0.018761085047039116, &#39;tol&#39;: 0.0022801930224322915, &#39;beta_1&#39;: 0.5235587584665045, &#39;beta_2&#39;: 0.9271263692965731, &#39;learning_rate_init&#39;: 0.01872985515608467}
Hypervolume: 0.5795015656948088

Iteration: 31
Trial parameters: {&#39;n_layers&#39;: 2.4915193654596806, &#39;layer_1_size&#39;: 17.468538910150528, &#39;layer_2_size&#39;: 19.65447761118412, &#39;layer_3_size&#39;: 45.31189887225628, &#39;alpha&#39;: 2.0079492125512588e-05, &#39;tol&#39;: 3.14211810765807e-05, &#39;beta_1&#39;: 0.3743622385747731, &#39;beta_2&#39;: 0.08230330316349864, &#39;learning_rate_init&#39;: 5.129068959110631e-06}
Hypervolume: 0.5795015656948088

Iteration: 32
Trial parameters: {&#39;n_layers&#39;: 2.4430059492588043, &#39;layer_1_size&#39;: 44.22796009480953, &#39;layer_2_size&#39;: 12.068569138646126, &#39;layer_3_size&#39;: 29.762721717357635, &#39;alpha&#39;: 0.0523906672082365, &#39;tol&#39;: 5.859004632964501e-05, &#39;beta_1&#39;: 0.5771822051890194, &#39;beta_2&#39;: 0.1737382767163217, &#39;learning_rate_init&#39;: 0.0009578864307476489}
Hypervolume: 0.5795015656948088

Iteration: 33
Trial parameters: {&#39;n_layers&#39;: 1.6961918324232101, &#39;layer_1_size&#39;: 7.755968153476715, &#39;layer_2_size&#39;: 36.97771528363228, &#39;layer_3_size&#39;: 24.080916240811348, &#39;alpha&#39;: 5.0324881740519216e-05, &#39;tol&#39;: 0.0075321843096224795, &#39;beta_1&#39;: 0.4443599012186751, &#39;beta_2&#39;: 0.8278080878695473, &#39;learning_rate_init&#39;: 9.930633133824226e-05}
Hypervolume: 0.5795015656948088

Iteration: 34
Trial parameters: {&#39;n_layers&#39;: 1.2757354285567999, &#39;layer_1_size&#39;: 32.68009229004383, &#39;layer_2_size&#39;: 15.241194054484367, &#39;layer_3_size&#39;: 43.683895632624626, &#39;alpha&#39;: 1.1571425387656801e-05, &#39;tol&#39;: 0.04050504590951469, &#39;beta_1&#39;: 0.9056580309532583, &#39;beta_2&#39;: 0.71573834513966, &#39;learning_rate_init&#39;: 1.7681350240671893e-05}
Hypervolume: 0.5795015656948088

Iteration: 35
Trial parameters: {&#39;n_layers&#39;: 2.522581970319152, &#39;layer_1_size&#39;: 19.277374535799026, &#39;layer_2_size&#39;: 39.80499255657196, &#39;layer_3_size&#39;: 13.908425897359848, &#39;alpha&#39;: 0.01110025455846956, &#39;tol&#39;: 0.000559351811264088, &#39;beta_1&#39;: 0.05430925779696554, &#39;beta_2&#39;: 0.2624988210070878, &#39;learning_rate_init&#39;: 0.005953835119370998}
Hypervolume: 0.5795015656948088

Iteration: 36
Trial parameters: {&#39;n_layers&#39;: 2.7889217901974916, &#39;layer_1_size&#39;: 27.64718697965145, &#39;layer_2_size&#39;: 26.25874724984169, &#39;layer_3_size&#39;: 6.0160273015499115, &#39;alpha&#39;: 0.0041299627852019825, &#39;tol&#39;: 2.421801953171003e-05, &#39;beta_1&#39;: 0.7772478979714215, &#39;beta_2&#39;: 0.5155059381555765, &#39;learning_rate_init&#39;: 1.19762177494387e-06}
Hypervolume: 0.5795015656948088

Iteration: 37
Trial parameters: {&#39;n_layers&#39;: 1.041621269658208, &#39;layer_1_size&#39;: 24.368967831134796, &#39;layer_2_size&#39;: 4.694971635937691, &#39;layer_3_size&#39;: 47.88597373664379, &#39;alpha&#39;: 4.411935685965514e-06, &#39;tol&#39;: 0.0018892622617045027, &#39;beta_1&#39;: 0.18271939077880234, &#39;beta_2&#39;: 0.494044682902284, &#39;learning_rate_init&#39;: 0.07842919840096199}
Hypervolume: 0.5910428380966187

Iteration: 38
Trial parameters: {&#39;n_layers&#39;: 1.9923177398741245, &#39;layer_1_size&#39;: 39.94477815926075, &#39;layer_2_size&#39;: 47.43144655227661, &#39;layer_3_size&#39;: 19.843262746930122, &#39;alpha&#39;: 0.0009102803417348881, &#39;tol&#39;: 0.017350328786851027, &#39;beta_1&#39;: 0.7113870242498815, &#39;beta_2&#39;: 0.06513453678321093, &#39;learning_rate_init&#39;: 0.001559010609757258}
Hypervolume: 0.5910428380966187

Iteration: 39
Trial parameters: {&#39;n_layers&#39;: 2.2396545000374317, &#39;layer_1_size&#39;: 12.097649604082108, &#39;layer_2_size&#39;: 25.52236907184124, &#39;layer_3_size&#39;: 37.80705636739731, &#39;alpha&#39;: 0.9743546046607365, &#39;tol&#39;: 0.00012598604539900017, &#39;beta_1&#39;: 0.310155082157813, &#39;beta_2&#39;: 0.9056717992275953, &#39;learning_rate_init&#39;: 6.819239691802227e-05}
Hypervolume: 0.5910428380966187

Iteration: 40
Trial parameters: {&#39;n_layers&#39;: 2.038333334028721, &#39;layer_1_size&#39;: 37.89460505545139, &#39;layer_2_size&#39;: 45.24028170108795, &#39;layer_3_size&#39;: 44.04705220460892, &#39;alpha&#39;: 0.00011940112551049051, &#39;tol&#39;: 0.0008865155378612799, &#39;beta_1&#39;: 0.8529244644232095, &#39;beta_2&#39;: 0.5935855520451442, &#39;learning_rate_init&#39;: 3.3586831867464384e-06}
Hypervolume: 0.5910428380966187

Iteration: 41
Trial parameters: {&#39;n_layers&#39;: 1.7910347506403923, &#39;layer_1_size&#39;: 14.18014332652092, &#39;layer_2_size&#39;: 21.807375475764275, &#39;layer_3_size&#39;: 2.2949285060167313, &#39;alpha&#39;: 0.12418150721771991, &#39;tol&#39;: 0.06882473103199008, &#39;beta_1&#39;: 0.23043025988247245, &#39;beta_2&#39;: 0.3866264053899795, &#39;learning_rate_init&#39;: 0.028925711278995004}
Hypervolume: 0.5910428380966187

Iteration: 42
Trial parameters: {&#39;n_layers&#39;: 1.2428966145962477, &#39;layer_1_size&#39;: 49.60717408359051, &#39;layer_2_size&#39;: 30.067284137010574, &#39;layer_3_size&#39;: 33.875763580203056, &#39;alpha&#39;: 0.026333095369846498, &#39;tol&#39;: 0.004763780740938282, &#39;beta_1&#39;: 0.6328134095035494, &#39;beta_2&#39;: 0.05061612636968493, &#39;learning_rate_init&#39;: 0.005035441104107058}
Hypervolume: 0.5910428380966187

Iteration: 43
Trial parameters: {&#39;n_layers&#39;: 2.990219993516803, &#39;layer_1_size&#39;: 2.446971893310547, &#39;layer_2_size&#39;: 6.9804594069719315, &#39;layer_3_size&#39;: 16.217487066984177, &#39;alpha&#39;: 2.743130953327706e-05, &#39;tol&#39;: 3.440950211744576e-05, &#39;beta_1&#39;: 0.2653414971446618, &#39;beta_2&#39;: 0.9529055010965094, &#39;learning_rate_init&#39;: 2.0412969362428316e-05}
Hypervolume: 0.5910428380966187

Iteration: 44
Trial parameters: {&#39;n_layers&#39;: 2.692598784342408, &#39;layer_1_size&#39;: 42.23033584654331, &#39;layer_2_size&#39;: 17.432725831866264, &#39;layer_3_size&#39;: 20.23022124171257, &#39;alpha&#39;: 1.8614148014067335e-06, &#39;tol&#39;: 0.00021357470333493577, &#39;beta_1&#39;: 0.5005418816395104, &#39;beta_2&#39;: 0.1901864708615467, &#39;learning_rate_init&#39;: 0.0004205372248552947}
Hypervolume: 0.5910428380966187

Iteration: 45
Trial parameters: {&#39;n_layers&#39;: 1.44579035975039, &#39;layer_1_size&#39;: 9.694998472929, &#39;layer_2_size&#39;: 43.51961922645569, &#39;layer_3_size&#39;: 26.17048589885235, &#39;alpha&#39;: 0.0017408474425489734, &#39;tol&#39;: 0.02755465438794672, &#39;beta_1&#39;: 0.39761302500870077, &#39;beta_2&#39;: 0.7825041107833386, &#39;learning_rate_init&#39;: 0.00022875807990824186}
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Hypervolume: 0.5910428380966187

Iteration: 46
Trial parameters: {&#39;n_layers&#39;: 1.5261293165385723, &#39;layer_1_size&#39;: 29.767856046557426, &#39;layer_2_size&#39;: 8.259665325284004, &#39;layer_3_size&#39;: 10.152871802449226, &#39;alpha&#39;: 0.4110527837427628, &#39;tol&#39;: 0.0011068267778721252, &#39;beta_1&#39;: 0.9832641158662736, &#39;beta_2&#39;: 0.6395885149817914, &#39;learning_rate_init&#39;: 4.446315850269228e-06}
Hypervolume: 0.5910428380966187

Iteration: 47
Trial parameters: {&#39;n_layers&#39;: 2.2729663513600826, &#39;layer_1_size&#39;: 22.177891492843628, &#39;layer_2_size&#39;: 34.692594438791275, &#39;layer_3_size&#39;: 39.999472200870514, &#39;alpha&#39;: 0.0003837277239000288, &#39;tol&#39;: 1.535021701862983e-05, &#39;beta_1&#39;: 0.10009060843940824, &#39;beta_2&#39;: 0.3718468233020976, &#39;learning_rate_init&#39;: 0.023411880438750914}
Hypervolume: 0.5910428380966187

Iteration: 48
Trial parameters: {&#39;n_layers&#39;: 2.3556875344365835, &#39;layer_1_size&#39;: 31.59673671424389, &#39;layer_2_size&#39;: 20.21629346907139, &#39;layer_3_size&#39;: 17.951682046055794, &#39;alpha&#39;: 0.005838667261995344, &#39;tol&#39;: 0.018739333334087626, &#39;beta_1&#39;: 0.4922941584549844, &#39;beta_2&#39;: 0.6656295829508454, &#39;learning_rate_init&#39;: 0.035180786304251116}
Hypervolume: 0.5910428380966187

Iteration: 49
Trial parameters: {&#39;n_layers&#39;: 1.5952015426009893, &#39;layer_1_size&#39;: 20.442760825157166, &#39;layer_2_size&#39;: 46.64376375079155, &#39;layer_3_size&#39;: 35.891946613788605, &#39;alpha&#39;: 6.083316753057435e-06, &#39;tol&#39;: 0.00023980834827542975, &#39;beta_1&#39;: 0.5912981302952394, &#39;beta_2&#39;: 0.3360753986397758, &#39;learning_rate_init&#39;: 2.7349772949846056e-06}
Hypervolume: 0.5910428380966187
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="visualize-results">
<h3>Visualize Results<a class="headerlink" href="#visualize-results" title="Permalink to this headline">¶</a></h3>
<p>We can visually compare the results of qEHVI (we only proposed and evaluated one point at a time, so q=1). We want to see a Pareto frontier of points in the lower right corner of the chart - points with the highest accuracy and lowest latency possible.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">sharey</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">mobo_outcomes</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">mobo_outcomes</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;MOBO&quot;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">random_outcomes</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">random_outcomes</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;SOBOL&quot;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Comparison&quot;</span><span class="p">)</span>

<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">mobo_outcomes</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">mobo_outcomes</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">mobo_outcomes</span><span class="p">[:,</span><span class="mi">0</span><span class="p">])))</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;qEHVI&quot;</span><span class="p">)</span>

<span class="n">axs</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">random_outcomes</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">random_outcomes</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">random_outcomes</span><span class="p">[:,</span><span class="mi">0</span><span class="p">])))</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Sobol&quot;</span><span class="p">)</span>

<span class="k">for</span> <span class="n">ax</span> <span class="ow">in</span> <span class="n">axs</span><span class="o">.</span><span class="n">flat</span><span class="p">:</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">xlabel</span><span class="o">=</span><span class="s2">&quot;AUC&quot;</span><span class="p">,</span> <span class="n">ylabel</span><span class="o">=</span><span class="s2">&quot;Latency (1 million points)&quot;</span><span class="p">)</span>

<span class="n">fig</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/T892775_Exploring_Multi_Objective_Hyperparameter_Optimization_52_0.png" src="_images/T892775_Exploring_Multi_Objective_Hyperparameter_Optimization_52_0.png" />
</div>
</div>
<p>The leftmost panel shows a comparison of the two experiments, while the middle and right panels show each experiment individually, with their points colored according to the order in which the trials were run. Notice in the qEHVI panel that earlier trials (purple points) are scattered about (reflecting the initialization phase), while later trials (yellow points) have honed in on the Pareto frontier. The same cannot be said for the Sobol experiment, which samples randomly for all trials.</p>
<p>Because we generated the dataset randomly, and initialized the sampling randomly, it’s likely your results differ from ours (we could of course have mitigated this by fixing seeds for the random number generators, but we’re very wary of cherry-picking good results, and wanted to encourage some exploration)! Here’s one of the more impressive results we obtained, specifically chosen for its spectacular separation of the qEHVI trials and the Sobol trials. Multi-objective Bayesian optimization (MOBO = qEHVI) found a host of models that are faster <em>and</em> more accurate than Sobol-random search found!</p>
<p><strong>A pretty great result for qEHVI</strong></p>
<p><center><img src='https://github.com/sparsh-ai/multiobjective-optimizations/blob/main/docs/_images/T892775_1.png?raw=1'></center></p><p>Alas, it’s not always the case. In this example, random search actually found the highest AUC model, though it does trade off quite some latency for it.</p>
<p><strong>A pretty good result for random</strong></p>
<p><center><img src='https://github.com/sparsh-ai/multiobjective-optimizations/blob/main/docs/_images/T892775_2.png?raw=1'></center></p><p>The focus on maximizing the expected hypervolume improvement can lead to a lack of exploration of the wider parts of the hyperparameter space, and sometimes randomly sampling finds better points. Try running the experiments a few times and see what you get!</p>
<p>Ax also provides some visualization helpers, including a handy one to construct the Pareto frontier. The plot below shows the Pareto frontier found by the EHVI algorithm. Note that not all the points on the frontier below were sampled during the optimization process. However, since we now have a posterior model of the map between the input hyperparameters and output (AUC, Latency) space, we can calculate the Pareto frontier based on that model. The uncertainty of that posterior model is reflected in the 90% CI intervals on the frontier points.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">init_notebook_plotting</span><span class="p">()</span>

<span class="n">frontier</span> <span class="o">=</span> <span class="n">compute_pareto_frontier</span><span class="p">(</span>
    <span class="n">experiment</span><span class="o">=</span><span class="n">mobo_experiment</span><span class="p">,</span>
    <span class="n">data</span><span class="o">=</span><span class="n">mobo_experiment</span><span class="o">.</span><span class="n">fetch_data</span><span class="p">(),</span>
    <span class="n">primary_objective</span><span class="o">=</span><span class="n">metric_latency</span><span class="p">,</span>
    <span class="n">secondary_objective</span><span class="o">=</span><span class="n">metric_auc</span><span class="p">,</span>
    <span class="n">absolute_metrics</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;latency&quot;</span><span class="p">,</span> <span class="s2">&quot;auc&quot;</span><span class="p">],</span>
    <span class="n">num_points</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">render</span><span class="p">(</span><span class="n">plot_pareto_frontier</span><span class="p">(</span><span class="n">frontier</span><span class="p">,</span> <span class="n">CI_level</span><span class="o">=</span><span class="mf">0.90</span><span class="p">))</span> 
</pre></div>
</div>
</div>
</div>
<p><center><img src='https://github.com/sparsh-ai/multiobjective-optimizations/blob/main/docs/_images/T892775_3.png?raw=1'></center></p><p>Is EHVI worth the implementation complexity vs random search? It’s hard to say from this toy example. It’s further hard, in general, to make blanket statements about optimal methods for hyperparameter search, since each problem and search space is different! It’s also a slightly unfair question, since the Sobol quasi-random search here is better than regular random search, and the latter is much easier to implement - just use scikit-learn.</p>
</div>
</div>
<div class="section" id="multi-objective-hyperparameter-optimization-with-optuna">
<h2>Multi-objective hyperparameter optimization with Optuna<a class="headerlink" href="#multi-objective-hyperparameter-optimization-with-optuna" title="Permalink to this headline">¶</a></h2>
<p>This section walks through a minimal example of multi-objective hyperparameter optimization using the Optuna automatic hyperparameter optimization framework.</p>
<p>First, we need something to learn, so we’ll generate a small, toy classification problem. We’ll define the metrics we care about: the area under the ROC curve, and prediction latency. Then, we’ll use the expected hypervolume improvement (EHVI) to find the Pareto frontier of points that maximize these objectives.</p>
<p>While the official Multi-Objective Optimization with Optuna tutorial provides a fantastic example for PyTorch models, here we demonstrate how to use Optuna with scikit-learn and include a comparison with random sampling for hyperparameter optimization.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pickle</span>
<span class="kn">import</span> <span class="nn">sys</span>
<span class="kn">import</span> <span class="nn">time</span>
<span class="kn">import</span> <span class="nn">warnings</span>

<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">optuna</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_classification</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">roc_auc_score</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.neural_network</span> <span class="kn">import</span> <span class="n">MLPClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.pipeline</span> <span class="kn">import</span> <span class="n">Pipeline</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="id2">
<h3>Data<a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h3>
<p>Let’s generate a classification problem. <code class="docutils literal notranslate"><span class="pre">sklearn</span></code> has a helper for that. The exact parameters aren’t too important! In this section, we really just want to explore how to use Optuna for multi-objective hyperparameter optimization.</p>
<p>First, we must generate some data, and divide it into a train and development set. We’ll train each model on the train set, then</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_classification</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">n_features</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span> <span class="n">n_informative</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>We’ll make an 80/20 split and use the larger portion for training, and the smaller portion for evaluation. If the algorithm we trained was going to be used, we’d want a third split to report our final metrics on. As is, we might risk overfitting the hyperparameters to the dev set across many trials. To avoid tripling (or more) the run time, we’ll skip cross-validation and simply train each algorithm on the train set, and evaluate on dev. We could get a more robust measure of the metrics associated with each hyperparameter configuration by cross-validating each trial.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_dev</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_dev</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">stratify</span><span class="o">=</span><span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="id3">
<h3>Optimization objectives<a class="headerlink" href="#id3" title="Permalink to this headline">¶</a></h3>
<p>Let’s say we care about two objectives: the area under the ROC curve (AUC), and latency. AUC captures the quality of a classifier in a threshold-independent way, and is appropriate for the balanced class distribution we have in our toy problem. By optimizing for this, we’re finding a good classifier without making a decision about how bad each flavour of misclassification (false positives and false negatives) are, whereas optimizing for a point metric like accuracy directly would be making that call. Latency measures how long a prediction takes. As it turns out, with the scikit-learn multi-layer perceptron model we’ll use, not very long. To help smooth out noise in the latency measurement, we’ll measure the time required to predict on a batch of a million points.</p>
<p>This will be a small model, and no GPU is used. If we were using a GPU-enabled framework (like PyTorch, TensorFlow, or Jax), we’d need to be a little more careful about how we measure latency (another option is presented in the official Optuna tutorial). Also note that in most applications, we’d probably be more interested in the total response time of a model end point, or the total processing time for a large batch of offline predictions, and it’s not necessarily the case that the actual model prediction time is the dominant bottleneck to those things at all!</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">million_points</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="mi">1000</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">roc</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">roc_auc_score</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">latency</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
    <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
    <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="n">end</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">end</span> <span class="o">-</span> <span class="n">start</span>
</pre></div>
</div>
</div>
</div>
<p>We define a training routine and compute the objectives. The search space for scikit-learn’s <code class="docutils literal notranslate"><span class="pre">MLPClassifier</span></code> is roughly the same as that in <a class="reference external" href="https://www.amazon.science/publications/multi-objective-multi-fidelity-hyperparameter-optimization-with-application-to-fairness">Multi-objective multi-fidelity hyperparameter optimization with application to fairness</a>. Notice that this is a tricky search space! We allow up to three layers in the network, and we only want to sample a layer size for those layers that exist. This means <code class="docutils literal notranslate"><span class="pre">layer_size</span></code> is dependent on <code class="docutils literal notranslate"><span class="pre">n_layers</span></code>. Thankfully, Optuna naturally lends itself to hierarchical search spaces like this because it dynamically constructs the hyperparameter search space for each trial (however, this comes with a drawback as we’ll see shortly).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">trial</span><span class="p">):</span>    
    <span class="n">hidden_layer_sizes</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">trial</span><span class="o">.</span><span class="n">suggest_int</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;layer_</span><span class="si">{</span><span class="n">layer</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">_size&quot;</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">50</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">trial</span><span class="o">.</span><span class="n">suggest_int</span><span class="p">(</span><span class="s2">&quot;n_layers&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
    <span class="p">]</span>
    
    <span class="n">pipeline</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">([</span>
        <span class="p">(</span><span class="s2">&quot;scaler&quot;</span><span class="p">,</span> <span class="n">StandardScaler</span><span class="p">()),</span>
        <span class="p">(</span><span class="s2">&quot;mlp&quot;</span><span class="p">,</span> <span class="n">MLPClassifier</span><span class="p">(</span>
            <span class="n">solver</span><span class="o">=</span><span class="s2">&quot;adam&quot;</span><span class="p">,</span>
            <span class="n">hidden_layer_sizes</span><span class="o">=</span><span class="n">hidden_layer_sizes</span><span class="p">,</span>
            <span class="n">alpha</span><span class="o">=</span><span class="n">trial</span><span class="o">.</span><span class="n">suggest_float</span><span class="p">(</span><span class="s2">&quot;alpha&quot;</span><span class="p">,</span> <span class="mf">1e-6</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="n">log</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
            <span class="n">tol</span><span class="o">=</span><span class="n">trial</span><span class="o">.</span><span class="n">suggest_float</span><span class="p">(</span><span class="s2">&quot;tol&quot;</span><span class="p">,</span> <span class="mf">1e-5</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="n">log</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
            <span class="n">beta_1</span><span class="o">=</span><span class="n">trial</span><span class="o">.</span><span class="n">suggest_float</span><span class="p">(</span><span class="s2">&quot;beta_1&quot;</span><span class="p">,</span> <span class="mf">0.001</span><span class="p">,</span> <span class="mf">0.99</span><span class="p">),</span>
            <span class="n">beta_2</span><span class="o">=</span><span class="n">trial</span><span class="o">.</span><span class="n">suggest_float</span><span class="p">(</span><span class="s2">&quot;beta_2&quot;</span><span class="p">,</span> <span class="mf">0.001</span><span class="p">,</span> <span class="mf">0.99</span><span class="p">),</span>
            <span class="n">learning_rate_init</span><span class="o">=</span><span class="n">trial</span><span class="o">.</span><span class="n">suggest_float</span><span class="p">(</span><span class="s2">&quot;learning_rate_init&quot;</span><span class="p">,</span> <span class="mf">1e-6</span><span class="p">,</span> <span class="mf">1e-1</span><span class="p">,</span> <span class="n">log</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="p">))</span>
    <span class="p">])</span>
    
    <span class="n">pipeline</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
    <span class="n">auc</span> <span class="o">=</span> <span class="n">roc</span><span class="p">(</span><span class="n">pipeline</span><span class="p">,</span> <span class="n">X_dev</span><span class="p">,</span> <span class="n">y_dev</span><span class="p">)</span>
    <span class="n">lat</span> <span class="o">=</span> <span class="n">latency</span><span class="p">(</span><span class="n">pipeline</span><span class="p">,</span> <span class="n">million_points</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">auc</span><span class="p">,</span> <span class="n">lat</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="id4">
<h3>Experiment setup<a class="headerlink" href="#id4" title="Permalink to this headline">¶</a></h3>
<p>We’ll run two hyperparameter searches: one using the expected hypervolume improvement (EHVI), and another with pseudo-random sampling for comparison. Optuna initializes the EHVI model for generating new hyperparameter configs with a few random points before performing a fixed number of trials, a process that is performed automatically under-the-hood.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">N_TRIAL</span> <span class="o">=</span> <span class="mi">55</span>  <span class="c1"># number of trials to run after initialization</span>
</pre></div>
</div>
</div>
</div>
<p>We’re just about ready to cleverly optimize our hyperparameters. For this experiment we’ll use the MOTPE algorithm detailed in <a class="reference external" href="https://dl.acm.org/doi/10.1145/3377930.3389817">Multiobjective tree-structured parzen estimator for computationally expensive optimization problems</a>. This algorithm uses tree-structured parzen estimators as the surrogate function and EHVI as the acquisition function. We explained how these algorithms work in <a class="reference external" href="https://blog.fastforwardlabs.com/2021/07/07/exploring-multi-objective-hyperparameter-optimization.html">Exploring Multi-Objective Hyperparameter Optimization</a>. In a nutshell, we create a model that takes hyperparameters as inputs and predicts the resultant optimization objectives as outputs. We use this model to find the input hyperparameter combination that will most improve the hypervolume thus expanding the Pareto frontier. Then, we try that hyperparameter config, generating a new data point with which to improve our model, and loop this whole procedure <code class="docutils literal notranslate"><span class="pre">N_TRIAL</span></code> times.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">motpe_sampler</span> <span class="o">=</span> <span class="n">optuna</span><span class="o">.</span><span class="n">samplers</span><span class="o">.</span><span class="n">MOTPESampler</span><span class="p">()</span>
<span class="n">motpe_experiment</span> <span class="o">=</span> <span class="n">optuna</span><span class="o">.</span><span class="n">create_study</span><span class="p">(</span><span class="n">sampler</span><span class="o">=</span><span class="n">motpe_sampler</span><span class="p">,</span> <span class="n">directions</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;maximize&quot;</span><span class="p">,</span> <span class="s2">&quot;minimize&quot;</span><span class="p">])</span>
<span class="n">motpe_experiment</span><span class="o">.</span><span class="n">optimize</span><span class="p">(</span><span class="n">train</span><span class="p">,</span> <span class="n">n_trials</span><span class="o">=</span><span class="n">N_TRIAL</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;ipython-input-8-88b9b9235eb2&gt;:1: ExperimentalWarning: MOTPESampler is experimental (supported from v2.4.0). The interface can change in the future.
  motpe_sampler = optuna.samplers.MOTPESampler()
<span class=" -Color -Color-Green">[I 2021-06-24 13:33:08,291]</span> A new study created in memory with name: no-name-2683aec9-a927-43aa-a061-b67e320433a8
<span class=" -Color -Color-Green">[I 2021-06-24 13:33:09,437]</span> Trial 0 finished with values: [0.8450000000000001, 0.8262290954589844] and parameters: {&#39;n_layers&#39;: 3, &#39;layer_1_size&#39;: 5, &#39;layer_2_size&#39;: 46, &#39;layer_3_size&#39;: 41, &#39;alpha&#39;: 9.16429681464628e-06, &#39;tol&#39;: 4.4073149861688505e-05, &#39;beta_1&#39;: 0.34931254944477413, &#39;beta_2&#39;: 0.9232175410049337, &#39;learning_rate_init&#39;: 0.011713650842477602}. 
<span class=" -Color -Color-Green">[I 2021-06-24 13:33:10,384]</span> Trial 1 finished with values: [0.8999999999999999, 0.8827610015869141] and parameters: {&#39;n_layers&#39;: 3, &#39;layer_1_size&#39;: 47, &#39;layer_2_size&#39;: 30, &#39;layer_3_size&#39;: 33, &#39;alpha&#39;: 0.000497863420997646, &#39;tol&#39;: 0.042431076986822885, &#39;beta_1&#39;: 0.40591916231423836, &#39;beta_2&#39;: 0.409687630011716, &#39;learning_rate_init&#39;: 0.012091492235530358}. 
/opt/anaconda3/envs/hpo/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn&#39;t converged yet.
  warnings.warn(
<span class=" -Color -Color-Green">[I 2021-06-24 13:33:11,313]</span> Trial 2 finished with values: [0.895, 0.48149800300598145] and parameters: {&#39;n_layers&#39;: 2, &#39;layer_1_size&#39;: 30, &#39;layer_2_size&#39;: 3, &#39;alpha&#39;: 3.038276168745543e-05, &#39;tol&#39;: 3.1396224961989394e-05, &#39;beta_1&#39;: 0.5517188842437748, &#39;beta_2&#39;: 0.006529385036930512, &#39;learning_rate_init&#39;: 0.00041299273929972354}. 
<span class=" -Color -Color-Green">[I 2021-06-24 13:33:11,688]</span> Trial 3 finished with values: [0.47500000000000003, 0.35079503059387207] and parameters: {&#39;n_layers&#39;: 1, &#39;layer_1_size&#39;: 10, &#39;alpha&#39;: 0.007869345652559853, &#39;tol&#39;: 5.497102524380759e-05, &#39;beta_1&#39;: 0.25209991704021734, &#39;beta_2&#39;: 0.10238799967205169, &#39;learning_rate_init&#39;: 1.3432997566445058e-06}. 
/opt/anaconda3/envs/hpo/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn&#39;t converged yet.
  warnings.warn(
<span class=" -Color -Color-Green">[I 2021-06-24 13:33:13,005]</span> Trial 4 finished with values: [0.7350000000000001, 0.6523792743682861] and parameters: {&#39;n_layers&#39;: 3, &#39;layer_1_size&#39;: 23, &#39;layer_2_size&#39;: 24, &#39;layer_3_size&#39;: 2, &#39;alpha&#39;: 0.00013963220952780344, &#39;tol&#39;: 2.1064664725154894e-05, &#39;beta_1&#39;: 0.9683884641443996, &#39;beta_2&#39;: 0.7395920927717222, &#39;learning_rate_init&#39;: 0.00010157408120893174}. 
<span class=" -Color -Color-Green">[I 2021-06-24 13:33:13,674]</span> Trial 5 finished with values: [0.7050000000000001, 0.6269810199737549] and parameters: {&#39;n_layers&#39;: 3, &#39;layer_1_size&#39;: 3, &#39;layer_2_size&#39;: 4, &#39;layer_3_size&#39;: 39, &#39;alpha&#39;: 2.6574599948815355e-05, &#39;tol&#39;: 0.09692403417909953, &#39;beta_1&#39;: 0.6617988223501713, &#39;beta_2&#39;: 0.7718478809362541, &#39;learning_rate_init&#39;: 0.0038072027111208037}. 
<span class=" -Color -Color-Green">[I 2021-06-24 13:33:14,526]</span> Trial 6 finished with values: [0.905, 0.8008348941802979] and parameters: {&#39;n_layers&#39;: 3, &#39;layer_1_size&#39;: 28, &#39;layer_2_size&#39;: 20, &#39;layer_3_size&#39;: 41, &#39;alpha&#39;: 0.02604155204819618, &#39;tol&#39;: 0.2150534277233646, &#39;beta_1&#39;: 0.6584798756571555, &#39;beta_2&#39;: 0.1763795518854328, &#39;learning_rate_init&#39;: 0.07963358328379677}. 
<span class=" -Color -Color-Green">[I 2021-06-24 13:33:16,180]</span> Trial 7 finished with values: [0.885, 0.8790798187255859] and parameters: {&#39;n_layers&#39;: 3, &#39;layer_1_size&#39;: 29, &#39;layer_2_size&#39;: 35, &#39;layer_3_size&#39;: 46, &#39;alpha&#39;: 4.543994517425308e-06, &#39;tol&#39;: 0.00011916514693828936, &#39;beta_1&#39;: 0.919988728094854, &#39;beta_2&#39;: 0.7743257374553468, &#39;learning_rate_init&#39;: 0.0003861316703292381}. 
<span class=" -Color -Color-Green">[I 2021-06-24 13:33:16,977]</span> Trial 8 finished with values: [0.915, 0.7575907707214355] and parameters: {&#39;n_layers&#39;: 2, &#39;layer_1_size&#39;: 38, &#39;layer_2_size&#39;: 45, &#39;alpha&#39;: 0.013560034935590921, &#39;tol&#39;: 0.6685371542516644, &#39;beta_1&#39;: 0.128661020423243, &#39;beta_2&#39;: 0.7201105780751946, &#39;learning_rate_init&#39;: 0.014071036028619628}. 
<span class=" -Color -Color-Green">[I 2021-06-24 13:33:17,654]</span> Trial 9 finished with values: [0.915, 0.6342971324920654] and parameters: {&#39;n_layers&#39;: 2, &#39;layer_1_size&#39;: 39, &#39;layer_2_size&#39;: 22, &#39;alpha&#39;: 0.17594677971231898, &#39;tol&#39;: 0.07035694001328437, &#39;beta_1&#39;: 0.36351333637023076, &#39;beta_2&#39;: 0.4178716102505961, &#39;learning_rate_init&#39;: 0.06040958609329022}. 
<span class=" -Color -Color-Green">[I 2021-06-24 13:33:18,097]</span> Trial 10 finished with values: [0.52, 0.39836716651916504] and parameters: {&#39;n_layers&#39;: 1, &#39;layer_1_size&#39;: 19, &#39;alpha&#39;: 1.3259685116626232e-06, &#39;tol&#39;: 0.0011323507429594967, &#39;beta_1&#39;: 0.655247094910459, &#39;beta_2&#39;: 0.24771477428955968, &#39;learning_rate_init&#39;: 2.706540462517576e-06}. 
<span class=" -Color -Color-Green">[I 2021-06-24 13:33:18,696]</span> Trial 11 finished with values: [0.6000000000000001, 0.5507080554962158] and parameters: {&#39;n_layers&#39;: 1, &#39;layer_1_size&#39;: 45, &#39;alpha&#39;: 0.00011268199542449281, &#39;tol&#39;: 0.0008452694354531599, &#39;beta_1&#39;: 0.013412735810499676, &#39;beta_2&#39;: 0.0017106012447751428, &#39;learning_rate_init&#39;: 2.0613948526760165e-05}. 
<span class=" -Color -Color-Green">[I 2021-06-24 13:33:19,184]</span> Trial 12 finished with values: [0.5399999999999999, 0.43607306480407715] and parameters: {&#39;n_layers&#39;: 2, &#39;layer_1_size&#39;: 15, &#39;layer_2_size&#39;: 7, &#39;alpha&#39;: 0.0013069810733212196, &#39;tol&#39;: 0.012218925536263683, &#39;beta_1&#39;: 0.8113858645642817, &#39;beta_2&#39;: 0.005957308012763156, &#39;learning_rate_init&#39;: 0.000286682371770664}. 
/opt/anaconda3/envs/hpo/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn&#39;t converged yet.
  warnings.warn(
<span class=" -Color -Color-Green">[I 2021-06-24 13:33:20,086]</span> Trial 13 finished with values: [0.515, 0.47922420501708984] and parameters: {&#39;n_layers&#39;: 1, &#39;layer_1_size&#39;: 34, &#39;alpha&#39;: 0.8519578187834544, &#39;tol&#39;: 1.1180861407790612e-05, &#39;beta_1&#39;: 0.5032659400449669, &#39;beta_2&#39;: 0.5439709434541915, &#39;learning_rate_init&#39;: 1.227166430077063e-05}. 
<span class=" -Color -Color-Green">[I 2021-06-24 13:33:20,987]</span> Trial 14 finished with values: [0.885, 0.44770359992980957] and parameters: {&#39;n_layers&#39;: 2, &#39;layer_1_size&#39;: 14, &#39;layer_2_size&#39;: 12, &#39;alpha&#39;: 1.0070144695138192e-06, &#39;tol&#39;: 0.00021423679915695925, &#39;beta_1&#39;: 0.02080658390365603, &#39;beta_2&#39;: 0.3079751855846036, &#39;learning_rate_init&#39;: 0.001335813224383925}. 
<span class=" -Color -Color-Green">[I 2021-06-24 13:33:21,657]</span> Trial 15 finished with values: [0.9049999999999999, 0.36991095542907715] and parameters: {&#39;n_layers&#39;: 1, &#39;layer_1_size&#39;: 12, &#39;alpha&#39;: 1.2419811704953315e-06, &#39;tol&#39;: 0.00025782422258097115, &#39;beta_1&#39;: 0.04915900647234839, &#39;beta_2&#39;: 0.2638688161574923, &#39;learning_rate_init&#39;: 0.003092165834130955}. 
<span class=" -Color -Color-Green">[I 2021-06-24 13:33:22,093]</span> Trial 16 finished with values: [0.8499999999999999, 0.35782790184020996] and parameters: {&#39;n_layers&#39;: 1, &#39;layer_1_size&#39;: 9, &#39;alpha&#39;: 0.0014466206582471482, &#39;tol&#39;: 0.008382379738575741, &#39;beta_1&#39;: 0.19066504937732265, &#39;beta_2&#39;: 0.5436387490435325, &#39;learning_rate_init&#39;: 0.0018505858034919347}. 
/opt/anaconda3/envs/hpo/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn&#39;t converged yet.
  warnings.warn(
<span class=" -Color -Color-Green">[I 2021-06-24 13:33:22,881]</span> Trial 17 finished with values: [0.7300000000000001, 0.41161584854125977] and parameters: {&#39;n_layers&#39;: 1, &#39;layer_1_size&#39;: 20, &#39;alpha&#39;: 3.412151537229594e-06, &#39;tol&#39;: 0.0005601470062129255, &#39;beta_1&#39;: 0.03360345505593364, &#39;beta_2&#39;: 0.32346359272864006, &#39;learning_rate_init&#39;: 7.024964062852993e-05}. 
<span class=" -Color -Color-Green">[I 2021-06-24 13:33:23,254]</span> Trial 18 finished with values: [0.875, 0.3131232261657715] and parameters: {&#39;n_layers&#39;: 1, &#39;layer_1_size&#39;: 2, &#39;alpha&#39;: 0.10160335653501168, &#39;tol&#39;: 0.003295747559005705, &#39;beta_1&#39;: 0.12973937149022546, &#39;beta_2&#39;: 0.12144622926630172, &#39;learning_rate_init&#39;: 0.03764977876147358}. 
<span class=" -Color -Color-Green">[I 2021-06-24 13:33:23,907]</span> Trial 19 finished with values: [0.9049999999999999, 0.3615438938140869] and parameters: {&#39;n_layers&#39;: 1, &#39;layer_1_size&#39;: 8, &#39;alpha&#39;: 0.00021230857347505363, &#39;tol&#39;: 0.00020666539188587117, &#39;beta_1&#39;: 0.27026240461197854, &#39;beta_2&#39;: 0.6340047740691164, &#39;learning_rate_init&#39;: 0.0036611758781709856}. 
<span class=" -Color -Color-Green">[I 2021-06-24 13:33:24,297]</span> Trial 20 finished with values: [0.875, 0.3256676197052002] and parameters: {&#39;n_layers&#39;: 1, &#39;layer_1_size&#39;: 2, &#39;alpha&#39;: 0.09688235880484591, &#39;tol&#39;: 0.003715321559537017, &#39;beta_1&#39;: 0.26116708187883253, &#39;beta_2&#39;: 0.9873796201507512, &#39;learning_rate_init&#39;: 0.045297031815301134}. 
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span><span class=" -Color -Color-Green">[I 2021-06-24 13:33:24,682]</span> Trial 21 finished with values: [0.8900000000000001, 0.34682798385620117] and parameters: {&#39;n_layers&#39;: 1, &#39;layer_1_size&#39;: 7, &#39;alpha&#39;: 0.4220468928331617, &#39;tol&#39;: 0.9205855947906584, &#39;beta_1&#39;: 0.1355045696452603, &#39;beta_2&#39;: 0.12358877798741968, &#39;learning_rate_init&#39;: 0.026215233749186093}. 
<span class=" -Color -Color-Green">[I 2021-06-24 13:33:25,363]</span> Trial 22 finished with values: [0.885, 0.5584681034088135] and parameters: {&#39;n_layers&#39;: 2, &#39;layer_1_size&#39;: 2, &#39;layer_2_size&#39;: 37, &#39;alpha&#39;: 0.05927541209993249, &#39;tol&#39;: 0.0029859542692567418, &#39;beta_1&#39;: 0.276718698028048, &#39;beta_2&#39;: 0.6467856636383467, &#39;learning_rate_init&#39;: 0.005313774791799439}. 
<span class=" -Color -Color-Green">[I 2021-06-24 13:33:25,820]</span> Trial 23 finished with values: [0.8949999999999999, 0.39627814292907715] and parameters: {&#39;n_layers&#39;: 1, &#39;layer_1_size&#39;: 17, &#39;alpha&#39;: 0.003966062336361071, &#39;tol&#39;: 0.022560292596133607, &#39;beta_1&#39;: 0.14092242274995986, &#39;beta_2&#39;: 0.8719943100057457, &#39;learning_rate_init&#39;: 0.0773444843114371}. 
<span class=" -Color -Color-Green">[I 2021-06-24 13:33:26,580]</span> Trial 24 finished with values: [0.6250000000000001, 0.6833100318908691] and parameters: {&#39;n_layers&#39;: 2, &#39;layer_1_size&#39;: 8, &#39;layer_2_size&#39;: 50, &#39;alpha&#39;: 0.7108676700695464, &#39;tol&#39;: 0.2268470093354554, &#39;beta_1&#39;: 0.46008802111568425, &#39;beta_2&#39;: 0.41425616974036406, &#39;learning_rate_init&#39;: 0.0008778356686439788}. 
<span class=" -Color -Color-Green">[I 2021-06-24 13:33:27,234]</span> Trial 25 finished with values: [0.92, 0.5472538471221924] and parameters: {&#39;n_layers&#39;: 1, &#39;layer_1_size&#39;: 24, &#39;alpha&#39;: 0.2527894319467446, &#39;tol&#39;: 0.003817907750851227, &#39;beta_1&#39;: 0.2201911044553502, &#39;beta_2&#39;: 0.6241977351358688, &#39;learning_rate_init&#39;: 0.030048244665819682}. 
<span class=" -Color -Color-Green">[I 2021-06-24 13:33:27,788]</span> Trial 26 finished with values: [0.885, 0.37929534912109375] and parameters: {&#39;n_layers&#39;: 1, &#39;layer_1_size&#39;: 5, &#39;alpha&#39;: 0.04035086404093535, &#39;tol&#39;: 0.0015772100597426092, &#39;beta_1&#39;: 0.31983210103399984, &#39;beta_2&#39;: 0.1070563172989808, &#39;learning_rate_init&#39;: 0.005771575460170749}. 
<span class=" -Color -Color-Green">[I 2021-06-24 13:33:28,390]</span> Trial 27 finished with values: [0.9000000000000001, 0.48125219345092773] and parameters: {&#39;n_layers&#39;: 2, &#39;layer_1_size&#39;: 12, &#39;layer_2_size&#39;: 13, &#39;alpha&#39;: 0.00037381374577335043, &#39;tol&#39;: 0.00036803987105919495, &#39;beta_1&#39;: 0.11509538011805209, &#39;beta_2&#39;: 0.17742514744841037, &#39;learning_rate_init&#39;: 0.098533253291663}. 
<span class=" -Color -Color-Green">[I 2021-06-24 13:33:28,860]</span> Trial 28 finished with values: [0.8750000000000001, 0.3381640911102295] and parameters: {&#39;n_layers&#39;: 1, &#39;layer_1_size&#39;: 2, &#39;alpha&#39;: 0.00374876329258752, &#39;tol&#39;: 0.00010074420481316591, &#39;beta_1&#39;: 0.09386371352016509, &#39;beta_2&#39;: 0.49437870204785506, &#39;learning_rate_init&#39;: 0.020107037225897507}. 
<span class=" -Color -Color-Green">[I 2021-06-24 13:33:29,701]</span> Trial 29 finished with values: [0.8499999999999999, 0.49644994735717773] and parameters: {&#39;n_layers&#39;: 2, &#39;layer_1_size&#39;: 6, &#39;layer_2_size&#39;: 16, &#39;alpha&#39;: 0.014500060798757118, &#39;tol&#39;: 1.4622337591523938e-05, &#39;beta_1&#39;: 0.586619467167693, &#39;beta_2&#39;: 0.3522051176460188, &#39;learning_rate_init&#39;: 0.006593017788231954}. 
<span class=" -Color -Color-Green">[I 2021-06-24 13:33:30,272]</span> Trial 30 finished with values: [0.9000000000000001, 0.46068811416625977] and parameters: {&#39;n_layers&#39;: 1, &#39;layer_1_size&#39;: 24, &#39;alpha&#39;: 0.3008911753048114, &#39;tol&#39;: 0.0033534446033679217, &#39;beta_1&#39;: 0.19498515264825206, &#39;beta_2&#39;: 0.6551011501379808, &#39;learning_rate_init&#39;: 0.03883702672966298}. 
<span class=" -Color -Color-Green">[I 2021-06-24 13:33:30,824]</span> Trial 31 finished with values: [0.8400000000000001, 0.44289684295654297] and parameters: {&#39;n_layers&#39;: 1, &#39;layer_1_size&#39;: 21, &#39;alpha&#39;: 9.426709413690369e-05, &#39;tol&#39;: 0.007162534596671162, &#39;beta_1&#39;: 0.20814855881268296, &#39;beta_2&#39;: 0.8537697992885942, &#39;learning_rate_init&#39;: 0.0009298636148540676}. 
<span class=" -Color -Color-Green">[I 2021-06-24 13:33:31,444]</span> Trial 32 finished with values: [0.875, 0.4068598747253418] and parameters: {&#39;n_layers&#39;: 1, &#39;layer_1_size&#39;: 15, &#39;alpha&#39;: 0.0033422210874827028, &#39;tol&#39;: 0.002035376572759364, &#39;beta_1&#39;: 0.44313630060602044, &#39;beta_2&#39;: 0.5778648066441404, &#39;learning_rate_init&#39;: 0.001888594068533848}. 
<span class=" -Color -Color-Green">[I 2021-06-24 13:33:31,966]</span> Trial 33 finished with values: [0.905, 0.3877236843109131] and parameters: {&#39;n_layers&#39;: 1, &#39;layer_1_size&#39;: 11, &#39;alpha&#39;: 0.00044803849825677996, &#39;tol&#39;: 0.0006915427662677004, &#39;beta_1&#39;: 0.3004110596555007, &#39;beta_2&#39;: 0.4684418682612699, &#39;learning_rate_init&#39;: 0.009289000014349987}. 
<span class=" -Color -Color-Green">[I 2021-06-24 13:33:32,616]</span> Trial 34 finished with values: [0.9199999999999999, 0.4958219528198242] and parameters: {&#39;n_layers&#39;: 1, &#39;layer_1_size&#39;: 33, &#39;alpha&#39;: 0.14060606614370916, &#39;tol&#39;: 0.00012756757439172888, &#39;beta_1&#39;: 0.22848912933791732, &#39;beta_2&#39;: 0.6545972845611261, &#39;learning_rate_init&#39;: 0.02580564194852311}. 
/opt/anaconda3/envs/hpo/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn&#39;t converged yet.
  warnings.warn(
<span class=" -Color -Color-Green">[I 2021-06-24 13:33:33,580]</span> Trial 35 finished with values: [0.855, 0.5056807994842529] and parameters: {&#39;n_layers&#39;: 1, &#39;layer_1_size&#39;: 33, &#39;alpha&#39;: 0.0957863159183445, &#39;tol&#39;: 6.40874896824633e-05, &#39;beta_1&#39;: 0.07742991990471831, &#39;beta_2&#39;: 0.5998278667628184, &#39;learning_rate_init&#39;: 0.00011631837814581641}. 
/opt/anaconda3/envs/hpo/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn&#39;t converged yet.
  warnings.warn(
<span class=" -Color -Color-Green">[I 2021-06-24 13:33:34,242]</span> Trial 36 finished with values: [0.8499999999999999, 0.35098886489868164] and parameters: {&#39;n_layers&#39;: 1, &#39;layer_1_size&#39;: 4, &#39;alpha&#39;: 2.174007603453939e-05, &#39;tol&#39;: 0.00015286204751364474, &#39;beta_1&#39;: 0.3630803156007151, &#39;beta_2&#39;: 0.7020282708365593, &#39;learning_rate_init&#39;: 0.0028755536166495056}. 
<span class=" -Color -Color-Green">[I 2021-06-24 13:33:34,902]</span> Trial 37 finished with values: [0.925, 0.5387508869171143] and parameters: {&#39;n_layers&#39;: 1, &#39;layer_1_size&#39;: 43, &#39;alpha&#39;: 6.0188742456746935e-05, &#39;tol&#39;: 1.0040821446288553e-05, &#39;beta_1&#39;: 0.1645110475891487, &#39;beta_2&#39;: 0.8396096082685681, &#39;learning_rate_init&#39;: 0.022323632177520362}. 
<span class=" -Color -Color-Green">[I 2021-06-24 13:33:35,886]</span> Trial 38 finished with values: [0.9100000000000001, 0.5701723098754883] and parameters: {&#39;n_layers&#39;: 1, &#39;layer_1_size&#39;: 49, &#39;alpha&#39;: 0.0002504621455206035, &#39;tol&#39;: 0.00037787339972339027, &#39;beta_1&#39;: 0.24273321885455418, &#39;beta_2&#39;: 0.4727441419986968, &#39;learning_rate_init&#39;: 0.0009367998241135101}. 
<span class=" -Color -Color-Green">[I 2021-06-24 13:33:36,559]</span> Trial 39 finished with values: [0.92, 0.417011022567749] and parameters: {&#39;n_layers&#39;: 1, &#39;layer_1_size&#39;: 18, &#39;alpha&#39;: 0.0010113552186004052, &#39;tol&#39;: 2.874829807605508e-05, &#39;beta_1&#39;: 0.4063875900172627, &#39;beta_2&#39;: 0.9828639432076915, &#39;learning_rate_init&#39;: 0.008797369485265938}. 
<span class=" -Color -Color-Green">[I 2021-06-24 13:33:37,185]</span> Trial 40 finished with values: [0.895, 0.35930705070495605] and parameters: {&#39;n_layers&#39;: 1, &#39;layer_1_size&#39;: 8, &#39;alpha&#39;: 0.0018696881435804787, &#39;tol&#39;: 2.3951945245563057e-05, &#39;beta_1&#39;: 0.15942601774207996, &#39;beta_2&#39;: 0.962166131226281, &#39;learning_rate_init&#39;: 0.009045721228370355}. 
<span class=" -Color -Color-Green">[I 2021-06-24 13:33:37,749]</span> Trial 41 finished with values: [0.895, 0.402346134185791] and parameters: {&#39;n_layers&#39;: 1, &#39;layer_1_size&#39;: 13, &#39;alpha&#39;: 0.000841496079462844, &#39;tol&#39;: 1.3934098779389252e-05, &#39;beta_1&#39;: 0.4421476422362953, &#39;beta_2&#39;: 0.8677263032426578, &#39;learning_rate_init&#39;: 0.017054047336829265}. 
<span class=" -Color -Color-Green">[I 2021-06-24 13:33:38,224]</span> Trial 42 finished with values: [0.8799999999999999, 0.4237329959869385] and parameters: {&#39;n_layers&#39;: 1, &#39;layer_1_size&#39;: 16, &#39;alpha&#39;: 5.031212203007172e-05, &#39;tol&#39;: 0.37014501349322015, &#39;beta_1&#39;: 0.394563833693018, &#39;beta_2&#39;: 0.9144650382404799, &#39;learning_rate_init&#39;: 0.09581970222326597}. 
<span class=" -Color -Color-Green">[I 2021-06-24 13:33:38,707]</span> Trial 43 finished with values: [0.8900000000000001, 0.41590380668640137] and parameters: {&#39;n_layers&#39;: 1, &#39;layer_1_size&#39;: 18, &#39;alpha&#39;: 0.010241939511614643, &#39;tol&#39;: 0.020073367917353223, &#39;beta_1&#39;: 0.3066851468597482, &#39;beta_2&#39;: 0.8020650479727522, &#39;learning_rate_init&#39;: 0.04059518238654081}. 
/opt/anaconda3/envs/hpo/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn&#39;t converged yet.
  warnings.warn(
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span><span class=" -Color -Color-Green">[I 2021-06-24 13:33:39,400]</span> Trial 44 finished with values: [0.9000000000000001, 0.35135507583618164] and parameters: {&#39;n_layers&#39;: 1, &#39;layer_1_size&#39;: 7, &#39;alpha&#39;: 1.057434176380296e-05, &#39;tol&#39;: 6.707959414130896e-05, &#39;beta_1&#39;: 0.06619735461953394, &#39;beta_2&#39;: 0.07107971291956858, &#39;learning_rate_init&#39;: 0.005405215145553146}. 
<span class=" -Color -Color-Green">[I 2021-06-24 13:33:39,876]</span> Trial 45 finished with values: [0.855, 0.323167085647583] and parameters: {&#39;n_layers&#39;: 1, &#39;layer_1_size&#39;: 2, &#39;alpha&#39;: 0.030428082581132613, &#39;tol&#39;: 0.00040121967401255234, &#39;beta_1&#39;: 0.16804638046778253, &#39;beta_2&#39;: 0.17554668696202536, &#39;learning_rate_init&#39;: 0.053521417432980556}. 
<span class=" -Color -Color-Green">[I 2021-06-24 13:33:40,449]</span> Trial 46 finished with values: [0.905, 0.3662989139556885] and parameters: {&#39;n_layers&#39;: 1, &#39;layer_1_size&#39;: 10, &#39;alpha&#39;: 0.0007266574593452988, &#39;tol&#39;: 1.0102977147122388e-05, &#39;beta_1&#39;: 0.5207391537224039, &#39;beta_2&#39;: 0.9804189824133966, &#39;learning_rate_init&#39;: 0.02066479427739678}. 
/opt/anaconda3/envs/hpo/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn&#39;t converged yet.
  warnings.warn(
<span class=" -Color -Color-Green">[I 2021-06-24 13:33:41,090]</span> Trial 47 finished with values: [0.86, 0.33815693855285645] and parameters: {&#39;n_layers&#39;: 1, &#39;layer_1_size&#39;: 4, &#39;alpha&#39;: 0.00022701071230889742, &#39;tol&#39;: 3.279104462178131e-05, &#39;beta_1&#39;: 0.32499309814314714, &#39;beta_2&#39;: 0.04567625158109884, &#39;learning_rate_init&#39;: 0.002213926603840808}. 
<span class=" -Color -Color-Green">[I 2021-06-24 13:33:41,680]</span> Trial 48 finished with values: [0.885, 0.33988308906555176] and parameters: {&#39;n_layers&#39;: 1, &#39;layer_1_size&#39;: 6, &#39;alpha&#39;: 0.00669288208685084, &#39;tol&#39;: 0.00020846516958136562, &#39;beta_1&#39;: 0.7582012751216174, &#39;beta_2&#39;: 0.823999384021163, &#39;learning_rate_init&#39;: 0.007246835115820264}. 
<span class=" -Color -Color-Green">[I 2021-06-24 13:33:42,158]</span> Trial 49 finished with values: [0.595, 0.42757487297058105] and parameters: {&#39;n_layers&#39;: 1, &#39;layer_1_size&#39;: 22, &#39;alpha&#39;: 0.002129998475488314, &#39;tol&#39;: 0.04731180742572224, &#39;beta_1&#39;: 0.27401537602710013, &#39;beta_2&#39;: 0.24723005295855346, &#39;learning_rate_init&#39;: 0.0005763291625101341}. 
<span class=" -Color -Color-Green">[I 2021-06-24 13:33:42,814]</span> Trial 50 finished with values: [0.9150000000000001, 0.45728302001953125] and parameters: {&#39;n_layers&#39;: 1, &#39;layer_1_size&#39;: 26, &#39;alpha&#39;: 1.2147295649122401e-05, &#39;tol&#39;: 6.890273606454276e-05, &#39;beta_1&#39;: 0.06968907676995637, &#39;beta_2&#39;: 0.05971563924269985, &#39;learning_rate_init&#39;: 0.004955076076501721}. 
<span class=" -Color -Color-Green">[I 2021-06-24 13:33:43,413]</span> Trial 51 finished with values: [0.885, 0.3462188243865967] and parameters: {&#39;n_layers&#39;: 1, &#39;layer_1_size&#39;: 7, &#39;alpha&#39;: 6.23136574187244e-05, &#39;tol&#39;: 2.073288787164361e-05, &#39;beta_1&#39;: 0.17117034240278398, &#39;beta_2&#39;: 0.9185294077608815, &#39;learning_rate_init&#39;: 0.015402461333351382}. 
<span class=" -Color -Color-Green">[I 2021-06-24 13:33:44,089]</span> Trial 52 finished with values: [0.9149999999999999, 0.5402209758758545] and parameters: {&#39;n_layers&#39;: 1, &#39;layer_1_size&#39;: 44, &#39;alpha&#39;: 0.596170955358332, &#39;tol&#39;: 8.971686212973368e-05, &#39;beta_1&#39;: 0.40430873654327387, &#39;beta_2&#39;: 0.9491304935612683, &#39;learning_rate_init&#39;: 0.031964242108817854}. 
<span class=" -Color -Color-Green">[I 2021-06-24 13:33:44,775]</span> Trial 53 finished with values: [0.9299999999999998, 0.4530980587005615] and parameters: {&#39;n_layers&#39;: 1, &#39;layer_1_size&#39;: 26, &#39;alpha&#39;: 0.0173117951359361, &#39;tol&#39;: 3.065172504685016e-05, &#39;beta_1&#39;: 0.10385550301659174, &#39;beta_2&#39;: 0.3691356144401184, &#39;learning_rate_init&#39;: 0.010241430487373542}. 
<span class=" -Color -Color-Green">[I 2021-06-24 13:33:45,151]</span> Trial 54 finished with values: [0.855, 0.3304738998413086] and parameters: {&#39;n_layers&#39;: 1, &#39;layer_1_size&#39;: 4, &#39;alpha&#39;: 0.0690676257681989, &#39;tol&#39;: 0.13384722503983637, &#39;beta_1&#39;: 0.09983794159959614, &#39;beta_2&#39;: 0.20509124886831753, &#39;learning_rate_init&#39;: 0.06607301485521548}. 
</pre></div>
</div>
</div>
</div>
<p>As a trade off to the flexibility of a define-by-run hyperparameter space, Optuna’s random search is suboptimal for problems with a fixed, non-dynamic hyperparameter space. Unlike Ax’s use of Sobol random, Optuna uses the <code class="docutils literal notranslate"><span class="pre">numpy</span></code> RandomState class to generate independent samples from the parameter space. You can read more about the difference between these types of random sampling in our main blog post (TODO: link again (to the specific section))?</p>
<p>However, generating an experiment with random sampling is as easy as changing the sampler!</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">random_sampler</span> <span class="o">=</span> <span class="n">optuna</span><span class="o">.</span><span class="n">samplers</span><span class="o">.</span><span class="n">RandomSampler</span><span class="p">()</span>
<span class="n">random_experiment</span> <span class="o">=</span> <span class="n">optuna</span><span class="o">.</span><span class="n">create_study</span><span class="p">(</span><span class="n">sampler</span><span class="o">=</span><span class="n">random_sampler</span><span class="p">,</span> <span class="n">directions</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;maximize&quot;</span><span class="p">,</span> <span class="s2">&quot;minimize&quot;</span><span class="p">])</span>
<span class="n">random_experiment</span><span class="o">.</span><span class="n">optimize</span><span class="p">(</span><span class="n">train</span><span class="p">,</span> <span class="n">n_trials</span><span class="o">=</span><span class="n">N_TRIAL</span><span class="p">,</span> <span class="n">timeout</span><span class="o">=</span><span class="mi">300</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span><span class=" -Color -Color-Green">[I 2021-06-24 13:33:45,156]</span> A new study created in memory with name: no-name-f5ea743f-5351-4c94-bb3b-073cfc71208a
<span class=" -Color -Color-Green">[I 2021-06-24 13:33:45,526]</span> Trial 0 finished with values: [0.855, 0.3397219181060791] and parameters: {&#39;n_layers&#39;: 1, &#39;layer_1_size&#39;: 5, &#39;alpha&#39;: 1.6583798916713135e-06, &#39;tol&#39;: 0.04131954057651528, &#39;beta_1&#39;: 0.03944237992080375, &#39;beta_2&#39;: 0.7740661460724664, &#39;learning_rate_init&#39;: 0.04852577351926202}. 
<span class=" -Color -Color-Green">[I 2021-06-24 13:33:46,086]</span> Trial 1 finished with values: [0.8949999999999999, 0.5287368297576904] and parameters: {&#39;n_layers&#39;: 1, &#39;layer_1_size&#39;: 42, &#39;alpha&#39;: 0.9963429830202633, &#39;tol&#39;: 0.09246280379558958, &#39;beta_1&#39;: 0.7195088193468046, &#39;beta_2&#39;: 0.29885617879226084, &#39;learning_rate_init&#39;: 0.06860857380535164}. 
<span class=" -Color -Color-Green">[I 2021-06-24 13:33:46,747]</span> Trial 2 finished with values: [0.64, 0.623384952545166] and parameters: {&#39;n_layers&#39;: 2, &#39;layer_1_size&#39;: 36, &#39;layer_2_size&#39;: 19, &#39;alpha&#39;: 3.1382750520930222e-06, &#39;tol&#39;: 0.008763082630036568, &#39;beta_1&#39;: 0.7933874414978763, &#39;beta_2&#39;: 0.32699564283576793, &#39;learning_rate_init&#39;: 0.00016543130829685916}. 
<span class=" -Color -Color-Green">[I 2021-06-24 13:33:47,382]</span> Trial 3 finished with values: [0.505, 0.596613883972168] and parameters: {&#39;n_layers&#39;: 2, &#39;layer_1_size&#39;: 20, &#39;layer_2_size&#39;: 31, &#39;alpha&#39;: 0.42397941653045, &#39;tol&#39;: 0.7581410178733912, &#39;beta_1&#39;: 0.373962412132173, &#39;beta_2&#39;: 0.6024219141589208, &#39;learning_rate_init&#39;: 1.8935334780803297e-06}. 
<span class=" -Color -Color-Green">[I 2021-06-24 13:33:47,841]</span> Trial 4 finished with values: [0.5299999999999999, 0.43263888359069824] and parameters: {&#39;n_layers&#39;: 1, &#39;layer_1_size&#39;: 22, &#39;alpha&#39;: 0.7236784657614748, &#39;tol&#39;: 0.6058590193629025, &#39;beta_1&#39;: 0.8490182333714608, &#39;beta_2&#39;: 0.49489308559283846, &#39;learning_rate_init&#39;: 0.00025438913324192535}. 
<span class=" -Color -Color-Green">[I 2021-06-24 13:33:48,456]</span> Trial 5 finished with values: [0.92, 0.5632467269897461] and parameters: {&#39;n_layers&#39;: 1, &#39;layer_1_size&#39;: 43, &#39;alpha&#39;: 0.005601695302365957, &#39;tol&#39;: 0.020153556066825066, &#39;beta_1&#39;: 0.8037899579284612, &#39;beta_2&#39;: 0.4189614188487681, &#39;learning_rate_init&#39;: 0.015437632775690668}. 
<span class=" -Color -Color-Green">[I 2021-06-24 13:33:49,118]</span> Trial 6 finished with values: [0.5, 0.6153221130371094] and parameters: {&#39;n_layers&#39;: 3, &#39;layer_1_size&#39;: 19, &#39;layer_2_size&#39;: 22, &#39;layer_3_size&#39;: 7, &#39;alpha&#39;: 0.2536664779941713, &#39;tol&#39;: 0.11306246065595354, &#39;beta_1&#39;: 0.22365200386713138, &#39;beta_2&#39;: 0.9722277568719978, &#39;learning_rate_init&#39;: 4.842107491111102e-05}. 
<span class=" -Color -Color-Green">[I 2021-06-24 13:33:49,791]</span> Trial 7 finished with values: [0.5149999999999999, 0.6333682537078857] and parameters: {&#39;n_layers&#39;: 2, &#39;layer_1_size&#39;: 7, &#39;layer_2_size&#39;: 41, &#39;alpha&#39;: 0.0004271872784203828, &#39;tol&#39;: 0.07664708651506831, &#39;beta_1&#39;: 0.9811908790685665, &#39;beta_2&#39;: 0.8259244904930456, &#39;learning_rate_init&#39;: 1.197868976673003e-05}. 
/opt/anaconda3/envs/hpo/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn&#39;t converged yet.
  warnings.warn(
<span class=" -Color -Color-Green">[I 2021-06-24 13:33:51,150]</span> Trial 8 finished with values: [0.9, 0.7186932563781738] and parameters: {&#39;n_layers&#39;: 2, &#39;layer_1_size&#39;: 46, &#39;layer_2_size&#39;: 27, &#39;alpha&#39;: 0.005243706209133101, &#39;tol&#39;: 3.1614281193465674e-05, &#39;beta_1&#39;: 0.2909670167287171, &#39;beta_2&#39;: 0.8084795135185837, &#39;learning_rate_init&#39;: 0.0003413866228000295}. 
<span class=" -Color -Color-Green">[I 2021-06-24 13:33:52,056]</span> Trial 9 finished with values: [0.4900000000000001, 0.8515229225158691] and parameters: {&#39;n_layers&#39;: 3, &#39;layer_1_size&#39;: 10, &#39;layer_2_size&#39;: 37, &#39;layer_3_size&#39;: 41, &#39;alpha&#39;: 5.977067562977668e-06, &#39;tol&#39;: 0.0001858564972687893, &#39;beta_1&#39;: 0.2614030814174309, &#39;beta_2&#39;: 0.1554436285822663, &#39;learning_rate_init&#39;: 1.0954115984727397e-05}. 
<span class=" -Color -Color-Green">[I 2021-06-24 13:33:53,506]</span> Trial 10 finished with values: [0.8200000000000001, 0.7752377986907959] and parameters: {&#39;n_layers&#39;: 3, &#39;layer_1_size&#39;: 8, &#39;layer_2_size&#39;: 27, &#39;layer_3_size&#39;: 42, &#39;alpha&#39;: 0.0011728533397120265, &#39;tol&#39;: 0.0012326734934114524, &#39;beta_1&#39;: 0.5216284138903077, &#39;beta_2&#39;: 0.9119061996071103, &#39;learning_rate_init&#39;: 0.0007369021217116634}. 
<span class=" -Color -Color-Green">[I 2021-06-24 13:33:54,541]</span> Trial 11 finished with values: [0.8550000000000002, 0.6994128227233887] and parameters: {&#39;n_layers&#39;: 3, &#39;layer_1_size&#39;: 8, &#39;layer_2_size&#39;: 19, &#39;layer_3_size&#39;: 38, &#39;alpha&#39;: 0.5538325901691606, &#39;tol&#39;: 0.0007668619951854919, &#39;beta_1&#39;: 0.6712553151717879, &#39;beta_2&#39;: 0.5570377134596991, &#39;learning_rate_init&#39;: 0.004286934471615339}. 
<span class=" -Color -Color-Green">[I 2021-06-24 13:33:55,494]</span> Trial 12 finished with values: [0.9149999999999999, 0.7734830379486084] and parameters: {&#39;n_layers&#39;: 2, &#39;layer_1_size&#39;: 45, &#39;layer_2_size&#39;: 38, &#39;alpha&#39;: 0.10330475805355407, &#39;tol&#39;: 6.933829845186716e-05, &#39;beta_1&#39;: 0.5411549805754399, &#39;beta_2&#39;: 0.8261044700714064, &#39;learning_rate_init&#39;: 0.012836281190961819}. 
<span class=" -Color -Color-Green">[I 2021-06-24 13:33:56,343]</span> Trial 13 finished with values: [0.91, 0.7191028594970703] and parameters: {&#39;n_layers&#39;: 2, &#39;layer_1_size&#39;: 36, &#39;layer_2_size&#39;: 38, &#39;alpha&#39;: 0.00018293294356677486, &#39;tol&#39;: 0.0017038915001011785, &#39;beta_1&#39;: 0.25176497601454806, &#39;beta_2&#39;: 0.6017723593140502, &#39;learning_rate_init&#39;: 0.0036629996866074264}. 
<span class=" -Color -Color-Green">[I 2021-06-24 13:33:56,800]</span> Trial 14 finished with values: [0.8799999999999999, 0.38059115409851074] and parameters: {&#39;n_layers&#39;: 1, &#39;layer_1_size&#39;: 11, &#39;alpha&#39;: 0.0014559109594048342, &#39;tol&#39;: 1.4704417828771943e-05, &#39;beta_1&#39;: 0.8308062313058308, &#39;beta_2&#39;: 0.06619582670978796, &#39;learning_rate_init&#39;: 0.01818759455072945}. 
<span class=" -Color -Color-Green">[I 2021-06-24 13:33:57,737]</span> Trial 15 finished with values: [0.54, 0.8825051784515381] and parameters: {&#39;n_layers&#39;: 3, &#39;layer_1_size&#39;: 46, &#39;layer_2_size&#39;: 13, &#39;layer_3_size&#39;: 38, &#39;alpha&#39;: 0.0008155199557337989, &#39;tol&#39;: 0.9274263741709988, &#39;beta_1&#39;: 0.08628533951365917, &#39;beta_2&#39;: 0.22145158263462905, &#39;learning_rate_init&#39;: 2.111706618963953e-05}. 
<span class=" -Color -Color-Green">[I 2021-06-24 13:33:58,112]</span> Trial 16 finished with values: [0.885, 0.34348392486572266] and parameters: {&#39;n_layers&#39;: 1, &#39;layer_1_size&#39;: 5, &#39;alpha&#39;: 2.4437319186487103e-05, &#39;tol&#39;: 0.024847942502518868, &#39;beta_1&#39;: 0.08255665258506001, &#39;beta_2&#39;: 0.11759147669654614, &#39;learning_rate_init&#39;: 0.026362335735349265}. 
<span class=" -Color -Color-Green">[I 2021-06-24 13:33:59,467]</span> Trial 17 finished with values: [0.8900000000000001, 1.0606257915496826] and parameters: {&#39;n_layers&#39;: 3, &#39;layer_1_size&#39;: 39, &#39;layer_2_size&#39;: 42, &#39;layer_3_size&#39;: 49, &#39;alpha&#39;: 0.00012881218852591343, &#39;tol&#39;: 8.160713830729106e-05, &#39;beta_1&#39;: 0.7543338098522159, &#39;beta_2&#39;: 0.2882403890404505, &#39;learning_rate_init&#39;: 0.002015949676692678}. 
/opt/anaconda3/envs/hpo/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn&#39;t converged yet.
  warnings.warn(
<span class=" -Color -Color-Green">[I 2021-06-24 13:34:01,117]</span> Trial 18 finished with values: [0.71, 0.850836992263794] and parameters: {&#39;n_layers&#39;: 3, &#39;layer_1_size&#39;: 38, &#39;layer_2_size&#39;: 37, &#39;layer_3_size&#39;: 19, &#39;alpha&#39;: 2.929803841476853e-05, &#39;tol&#39;: 2.463734430094916e-05, &#39;beta_1&#39;: 0.3163110539111522, &#39;beta_2&#39;: 0.8900123217662869, &#39;learning_rate_init&#39;: 2.8819353954641113e-05}. 
/opt/anaconda3/envs/hpo/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn&#39;t converged yet.
  warnings.warn(
<span class=" -Color -Color-Green">[I 2021-06-24 13:34:02,017]</span> Trial 19 finished with values: [0.55, 0.4279639720916748] and parameters: {&#39;n_layers&#39;: 2, &#39;layer_1_size&#39;: 10, &#39;layer_2_size&#39;: 6, &#39;alpha&#39;: 1.991925800723186e-05, &#39;tol&#39;: 1.1108773191494136e-05, &#39;beta_1&#39;: 0.15620543788512337, &#39;beta_2&#39;: 0.23519189125732423, &#39;learning_rate_init&#39;: 4.0655131391347275e-05}. 
/opt/anaconda3/envs/hpo/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn&#39;t converged yet.
  warnings.warn(
<span class=" -Color -Color-Green">[I 2021-06-24 13:34:03,294]</span> Trial 20 finished with values: [0.8200000000000001, 0.6442630290985107] and parameters: {&#39;n_layers&#39;: 3, &#39;layer_1_size&#39;: 2, &#39;layer_2_size&#39;: 13, &#39;layer_3_size&#39;: 36, &#39;alpha&#39;: 4.307738569153522e-06, &#39;tol&#39;: 4.512112322889721e-05, &#39;beta_1&#39;: 0.5507619382339853, &#39;beta_2&#39;: 0.5115341186797726, &#39;learning_rate_init&#39;: 0.001962163151533638}. 
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span><span class=" -Color -Color-Green">[I 2021-06-24 13:34:03,977]</span> Trial 21 finished with values: [0.875, 0.6066792011260986] and parameters: {&#39;n_layers&#39;: 2, &#39;layer_1_size&#39;: 50, &#39;layer_2_size&#39;: 3, &#39;alpha&#39;: 0.0007933111984618153, &#39;tol&#39;: 0.04194318984051112, &#39;beta_1&#39;: 0.942039711044863, &#39;beta_2&#39;: 0.8655986481594887, &#39;learning_rate_init&#39;: 0.010487012710084159}. 
/opt/anaconda3/envs/hpo/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn&#39;t converged yet.
  warnings.warn(
<span class=" -Color -Color-Green">[I 2021-06-24 13:34:04,760]</span> Trial 22 finished with values: [0.55, 0.40920495986938477] and parameters: {&#39;n_layers&#39;: 1, &#39;layer_1_size&#39;: 15, &#39;alpha&#39;: 0.08577432928050567, &#39;tol&#39;: 0.0005611833437950967, &#39;beta_1&#39;: 0.06544242583279844, &#39;beta_2&#39;: 0.1581465699036227, &#39;learning_rate_init&#39;: 2.9969661296918156e-05}. 
<span class=" -Color -Color-Green">[I 2021-06-24 13:34:05,612]</span> Trial 23 finished with values: [0.5, 0.7926290035247803] and parameters: {&#39;n_layers&#39;: 3, &#39;layer_1_size&#39;: 23, &#39;layer_2_size&#39;: 36, &#39;layer_3_size&#39;: 23, &#39;alpha&#39;: 0.010466850264954053, &#39;tol&#39;: 0.04290765902542515, &#39;beta_1&#39;: 0.03243343830273739, &#39;beta_2&#39;: 0.4603344337508877, &#39;learning_rate_init&#39;: 8.544154512593795e-06}. 
<span class=" -Color -Color-Green">[I 2021-06-24 13:34:06,443]</span> Trial 24 finished with values: [0.855, 0.5007359981536865] and parameters: {&#39;n_layers&#39;: 3, &#39;layer_1_size&#39;: 6, &#39;layer_2_size&#39;: 9, &#39;layer_3_size&#39;: 12, &#39;alpha&#39;: 0.0006435187074001271, &#39;tol&#39;: 0.00027602508229182467, &#39;beta_1&#39;: 0.5084413597862643, &#39;beta_2&#39;: 0.7393831583344864, &#39;learning_rate_init&#39;: 0.01198268847665325}. 
<span class=" -Color -Color-Green">[I 2021-06-24 13:34:07,405]</span> Trial 25 finished with values: [0.54, 0.9010748863220215] and parameters: {&#39;n_layers&#39;: 3, &#39;layer_1_size&#39;: 15, &#39;layer_2_size&#39;: 33, &#39;layer_3_size&#39;: 48, &#39;alpha&#39;: 0.051414799565382745, &#39;tol&#39;: 0.031764352247501806, &#39;beta_1&#39;: 0.9587544992783016, &#39;beta_2&#39;: 0.279442607486061, &#39;learning_rate_init&#39;: 1.5063970873016203e-05}. 
<span class=" -Color -Color-Green">[I 2021-06-24 13:34:08,001]</span> Trial 26 finished with values: [0.43499999999999994, 0.5616238117218018] and parameters: {&#39;n_layers&#39;: 1, &#39;layer_1_size&#39;: 42, &#39;alpha&#39;: 1.624737859599849e-05, &#39;tol&#39;: 0.26871842526274853, &#39;beta_1&#39;: 0.3456434949987803, &#39;beta_2&#39;: 0.7082101962847671, &#39;learning_rate_init&#39;: 2.9370582989984074e-06}. 
/opt/anaconda3/envs/hpo/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn&#39;t converged yet.
  warnings.warn(
<span class=" -Color -Color-Green">[I 2021-06-24 13:34:09,343]</span> Trial 27 finished with values: [0.5249999999999999, 0.681602954864502] and parameters: {&#39;n_layers&#39;: 2, &#39;layer_1_size&#39;: 42, &#39;layer_2_size&#39;: 23, &#39;alpha&#39;: 0.03945676786130281, &#39;tol&#39;: 2.6246078981459565e-05, &#39;beta_1&#39;: 0.680397427899695, &#39;beta_2&#39;: 0.8567586133823424, &#39;learning_rate_init&#39;: 1.174314947044249e-06}. 
<span class=" -Color -Color-Green">[I 2021-06-24 13:34:09,784]</span> Trial 28 finished with values: [0.49499999999999994, 0.4152200222015381] and parameters: {&#39;n_layers&#39;: 1, &#39;layer_1_size&#39;: 16, &#39;alpha&#39;: 0.006768462925861922, &#39;tol&#39;: 0.00023298985248277273, &#39;beta_1&#39;: 0.5004547307104211, &#39;beta_2&#39;: 0.5001752932075587, &#39;learning_rate_init&#39;: 4.347504095107995e-06}. 
/opt/anaconda3/envs/hpo/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn&#39;t converged yet.
  warnings.warn(
<span class=" -Color -Color-Green">[I 2021-06-24 13:34:10,624]</span> Trial 29 finished with values: [0.86, 0.44705986976623535] and parameters: {&#39;n_layers&#39;: 1, &#39;layer_1_size&#39;: 23, &#39;alpha&#39;: 1.3761938079642387e-06, &#39;tol&#39;: 0.0005408636825472867, &#39;beta_1&#39;: 0.4108202108882131, &#39;beta_2&#39;: 0.0616851861392132, &#39;learning_rate_init&#39;: 0.0001846989091908955}. 
<span class=" -Color -Color-Green">[I 2021-06-24 13:34:11,240]</span> Trial 30 finished with values: [0.45999999999999996, 0.5760910511016846] and parameters: {&#39;n_layers&#39;: 3, &#39;layer_1_size&#39;: 10, &#39;layer_2_size&#39;: 2, &#39;layer_3_size&#39;: 32, &#39;alpha&#39;: 0.024585381327682697, &#39;tol&#39;: 0.011048799096017222, &#39;beta_1&#39;: 0.5286940395655706, &#39;beta_2&#39;: 0.9532751026932333, &#39;learning_rate_init&#39;: 3.5289168173536404e-05}. 
/opt/anaconda3/envs/hpo/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn&#39;t converged yet.
  warnings.warn(
<span class=" -Color -Color-Green">[I 2021-06-24 13:34:12,710]</span> Trial 31 finished with values: [0.54, 0.7988591194152832] and parameters: {&#39;n_layers&#39;: 2, &#39;layer_1_size&#39;: 48, &#39;layer_2_size&#39;: 33, &#39;alpha&#39;: 3.1409790082700724e-06, &#39;tol&#39;: 1.5694702780057205e-05, &#39;beta_1&#39;: 0.62638611404597, &#39;beta_2&#39;: 0.5432363605559006, &#39;learning_rate_init&#39;: 4.030006254008868e-06}. 
/opt/anaconda3/envs/hpo/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn&#39;t converged yet.
  warnings.warn(
<span class=" -Color -Color-Green">[I 2021-06-24 13:34:14,629]</span> Trial 32 finished with values: [0.8499999999999999, 1.0101428031921387] and parameters: {&#39;n_layers&#39;: 3, &#39;layer_1_size&#39;: 17, &#39;layer_2_size&#39;: 43, &#39;layer_3_size&#39;: 50, &#39;alpha&#39;: 0.0020394733454806068, &#39;tol&#39;: 0.000737611371478881, &#39;beta_1&#39;: 0.6389748930474267, &#39;beta_2&#39;: 0.9579072617603983, &#39;learning_rate_init&#39;: 9.910534011134151e-05}. 
/opt/anaconda3/envs/hpo/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn&#39;t converged yet.
  warnings.warn(
<span class=" -Color -Color-Green">[I 2021-06-24 13:34:16,694]</span> Trial 33 finished with values: [0.6000000000000001, 1.119797945022583] and parameters: {&#39;n_layers&#39;: 3, &#39;layer_1_size&#39;: 49, &#39;layer_2_size&#39;: 37, &#39;layer_3_size&#39;: 44, &#39;alpha&#39;: 0.002382387886025946, &#39;tol&#39;: 2.3265033409221653e-05, &#39;beta_1&#39;: 0.5456980326652223, &#39;beta_2&#39;: 0.8028561054219006, &#39;learning_rate_init&#39;: 1.5408055104570808e-05}. 
<span class=" -Color -Color-Green">[I 2021-06-24 13:34:17,419]</span> Trial 34 finished with values: [0.46499999999999997, 0.6823008060455322] and parameters: {&#39;n_layers&#39;: 2, &#39;layer_1_size&#39;: 9, &#39;layer_2_size&#39;: 41, &#39;alpha&#39;: 5.197871032815299e-06, &#39;tol&#39;: 0.34922065975912536, &#39;beta_1&#39;: 0.877987038234385, &#39;beta_2&#39;: 0.6764641425499057, &#39;learning_rate_init&#39;: 2.434850287289835e-05}. 
/opt/anaconda3/envs/hpo/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn&#39;t converged yet.
  warnings.warn(
<span class=" -Color -Color-Green">[I 2021-06-24 13:34:19,249]</span> Trial 35 finished with values: [0.685, 0.9688162803649902] and parameters: {&#39;n_layers&#39;: 3, &#39;layer_1_size&#39;: 41, &#39;layer_2_size&#39;: 17, &#39;layer_3_size&#39;: 46, &#39;alpha&#39;: 0.06446830412773256, &#39;tol&#39;: 4.4446477484800655e-05, &#39;beta_1&#39;: 0.5284062936253946, &#39;beta_2&#39;: 0.4034825970274928, &#39;learning_rate_init&#39;: 2.4954707549228485e-05}. 
<span class=" -Color -Color-Green">[I 2021-06-24 13:34:19,936]</span> Trial 36 finished with values: [0.5, 0.642056941986084] and parameters: {&#39;n_layers&#39;: 2, &#39;layer_1_size&#39;: 49, &#39;layer_2_size&#39;: 5, &#39;alpha&#39;: 7.484642780385038e-05, &#39;tol&#39;: 0.8026983711041319, &#39;beta_1&#39;: 0.8211770620959823, &#39;beta_2&#39;: 0.009330230528697806, &#39;learning_rate_init&#39;: 0.016142827887176994}. 
/opt/anaconda3/envs/hpo/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn&#39;t converged yet.
  warnings.warn(
<span class=" -Color -Color-Green">[I 2021-06-24 13:34:21,142]</span> Trial 37 finished with values: [0.76, 0.6158819198608398] and parameters: {&#39;n_layers&#39;: 2, &#39;layer_1_size&#39;: 15, &#39;layer_2_size&#39;: 29, &#39;alpha&#39;: 0.5474988526548683, &#39;tol&#39;: 1.0787805202803713e-05, &#39;beta_1&#39;: 0.5157074397897745, &#39;beta_2&#39;: 0.5623035298482, &#39;learning_rate_init&#39;: 5.98508726436748e-05}. 
<span class=" -Color -Color-Green">[I 2021-06-24 13:34:21,798]</span> Trial 38 finished with values: [0.91, 0.615792989730835] and parameters: {&#39;n_layers&#39;: 1, &#39;layer_1_size&#39;: 44, &#39;alpha&#39;: 0.19102495225479055, &#39;tol&#39;: 0.050634509376439665, &#39;beta_1&#39;: 0.2722178227605082, &#39;beta_2&#39;: 0.42512133521713275, &#39;learning_rate_init&#39;: 0.02664631455679372}. 
<span class=" -Color -Color-Green">[I 2021-06-24 13:34:22,475]</span> Trial 39 finished with values: [0.85, 0.6206238269805908] and parameters: {&#39;n_layers&#39;: 2, &#39;layer_1_size&#39;: 6, &#39;layer_2_size&#39;: 35, &#39;alpha&#39;: 4.3534383191541425e-05, &#39;tol&#39;: 0.07000379573442753, &#39;beta_1&#39;: 0.19834320285948503, &#39;beta_2&#39;: 0.2658104205179576, &#39;learning_rate_init&#39;: 0.010038140134207767}. 
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span><span class=" -Color -Color-Green">[I 2021-06-24 13:34:23,036]</span> Trial 40 finished with values: [0.6000000000000001, 0.5281221866607666] and parameters: {&#39;n_layers&#39;: 1, &#39;layer_1_size&#39;: 35, &#39;alpha&#39;: 0.0012641283486926052, &#39;tol&#39;: 0.9282753886154791, &#39;beta_1&#39;: 0.37774095460471235, &#39;beta_2&#39;: 0.6407286452574744, &#39;learning_rate_init&#39;: 1.977844210971094e-06}. 
<span class=" -Color -Color-Green">[I 2021-06-24 13:34:23,420]</span> Trial 41 finished with values: [0.485, 0.35796308517456055] and parameters: {&#39;n_layers&#39;: 1, &#39;layer_1_size&#39;: 5, &#39;alpha&#39;: 1.3989383647500107e-05, &#39;tol&#39;: 0.038166776868459595, &#39;beta_1&#39;: 0.5631958946498815, &#39;beta_2&#39;: 0.6935417098351765, &#39;learning_rate_init&#39;: 3.241404263654068e-06}. 
<span class=" -Color -Color-Green">[I 2021-06-24 13:34:24,553]</span> Trial 42 finished with values: [0.8900000000000001, 0.9083409309387207] and parameters: {&#39;n_layers&#39;: 3, &#39;layer_1_size&#39;: 27, &#39;layer_2_size&#39;: 26, &#39;layer_3_size&#39;: 34, &#39;alpha&#39;: 1.123847998137992e-05, &#39;tol&#39;: 0.0018205660184223804, &#39;beta_1&#39;: 0.3326677975728737, &#39;beta_2&#39;: 0.1256364390286232, &#39;learning_rate_init&#39;: 0.0017725719114662698}. 
<span class=" -Color -Color-Green">[I 2021-06-24 13:34:25,490]</span> Trial 43 finished with values: [0.6749999999999999, 0.7808301448822021] and parameters: {&#39;n_layers&#39;: 3, &#39;layer_1_size&#39;: 5, &#39;layer_2_size&#39;: 27, &#39;layer_3_size&#39;: 36, &#39;alpha&#39;: 0.0047016429006261524, &#39;tol&#39;: 0.0033275855653821618, &#39;beta_1&#39;: 0.05521239988246544, &#39;beta_2&#39;: 0.6501295018857687, &#39;learning_rate_init&#39;: 0.00034545741656327045}. 
<span class=" -Color -Color-Green">[I 2021-06-24 13:34:26,138]</span> Trial 44 finished with values: [0.48, 0.6123931407928467] and parameters: {&#39;n_layers&#39;: 1, &#39;layer_1_size&#39;: 45, &#39;alpha&#39;: 0.0005374729355163436, &#39;tol&#39;: 0.043896199593101065, &#39;beta_1&#39;: 0.6546103631566926, &#39;beta_2&#39;: 0.8335106441103929, &#39;learning_rate_init&#39;: 8.869773707109956e-06}. 
<span class=" -Color -Color-Green">[I 2021-06-24 13:34:26,974]</span> Trial 45 finished with values: [0.4650000000000001, 0.7866928577423096] and parameters: {&#39;n_layers&#39;: 2, &#39;layer_1_size&#39;: 45, &#39;layer_2_size&#39;: 28, &#39;alpha&#39;: 2.8588588882050116e-05, &#39;tol&#39;: 0.12825741154825399, &#39;beta_1&#39;: 0.0035546165875830382, &#39;beta_2&#39;: 0.04155368415087471, &#39;learning_rate_init&#39;: 4.670885073758701e-05}. 
<span class=" -Color -Color-Green">[I 2021-06-24 13:34:27,541]</span> Trial 46 finished with values: [0.875, 0.4715080261230469] and parameters: {&#39;n_layers&#39;: 1, &#39;layer_1_size&#39;: 16, &#39;alpha&#39;: 0.05035684906215516, &#39;tol&#39;: 0.00487310448019807, &#39;beta_1&#39;: 0.39122115729730395, &#39;beta_2&#39;: 0.835814075922457, &#39;learning_rate_init&#39;: 0.002753535093979246}. 
<span class=" -Color -Color-Green">[I 2021-06-24 13:34:28,735]</span> Trial 47 finished with values: [0.8299999999999998, 1.0442109107971191] and parameters: {&#39;n_layers&#39;: 3, &#39;layer_1_size&#39;: 17, &#39;layer_2_size&#39;: 44, &#39;layer_3_size&#39;: 50, &#39;alpha&#39;: 0.009609284111752372, &#39;tol&#39;: 0.015018481272420114, &#39;beta_1&#39;: 0.37690607974052004, &#39;beta_2&#39;: 0.08525260827450304, &#39;learning_rate_init&#39;: 0.0008199363166659755}. 
/opt/anaconda3/envs/hpo/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn&#39;t converged yet.
  warnings.warn(
<span class=" -Color -Color-Green">[I 2021-06-24 13:34:30,319]</span> Trial 48 finished with values: [0.905, 0.8430337905883789] and parameters: {&#39;n_layers&#39;: 2, &#39;layer_1_size&#39;: 37, &#39;layer_2_size&#39;: 45, &#39;alpha&#39;: 0.10612273808327918, &#39;tol&#39;: 3.3620762652058e-05, &#39;beta_1&#39;: 0.6713143528457702, &#39;beta_2&#39;: 0.40658339286182416, &#39;learning_rate_init&#39;: 0.0004215816010897672}. 
/opt/anaconda3/envs/hpo/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn&#39;t converged yet.
  warnings.warn(
<span class=" -Color -Color-Green">[I 2021-06-24 13:34:31,187]</span> Trial 49 finished with values: [0.475, 0.46172499656677246] and parameters: {&#39;n_layers&#39;: 1, &#39;layer_1_size&#39;: 18, &#39;alpha&#39;: 1.3407717747060347e-05, &#39;tol&#39;: 2.833612831697428e-05, &#39;beta_1&#39;: 0.598225759514398, &#39;beta_2&#39;: 0.6857723167686244, &#39;learning_rate_init&#39;: 1.2381650680544528e-06}. 
/opt/anaconda3/envs/hpo/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn&#39;t converged yet.
  warnings.warn(
<span class=" -Color -Color-Green">[I 2021-06-24 13:34:32,722]</span> Trial 50 finished with values: [0.505, 0.7949621677398682] and parameters: {&#39;n_layers&#39;: 3, &#39;layer_1_size&#39;: 4, &#39;layer_2_size&#39;: 35, &#39;layer_3_size&#39;: 25, &#39;alpha&#39;: 0.299573687157635, &#39;tol&#39;: 2.5987417662356293e-05, &#39;beta_1&#39;: 0.7330778867852773, &#39;beta_2&#39;: 0.6078188681395581, &#39;learning_rate_init&#39;: 9.170306610311602e-06}. 
<span class=" -Color -Color-Green">[I 2021-06-24 13:34:33,422]</span> Trial 51 finished with values: [0.55, 0.6555390357971191] and parameters: {&#39;n_layers&#39;: 3, &#39;layer_1_size&#39;: 4, &#39;layer_2_size&#39;: 36, &#39;layer_3_size&#39;: 5, &#39;alpha&#39;: 4.883106191941164e-05, &#39;tol&#39;: 0.023259746876049123, &#39;beta_1&#39;: 0.8746642547164667, &#39;beta_2&#39;: 0.4758580715642077, &#39;learning_rate_init&#39;: 9.688075251402541e-06}. 
<span class=" -Color -Color-Green">[I 2021-06-24 13:34:33,813]</span> Trial 52 finished with values: [0.395, 0.36202383041381836] and parameters: {&#39;n_layers&#39;: 1, &#39;layer_1_size&#39;: 7, &#39;alpha&#39;: 2.25192461437767e-06, &#39;tol&#39;: 0.04401127662912664, &#39;beta_1&#39;: 0.9129755595332987, &#39;beta_2&#39;: 0.21240965455242183, &#39;learning_rate_init&#39;: 4.335062761351774e-05}. 
<span class=" -Color -Color-Green">[I 2021-06-24 13:34:34,320]</span> Trial 53 finished with values: [0.44000000000000006, 0.473236083984375] and parameters: {&#39;n_layers&#39;: 1, &#39;layer_1_size&#39;: 27, &#39;alpha&#39;: 0.01643970382856014, &#39;tol&#39;: 0.002741058586533343, &#39;beta_1&#39;: 0.5990453920419062, &#39;beta_2&#39;: 0.20520586612661593, &#39;learning_rate_init&#39;: 7.337546198419339e-06}. 
<span class=" -Color -Color-Green">[I 2021-06-24 13:34:35,326]</span> Trial 54 finished with values: [0.45999999999999996, 0.9478251934051514] and parameters: {&#39;n_layers&#39;: 3, &#39;layer_1_size&#39;: 6, &#39;layer_2_size&#39;: 48, &#39;layer_3_size&#39;: 44, &#39;alpha&#39;: 0.14520299591564892, &#39;tol&#39;: 0.04787757788198888, &#39;beta_1&#39;: 0.3836450839555026, &#39;beta_2&#39;: 0.33950105302325123, &#39;learning_rate_init&#39;: 2.6012407491351593e-05}. 
</pre></div>
</div>
</div>
</div>
<p>We provide a helper function that casts the output of the Optuna experiments into an easier-to-read format.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Helper function that </span>
<span class="k">def</span> <span class="nf">cast_as_df</span><span class="p">(</span><span class="n">optuna_experiment</span><span class="p">,</span> <span class="n">experiment_type</span><span class="p">):</span>
    <span class="n">df</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">optuna_experiment</span>
        <span class="o">.</span><span class="n">trials_dataframe</span><span class="p">()</span>
        <span class="o">.</span><span class="n">rename</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;values_0&quot;</span><span class="p">:</span> <span class="s2">&quot;auprc&quot;</span><span class="p">,</span> <span class="s2">&quot;values_1&quot;</span><span class="p">:</span> <span class="s2">&quot;latency&quot;</span><span class="p">})</span>
        <span class="o">.</span><span class="n">assign</span><span class="p">(</span><span class="n">experiment</span> <span class="o">=</span> <span class="n">experiment_type</span><span class="p">)</span>
    <span class="p">)</span>
    <span class="c1"># the following lines create a column identifing which of the trials resulted in a</span>
    <span class="c1"># Best configuration -- a model with these HPs lies along the empirical Pareto frontier</span>
    <span class="n">best_trials</span> <span class="o">=</span> <span class="p">[</span><span class="n">trial</span><span class="o">.</span><span class="n">number</span> <span class="k">for</span> <span class="n">trial</span> <span class="ow">in</span> <span class="n">optuna_experiment</span><span class="o">.</span><span class="n">best_trials</span><span class="p">]</span>
    <span class="n">df</span><span class="p">[</span><span class="s1">&#39;best_trial&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">number</span><span class="o">.</span><span class="n">isin</span><span class="p">(</span><span class="n">best_trials</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">df</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">motpe_df</span> <span class="o">=</span> <span class="n">cast_as_df</span><span class="p">(</span><span class="n">motpe_experiment</span><span class="p">,</span> <span class="s2">&quot;MOTPE&quot;</span><span class="p">)</span>
<span class="n">random_df</span> <span class="o">=</span> <span class="n">cast_as_df</span><span class="p">(</span><span class="n">random_experiment</span><span class="p">,</span> <span class="s2">&quot;Random&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="id5">
<h3>Visualize Results<a class="headerlink" href="#id5" title="Permalink to this headline">¶</a></h3>
<p>We can visually compare the results of MOPTE and random sampling. We want to see a Pareto frontier of points in the lower right corner of the chart - points with the highest accuracy and lowest latency possible.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">sharey</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">motpe_df</span><span class="o">.</span><span class="n">auprc</span><span class="p">,</span> <span class="n">motpe_df</span><span class="o">.</span><span class="n">latency</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">motpe_df</span><span class="o">.</span><span class="n">experiment</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">random_df</span><span class="o">.</span><span class="n">auprc</span><span class="p">,</span> <span class="n">random_df</span><span class="o">.</span><span class="n">latency</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">random_df</span><span class="o">.</span><span class="n">experiment</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Comparison&quot;</span><span class="p">)</span>

<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">motpe_df</span><span class="o">.</span><span class="n">auprc</span><span class="p">,</span> <span class="n">motpe_df</span><span class="o">.</span><span class="n">latency</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">motpe_df</span><span class="o">.</span><span class="n">number</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="n">motpe_df</span><span class="o">.</span><span class="n">experiment</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

<span class="n">axs</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">random_df</span><span class="o">.</span><span class="n">auprc</span><span class="p">,</span> <span class="n">random_df</span><span class="o">.</span><span class="n">latency</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">random_df</span><span class="o">.</span><span class="n">number</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="n">random_df</span><span class="o">.</span><span class="n">experiment</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

<span class="k">for</span> <span class="n">ax</span> <span class="ow">in</span> <span class="n">axs</span><span class="o">.</span><span class="n">flat</span><span class="p">:</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">xlabel</span><span class="o">=</span><span class="s2">&quot;AUC&quot;</span><span class="p">,</span> <span class="n">ylabel</span><span class="o">=</span><span class="s2">&quot;Latency (1 million points)&quot;</span><span class="p">)</span>

<span class="n">fig</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;ipython-input-12-2d3c23fbdebc&gt;:16: UserWarning: Matplotlib is currently using module://ipykernel.pylab.backend_inline, which is a non-GUI backend, so cannot show the figure.
  fig.show()
</pre></div>
</div>
<img alt="_images/T892775_Exploring_Multi_Objective_Hyperparameter_Optimization_86_1.png" src="_images/T892775_Exploring_Multi_Objective_Hyperparameter_Optimization_86_1.png" />
</div>
</div>
<p>The leftmost panel shows a comparison of the two experiments, while the middle and right panels show each experiment individually, with their points colored according to the order in which the trials were run. Notice in the MOTPE panel that earlier trials (purple points) are scattered about (reflecting the initialization phase), while later trials (yellow points) have honed in on the Pareto frontier. The same cannot be said for the random experiment, which samples randomly for all trials.</p>
<p>Because we generated the dataset randomly, and initialized the sampling randomly, it’s likely your results differ from ours (we could of course have mitigated this by fixing seeds for the random number generators, but we’re very wary of cherry-picking good results, and wanted to encourage some exploration)!  If you’ve already taken a look at our similar <a class="reference external" href="https://github.com/fastforwardlabs/multi-objective-hyperparameter-optimization/tree/master/ax-on-synthetic-data">tutorial using the Ax library</a>, you might notice that MOTPE and Random are often far more separated than Sobol vs qEVHI in that section. While a host of factors contribute to why, we’ll mention two explicitly:</p>
<ol class="simple">
<li><p>MOTPE’s tree-structured surrogate model (rather than the GP surrogate model used by the Ax library) is likely better suited for hierarchical hyperparameter spaces like that explored here</p></li>
<li><p>Not all random sampling is equal: Sobol random sampling (in the Ax section) provides better coverage than pseudo-random sampling (in this section) over fixed hyperparamter spaces.</p></li>
</ol>
<p>With that said, here’s an example of one of the more impressive results we obtained, specifically chosen for its spectacular separation of the MOTPE trials and the random trials. Multi-objective Bayesian optimization found a host of models that are faster <em>and</em> more accurate than random search found!</p>
<p><strong>A pretty great result for MOTPE</strong></p>
<p><center><img src='https://github.com/sparsh-ai/multiobjective-optimizations/blob/main/docs/_images/T892775_4.png?raw=1'></center></p><p>But sometimes, even when MOTPE performs better, it’s not always clear why. For example, in the figure below, MOTPE unequivocally better samples the Pareto frontier, but if we look at the order in which those trials were run, these Pareto-esque points were found very early in the optimization (dark purple) and then the algorithm moved <em>away</em> from the Pareto frontier in the remaining trials, as indicated by the yellow points being scattered all over. So we might just have got lucky this time.</p>
<p><strong>A strange result for MOTPE</strong></p>
<p><center><img src='https://github.com/sparsh-ai/multiobjective-optimizations/blob/main/docs/_images/T892775_5.png?raw=1'></center></p></div>
</div>
<div class="section" id="multi-objective-hyperparameter-optimization-for-credit-card-fraud">
<h2>Multi-objective hyperparameter optimization for credit card fraud<a class="headerlink" href="#multi-objective-hyperparameter-optimization-for-credit-card-fraud" title="Permalink to this headline">¶</a></h2>
<p>In multi-objective hyperparameter optimization, we seek to find the “Pareto frontier” of points representing optimal tradeoffs between multiple objectives. In this example, we trade off prediction latency with predictive performance. The class distribution is heavily imbalanced, so we measure performance with the area under the PR curve. The section compares a Bayesian optimization algorithm: maximizing the expected hypervolume increase (EHVI) using multi-objective tree-structured Parzen estimators (MOTPE), with good old random search.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>!pip install -q -U kaggle
!pip install --upgrade --force-reinstall --no-deps kaggle
!mkdir ~/.kaggle
!cp /content/drive/MyDrive/kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json
!kaggle datasets download -d mlg-ulb/creditcardfraud
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Collecting kaggle
  Downloading kaggle-1.5.12.tar.gz (58 kB)
?25l
     |█████▋                          | 10 kB 24.0 MB/s eta 0:00:01
     |███████████▏                    | 20 kB 19.3 MB/s eta 0:00:01
     |████████████████▊               | 30 kB 23.7 MB/s eta 0:00:01
     |██████████████████████▎         | 40 kB 27.7 MB/s eta 0:00:01
     |███████████████████████████▉    | 51 kB 31.3 MB/s eta 0:00:01
     |████████████████████████████████| 58 kB 4.4 MB/s 
?25hBuilding wheels for collected packages: kaggle
  Building wheel for kaggle (setup.py) ... ?25l?25hdone
  Created wheel for kaggle: filename=kaggle-1.5.12-py3-none-any.whl size=73051 sha256=e242ead18420b915ef01a5ebde5538c731c340ab748dd8772d81ee146918beb7
  Stored in directory: /root/.cache/pip/wheels/62/d6/58/5853130f941e75b2177d281eb7e44b4a98ed46dd155f556dc5
Successfully built kaggle
Installing collected packages: kaggle
  Attempting uninstall: kaggle
    Found existing installation: kaggle 1.5.12
    Uninstalling kaggle-1.5.12:
      Successfully uninstalled kaggle-1.5.12
Successfully installed kaggle-1.5.12
Downloading creditcardfraud.zip to /content
 74% 49.0M/66.0M [00:00&lt;00:00, 65.3MB/s]
100% 66.0M/66.0M [00:00&lt;00:00, 76.7MB/s]
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="multi-objective-hyperparameter-optimization-for-credit-card-fraud-with-optuna">
<h2>Multi-objective hyperparameter optimization for Credit Card Fraud with Optuna<a class="headerlink" href="#multi-objective-hyperparameter-optimization-for-credit-card-fraud-with-optuna" title="Permalink to this headline">¶</a></h2>
<div class="section" id="a-tale-of-two-tradeoffs">
<h3>A tale of two tradeoffs<a class="headerlink" href="#a-tale-of-two-tradeoffs" title="Permalink to this headline">¶</a></h3>
<p>Credit card fraud is a nice problem, from a machine learning perspective, because (at least in it’s naive version) it’s a straightforward binary classification that very likely has a real attributable amount of money associated with true and false positives and negatives. As such, there’s a genuine optimization problem a bank can perform on how to trade off precision and recall. For example, a false positive labels a legit transaction as fraud. While this might be annoying for existing customers who have to call the bank and verify their purchase, it does not result in lost revenue. On the other hand, a false negative labels a fraudulent transaction as legit, and this has significant costs, both in terms of lost revenue and a potentially angry customer.</p>
<p>While we <em>could</em> train a model to minimize false negatives (FN) or false positives (FP), or even accuracy – these come with considerable drawbacks. Minimizing either of FPs or FNs ignores the impact of the other. Even worse, as business needs change (say, the real cost associated with FPs increases) then a new model must be trained from scratch to address the updated business requirements. Optimizing for pure accuracy might be okay for balanced datasets but leads to overly-optimistic models when working with unbalanced datasets (since the model can achieve a high score by simply labeled <em>everything</em> as nonfraudulent).</p>
<p>In reality, the precision-recall tradeoff is ultimately a <em>business decision</em> and should be made in conversation with expert business stakeholders. As machine learning engineers, our job is to give the business stakeholder the most favourable precision and recall curve possible, so that we trade off the least amount of precision for recall, or vice versa. To do this, we’ll optimize for the area under the PR curve (AUPRC). A model optimized for AUPRC can be used at several thresholds without retraining to accomodate evolving business needs. Additionally, AUPRC is meaningful even for imbalanced datasets, and credit card fraud is highly imbalanced; most transactions are not fraudulent.</p>
<p>But there is another relevant tradeoff to consider! Not only should our model have strong predictive power, it should also be fast at making predictions. Fraudulent transactions should be detected quickly to reduce the most harm, and there may even be a hard constraint on the allowable latency from prediction. So we actually care about both AUPRC and latency. Unfortunately, there is no way for a single model to exhibit a wide range of AUPRC-latency values – there’s no “threshold” to sweep across like there is with precision vs recall. Instead, each model we train will have a different combination of predictive power and latency (inherited by our choice of hyperparameters) and our job will be to find the <em>set</em> of such models that represent the optimal tradeoffs between the two. This sounds like a job for multi-objective Bayesian optimization!</p>
<p>In this section, we’ll demonstrate how to perform multi-objective Bayesian optimization to a canonical credit card fraud dataset. We’ll consider an XGBoost model for several reasons:</p>
<ol class="simple">
<li><p>it’s a powerful and well-known algorithm that works in many situations</p></li>
<li><p>tree-based models often have prediction efficacy – prediction latency tradeoffs: more/deeper trees usually provide better predictions but generally take longer at inference time</p></li>
<li><p>XGBoost has many hyperparameters in addition to <code class="docutils literal notranslate"><span class="pre">max_depth</span></code> that we might wish to optimize, and their relationship with our output objectives (AUPRC and latency) is unclear</p></li>
</ol>
<p>Below we start with some install and imports.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">time</span>
<span class="kn">import</span> <span class="nn">warnings</span>

<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">optuna</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">import</span> <span class="nn">xgboost</span> <span class="k">as</span> <span class="nn">xgb</span>

<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">auc</span><span class="p">,</span> <span class="n">precision_recall_curve</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">cross_validate</span><span class="p">,</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.pipeline</span> <span class="kn">import</span> <span class="n">Pipeline</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s1">&#39;ignore&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="id6">
<h3>Data<a class="headerlink" href="#id6" title="Permalink to this headline">¶</a></h3>
<p>In this section we use the anonymized <a class="reference external" href="https://www.kaggle.com/mlg-ulb/creditcardfraud">Credit Card Fraud dataset on Kaggle</a>. Sign-in is required to download this dataset. We have already performed this step and stored the dataset at <code class="docutils literal notranslate"><span class="pre">[project_directory]/data</span></code>, so we’ll load it from there.</p>
<p>The data comes somewhat pre-feature-engineered, having gone through (at least) PCA. This was done in the name of privacy preservation, but also means the data is in reasonable shape to put directly into a supervised learning pipeline.</p>
<p>In an unrealistic (but possibly good) assumption, we’ll exclude the time variable and treat the remaining features as i.i.d. While it’s possbile that fraudulent transactions are more likely at a certain time, creating a classifier that discriminates on the basis of time presents an easily-exploitable means of improving the likelihood of a successful fraudulent transaction, as an adversary. It’s likely that time is confounded with the other variables from which features were derived. Unfortunately, we have no way of knowing. Causal inference always requires model assumptions, and is impossible with obfuscated variables.</p>
<p>The task is “point-like” prediction of fraud, in the sense that transactions are not linked by any ID, so we can do no analysis of events from the same card over time. As such, each incoming datapoint will be subject to a prediction independent of the others. Time could be used as a feature, but only in the naive sense (time of day, etc). Neglecting time seems appropriate for this point-like prediction task.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">cc</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&quot;creditcard.csv&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s2">&quot;Time&quot;</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="s2">&quot;columns&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>We’ll also separate the label column (<code class="docutils literal notranslate"><span class="pre">Class</span></code>) from the features in the dataset.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">X</span> <span class="o">=</span> <span class="n">cc</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s2">&quot;Class&quot;</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="s2">&quot;columns&quot;</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">cc</span><span class="o">.</span><span class="n">Class</span>
</pre></div>
</div>
</div>
</div>
<p>The target classes are heavily imbalanced.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">class_counts</span> <span class="o">=</span> <span class="n">cc</span><span class="o">.</span><span class="n">Class</span><span class="o">.</span><span class="n">value_counts</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">There are </span><span class="si">{</span><span class="n">class_counts</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s2"> identified cases of fraud in </span><span class="si">{</span><span class="n">class_counts</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="si">}</span><span class="s2"> total cases.</span>
<span class="s2">That&#39;s only </span><span class="si">{</span><span class="mi">100</span> <span class="o">*</span> <span class="n">class_counts</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">/</span><span class="n">class_counts</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">%!</span>
<span class="s2">&quot;&quot;&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>There are 492 identified cases of fraud in 284807 total cases.
That&#39;s only 0.17%!
</pre></div>
</div>
</div>
</div>
<p>Next we’ll perform a standard train/dev split. However, if this model were to be put into production, we’d want to also create a dev/test split so as not to overfit to our dev set.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_dev</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_dev</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">stratify</span><span class="o">=</span><span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>There are no missing entries in the data set.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">cc</span><span class="o">.</span><span class="n">isna</span><span class="p">()</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>False
</pre></div>
</div>
</div>
</div>
<p>We now have a set of features <code class="docutils literal notranslate"><span class="pre">V1</span></code> through <code class="docutils literal notranslate"><span class="pre">V28</span></code> which are the output of the privacy-preserving PCA for each transaction, along with the <code class="docutils literal notranslate"><span class="pre">Amount</span></code> column. Let’s get a feel for how these features look and how they compare across classes.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">X_train</span><span class="o">.</span><span class="n">describe</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>V1</th>
      <th>V2</th>
      <th>V3</th>
      <th>V4</th>
      <th>V5</th>
      <th>V6</th>
      <th>V7</th>
      <th>V8</th>
      <th>V9</th>
      <th>V10</th>
      <th>...</th>
      <th>V20</th>
      <th>V21</th>
      <th>V22</th>
      <th>V23</th>
      <th>V24</th>
      <th>V25</th>
      <th>V26</th>
      <th>V27</th>
      <th>V28</th>
      <th>Amount</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>199364.000000</td>
      <td>199364.000000</td>
      <td>199364.000000</td>
      <td>199364.000000</td>
      <td>199364.000000</td>
      <td>199364.000000</td>
      <td>199364.000000</td>
      <td>199364.000000</td>
      <td>199364.000000</td>
      <td>199364.000000</td>
      <td>...</td>
      <td>199364.000000</td>
      <td>199364.000000</td>
      <td>199364.000000</td>
      <td>199364.000000</td>
      <td>199364.000000</td>
      <td>199364.000000</td>
      <td>199364.000000</td>
      <td>199364.000000</td>
      <td>199364.000000</td>
      <td>199364.000000</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>0.002782</td>
      <td>-0.001017</td>
      <td>-0.001026</td>
      <td>-0.000179</td>
      <td>0.001403</td>
      <td>0.001152</td>
      <td>0.002189</td>
      <td>-0.001126</td>
      <td>-0.000176</td>
      <td>-0.001169</td>
      <td>...</td>
      <td>-0.001221</td>
      <td>-0.000514</td>
      <td>0.000803</td>
      <td>-0.000591</td>
      <td>0.000665</td>
      <td>0.000083</td>
      <td>-0.000136</td>
      <td>-0.000114</td>
      <td>-0.000129</td>
      <td>88.424873</td>
    </tr>
    <tr>
      <th>std</th>
      <td>1.955035</td>
      <td>1.650328</td>
      <td>1.509339</td>
      <td>1.413776</td>
      <td>1.363592</td>
      <td>1.326988</td>
      <td>1.217031</td>
      <td>1.190037</td>
      <td>1.098078</td>
      <td>1.083900</td>
      <td>...</td>
      <td>0.772523</td>
      <td>0.732521</td>
      <td>0.726467</td>
      <td>0.636842</td>
      <td>0.605492</td>
      <td>0.522166</td>
      <td>0.482836</td>
      <td>0.400808</td>
      <td>0.327791</td>
      <td>249.621608</td>
    </tr>
    <tr>
      <th>min</th>
      <td>-41.928738</td>
      <td>-63.344698</td>
      <td>-32.965346</td>
      <td>-5.683171</td>
      <td>-40.427726</td>
      <td>-21.929312</td>
      <td>-43.557242</td>
      <td>-50.688419</td>
      <td>-13.434066</td>
      <td>-24.588262</td>
      <td>...</td>
      <td>-28.009635</td>
      <td>-22.889347</td>
      <td>-10.933144</td>
      <td>-44.807735</td>
      <td>-2.836627</td>
      <td>-10.295397</td>
      <td>-2.604551</td>
      <td>-22.565679</td>
      <td>-11.710896</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>25%</th>
      <td>-0.919352</td>
      <td>-0.596672</td>
      <td>-0.891617</td>
      <td>-0.847025</td>
      <td>-0.690694</td>
      <td>-0.768760</td>
      <td>-0.553161</td>
      <td>-0.209098</td>
      <td>-0.641332</td>
      <td>-0.535903</td>
      <td>...</td>
      <td>-0.212083</td>
      <td>-0.228452</td>
      <td>-0.542162</td>
      <td>-0.161974</td>
      <td>-0.354043</td>
      <td>-0.317239</td>
      <td>-0.327428</td>
      <td>-0.070636</td>
      <td>-0.052863</td>
      <td>5.520000</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>0.021603</td>
      <td>0.066449</td>
      <td>0.177854</td>
      <td>-0.019554</td>
      <td>-0.054717</td>
      <td>-0.274843</td>
      <td>0.041344</td>
      <td>0.021957</td>
      <td>-0.050598</td>
      <td>-0.093471</td>
      <td>...</td>
      <td>-0.063053</td>
      <td>-0.029246</td>
      <td>0.007802</td>
      <td>-0.011077</td>
      <td>0.041296</td>
      <td>0.017396</td>
      <td>-0.053530</td>
      <td>0.001343</td>
      <td>0.011316</td>
      <td>21.960000</td>
    </tr>
    <tr>
      <th>75%</th>
      <td>1.315787</td>
      <td>0.802569</td>
      <td>1.025892</td>
      <td>0.741960</td>
      <td>0.614586</td>
      <td>0.400368</td>
      <td>0.573119</td>
      <td>0.327845</td>
      <td>0.595768</td>
      <td>0.452529</td>
      <td>...</td>
      <td>0.132733</td>
      <td>0.186195</td>
      <td>0.530376</td>
      <td>0.147431</td>
      <td>0.440686</td>
      <td>0.350621</td>
      <td>0.241311</td>
      <td>0.091280</td>
      <td>0.078230</td>
      <td>76.900000</td>
    </tr>
    <tr>
      <th>max</th>
      <td>2.454930</td>
      <td>22.057729</td>
      <td>9.382558</td>
      <td>16.875344</td>
      <td>32.911462</td>
      <td>23.917837</td>
      <td>44.054461</td>
      <td>19.587773</td>
      <td>15.594995</td>
      <td>23.745136</td>
      <td>...</td>
      <td>39.420904</td>
      <td>27.202839</td>
      <td>8.361985</td>
      <td>22.528412</td>
      <td>4.022866</td>
      <td>7.519589</td>
      <td>3.463246</td>
      <td>12.152401</td>
      <td>22.620072</td>
      <td>19656.530000</td>
    </tr>
  </tbody>
</table>
<p>8 rows × 29 columns</p>
</div></div></div>
</div>
<p>Maybe some features will be well correlated with the target. Let’s do a visual spot check. Because there’s such a huge class imbalance, we randomly downsample the dominant class to match the prevalence of the minority class.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">melted_cc</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">cc</span><span class="p">[</span><span class="n">cc</span><span class="o">.</span><span class="n">Class</span> <span class="o">==</span> <span class="mi">1</span><span class="p">],</span> <span class="n">cc</span><span class="p">[</span><span class="n">cc</span><span class="o">.</span><span class="n">Class</span> <span class="o">==</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="mi">492</span><span class="p">)])</span><span class="c1">#</span>
    <span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s2">&quot;Amount&quot;</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="s2">&quot;columns&quot;</span><span class="p">)</span>
    <span class="o">.</span><span class="n">melt</span><span class="p">(</span><span class="n">id_vars</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;Class&quot;</span><span class="p">],</span>
          <span class="n">value_vars</span><span class="o">=</span><span class="p">[</span><span class="n">c</span> <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">cc</span><span class="o">.</span><span class="n">columns</span> <span class="k">if</span> <span class="n">c</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;Class&quot;</span><span class="p">,</span> <span class="s2">&quot;Amount&quot;</span><span class="p">]])</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">grid</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">FacetGrid</span><span class="p">(</span><span class="n">melted_cc</span><span class="p">,</span> <span class="n">col</span><span class="o">=</span><span class="s2">&quot;variable&quot;</span><span class="p">,</span> <span class="n">col_wrap</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="n">grid</span><span class="o">.</span><span class="n">map_dataframe</span><span class="p">(</span><span class="n">sns</span><span class="o">.</span><span class="n">histplot</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="s2">&quot;value&quot;</span><span class="p">,</span> <span class="n">hue</span><span class="o">=</span><span class="s2">&quot;Class&quot;</span><span class="p">,</span> <span class="n">element</span><span class="o">=</span><span class="s2">&quot;step&quot;</span><span class="p">)</span>
<span class="n">grid</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">xlim</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span><span class="mi">10</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;seaborn.axisgrid.FacetGrid at 0x138483610&gt;
</pre></div>
</div>
<img alt="_images/T892775_Exploring_Multi_Objective_Hyperparameter_Optimization_114_1.png" src="_images/T892775_Exploring_Multi_Objective_Hyperparameter_Optimization_114_1.png" />
</div>
</div>
<p>The scale of the <code class="docutils literal notranslate"><span class="pre">Amount</span></code> variable differs from the PCA variables above, so we plot it separately (again, downsampling non-fraudulent transactions).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">sns</span><span class="o">.</span><span class="n">histplot</span><span class="p">(</span>
    <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">cc</span><span class="p">[</span><span class="n">cc</span><span class="o">.</span><span class="n">Class</span> <span class="o">==</span> <span class="mi">1</span><span class="p">],</span> <span class="n">cc</span><span class="p">[</span><span class="n">cc</span><span class="o">.</span><span class="n">Class</span> <span class="o">==</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="mi">492</span><span class="p">)]),</span> <span class="c1"># downsample</span>
    <span class="n">x</span><span class="o">=</span><span class="s2">&quot;Amount&quot;</span><span class="p">,</span> <span class="n">hue</span><span class="o">=</span><span class="s2">&quot;Class&quot;</span><span class="p">,</span> <span class="n">element</span><span class="o">=</span><span class="s2">&quot;step&quot;</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">100</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;AxesSubplot:xlabel=&#39;Amount&#39;, ylabel=&#39;Count&#39;&gt;
</pre></div>
</div>
<img alt="_images/T892775_Exploring_Multi_Objective_Hyperparameter_Optimization_116_1.png" src="_images/T892775_Exploring_Multi_Objective_Hyperparameter_Optimization_116_1.png" />
</div>
</div>
<p>The distribution of fraudulent transactions looks relatively well spread over the transaction size (recalling that downsampling non-fraudulent transactions so severely introduces a lot of statistical noise - run the function repeatedly to draw new samples). It looks like there’s some reasonable separation between some of the PCA feature distributions though. This signals that there’s a good chance we can learn to discriminate fraudulent transactions on the basis of these features. (Even those distributions that overlap extremely well may be useful in combination with other features).</p>
<p>Another thing to note is that the features are not perfect standard normal distributions, but are all close to mean zero and standard deviation one, except the transaction amount. Taking all features <code class="docutils literal notranslate"><span class="pre">V1</span></code> through <code class="docutils literal notranslate"><span class="pre">V28</span></code> and <code class="docutils literal notranslate"><span class="pre">Amount</span></code> as our feature matrix, we can apply standard scaling to all (it will not harm those already scaled, and performance is not a concern).</p>
<p>Now that we have a good idea about the distribution of our data and the features we have to work with, it’s time to find the set of models that optimize the AUPRC-latency multi-objective space.</p>
</div>
<div class="section" id="hyperparameter-optimization-with-optuna">
<h3>Hyperparameter optimization with Optuna<a class="headerlink" href="#hyperparameter-optimization-with-optuna" title="Permalink to this headline">¶</a></h3>
<p>The first step on our optimization journey is to define our optimization targets. We have two: AUPRC and latency. Measuring AUPRC is straightforward with <code class="docutils literal notranslate"><span class="pre">sklearn</span></code>. For latency, we’ll time how long it takes to infer on one million randomly sampled datapoints.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">million_points</span> <span class="o">=</span> <span class="n">X_train</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="mf">1e6</span><span class="p">),</span> <span class="n">replace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">area_under_pr_curve</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="n">precision</span><span class="p">,</span> <span class="n">recall</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">precision_recall_curve</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>
    <span class="n">area</span> <span class="o">=</span> <span class="n">auc</span><span class="p">(</span><span class="n">recall</span><span class="p">,</span> <span class="n">precision</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">area</span>

<span class="k">def</span> <span class="nf">latency</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
    <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
    <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="n">end</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">end</span> <span class="o">-</span> <span class="n">start</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="training-routine">
<h3>Training routine<a class="headerlink" href="#training-routine" title="Permalink to this headline">¶</a></h3>
<p>Next we define the training routine that will allow Optuna to perform hyperparameter optimization. We pass the function a <code class="docutils literal notranslate"><span class="pre">trial</span></code> object through which Optuna will suggest a new value for each hyperparameter according to a pre-defined sampling strategy for each trial we run.</p>
<p><strong>Note:</strong> Optuna will train and evaluate a model for each trial. While the hyperparameter values, metric scores, and additional metadata are saved from each trial, the model itself is discarded. This means we would have to retrain our preferred model once optimization was complete. For most cases, this isn’t an issue, however it is possible to save each model to disk during the optimization loop. Models of interest can then be fetched using the associated trial number.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">trial</span><span class="p">):</span>
    <span class="n">pipeline</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">([</span>
        <span class="p">(</span><span class="s1">&#39;scaler&#39;</span><span class="p">,</span> <span class="n">StandardScaler</span><span class="p">()),</span>
        <span class="p">(</span><span class="s1">&#39;xgboost&#39;</span><span class="p">,</span> <span class="n">xgb</span><span class="o">.</span><span class="n">XGBClassifier</span><span class="p">(</span>
            <span class="n">eval_metric</span> <span class="o">=</span> <span class="s2">&quot;logloss&quot;</span><span class="p">,</span>
            <span class="n">use_label_encoder</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
            <span class="n">learning_rate</span> <span class="o">=</span>    <span class="n">trial</span><span class="o">.</span><span class="n">suggest_float</span><span class="p">(</span><span class="s2">&quot;learning_rate&quot;</span><span class="p">,</span> <span class="mf">1e-6</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">log</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
            <span class="n">n_estimators</span> <span class="o">=</span>     <span class="n">trial</span><span class="o">.</span><span class="n">suggest_int</span><span class="p">(</span><span class="s2">&quot;n_estimators&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">500</span><span class="p">),</span>
            <span class="n">gamma</span> <span class="o">=</span>            <span class="n">trial</span><span class="o">.</span><span class="n">suggest_float</span><span class="p">(</span><span class="s2">&quot;gamma&quot;</span><span class="p">,</span> <span class="mf">1e-6</span><span class="p">,</span> <span class="mf">1e1</span><span class="p">,</span> <span class="n">log</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
            <span class="n">max_depth</span> <span class="o">=</span>        <span class="n">trial</span><span class="o">.</span><span class="n">suggest_int</span><span class="p">(</span><span class="s2">&quot;max_depth&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">20</span><span class="p">),</span>
            <span class="n">min_child_weight</span> <span class="o">=</span> <span class="n">trial</span><span class="o">.</span><span class="n">suggest_float</span><span class="p">(</span><span class="s2">&quot;min_child_weight&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span>
            <span class="n">max_delta_step</span> <span class="o">=</span>   <span class="n">trial</span><span class="o">.</span><span class="n">suggest_float</span><span class="p">(</span><span class="s2">&quot;max_delta_step&quot;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span>
            <span class="n">subsample</span> <span class="o">=</span>        <span class="n">trial</span><span class="o">.</span><span class="n">suggest_float</span><span class="p">(</span><span class="s2">&quot;subsample&quot;</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
            <span class="n">reg_lambda</span> <span class="o">=</span>       <span class="n">trial</span><span class="o">.</span><span class="n">suggest_float</span><span class="p">(</span><span class="s2">&quot;lambda&quot;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span>
            <span class="n">reg_alpha</span> <span class="o">=</span>        <span class="n">trial</span><span class="o">.</span><span class="n">suggest_float</span><span class="p">(</span><span class="s2">&quot;alpha&quot;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
        <span class="p">))</span>
    <span class="p">])</span>

    <span class="c1"># if we were patient, we could cross-validate here and</span>
    <span class="c1"># report metrics on the average hold-out score</span>
    <span class="n">pipeline</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
    
    <span class="c1"># evaluate metrics</span>
    <span class="n">auprc</span> <span class="o">=</span> <span class="n">area_under_pr_curve</span><span class="p">(</span><span class="n">pipeline</span><span class="p">,</span> <span class="n">X_dev</span><span class="p">,</span> <span class="n">y_dev</span><span class="p">)</span>
    <span class="n">lat</span> <span class="o">=</span> <span class="n">latency</span><span class="p">(</span><span class="n">pipeline</span><span class="p">,</span> <span class="n">million_points</span><span class="p">)</span>

    <span class="c1"># save a trained model to a file</span>
    <span class="k">if</span> <span class="n">SAVE_MODELS</span><span class="p">:</span>
        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s2">&quot;../output/models/</span><span class="si">{}</span><span class="s2">.pickle&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">trial</span><span class="o">.</span><span class="n">number</span><span class="p">),</span> <span class="s2">&quot;wb&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">fout</span><span class="p">:</span>
            <span class="n">pickle</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="n">pipeline</span><span class="p">,</span> <span class="n">fout</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">auprc</span><span class="p">,</span> <span class="n">lat</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="id7">
<h3>Experiment setup<a class="headerlink" href="#id7" title="Permalink to this headline">¶</a></h3>
<p>We’ll first use a multi-objective Bayesian optimization algorithm (MOTPE) to cleverly sample hyperparameter values that (hopefully) optimize both our objectives. You can read more about how the MOTPE algorithm works in our blog post <a class="reference external" href="https://blog.fastforwardlabs.com/2021/07/07/exploring-multi-objective-hyperparameter-optimization.html">Exploring Multi-Objective Hyperparameter Optimization</a>.</p>
<p>Setting up an experiment is easy. The first line defines the sampling strategy. The second creates an Optuna <code class="docutils literal notranslate"><span class="pre">study</span></code> which tracks the details of the optimization run. For this, we need to tell Optuna how to optimize our objectives. In this case we want to <em>maximize</em> AUPRC and <em>minimize</em> latency. The order should reflect the order in which the values are passed out from the <code class="docutils literal notranslate"><span class="pre">train</span></code> routine above. Finally, we execute the experiment with a call to the <code class="docutils literal notranslate"><span class="pre">optimize</span></code> method, where we pass it the function we wish to optimize along with the number of trials for the optimization process.</p>
<p><strong>Note:</strong> The experiments in this section take a considerable amount of time to execute since each performs 50 trials and each trial trains a new XGBoost model on 200K credit card transactions. Expect these cells to take several minutes or more, depending on your hardware. If you don’t want to wait, we’ve included a set of results from our experiments that you can examine in the next section.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">N_TRIAL</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">SAVE_MODELS</span> <span class="o">=</span> <span class="kc">False</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">motpe_sampler</span> <span class="o">=</span> <span class="n">optuna</span><span class="o">.</span><span class="n">samplers</span><span class="o">.</span><span class="n">MOTPESampler</span><span class="p">()</span>
<span class="n">motpe_experiment</span> <span class="o">=</span> <span class="n">optuna</span><span class="o">.</span><span class="n">create_study</span><span class="p">(</span><span class="n">sampler</span><span class="o">=</span><span class="n">motpe_sampler</span><span class="p">,</span> <span class="n">directions</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;maximize&quot;</span><span class="p">,</span> <span class="s2">&quot;minimize&quot;</span><span class="p">])</span>
<span class="n">motpe_experiment</span><span class="o">.</span><span class="n">optimize</span><span class="p">(</span><span class="n">train</span><span class="p">,</span> <span class="n">n_trials</span><span class="o">=</span><span class="n">N_TRIAL</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span><span class=" -Color -Color-Green">[I 2021-06-24 20:46:20,545]</span> A new study created in memory with name: no-name-fd328da8-cd5e-4bf5-84b5-7ea6bcfb3561
<span class=" -Color -Color-Green">[I 2021-06-24 20:46:27,486]</span> Trial 0 finished with values: [0.7218528003744463, 0.6946229934692383] and parameters: {&#39;learning_rate&#39;: 0.0001758226885007034, &#39;n_estimators&#39;: 258, &#39;gamma&#39;: 0.000254816035807138, &#39;max_depth&#39;: 1, &#39;min_child_weight&#39;: 1.5844561218068764, &#39;max_delta_step&#39;: 7.269360054927513, &#39;subsample&#39;: 0.21175314615388371, &#39;lambda&#39;: 9.398630749093787, &#39;alpha&#39;: 4.86142669944999}. 
<span class=" -Color -Color-Green">[I 2021-06-24 20:46:59,324]</span> Trial 1 finished with values: [0.8099176061326265, 1.3288612365722656] and parameters: {&#39;learning_rate&#39;: 0.0006278354697490014, &#39;n_estimators&#39;: 253, &#39;gamma&#39;: 0.00010440178614951517, &#39;max_depth&#39;: 16, &#39;min_child_weight&#39;: 6.102792411013507, &#39;max_delta_step&#39;: 8.974872787219288, &#39;subsample&#39;: 0.5194152189405955, &#39;lambda&#39;: 1.9968383305021187, &#39;alpha&#39;: 0.5159333571793945}. 
<span class=" -Color -Color-Green">[I 2021-06-24 20:47:21,222]</span> Trial 2 finished with values: [0.7893394252069803, 1.1087522506713867] and parameters: {&#39;learning_rate&#39;: 2.5067498861796192e-06, &#39;n_estimators&#39;: 188, &#39;gamma&#39;: 1.1910314273054912, &#39;max_depth&#39;: 7, &#39;min_child_weight&#39;: 4.272659085759665, &#39;max_delta_step&#39;: 0.34068854689683015, &#39;subsample&#39;: 0.489353021096681, &#39;lambda&#39;: 1.9956722408009298, &#39;alpha&#39;: 8.433636306977103}. 
<span class=" -Color -Color-Green">[I 2021-06-24 20:47:56,768]</span> Trial 3 finished with values: [0.8732328870535043, 1.1199331283569336] and parameters: {&#39;learning_rate&#39;: 0.10307696192673896, &#39;n_estimators&#39;: 166, &#39;gamma&#39;: 0.019271265144421266, &#39;max_depth&#39;: 12, &#39;min_child_weight&#39;: 1.9957862993965563, &#39;max_delta_step&#39;: 3.1969446273710522, &#39;subsample&#39;: 0.7687886009748368, &#39;lambda&#39;: 5.547020366963203, &#39;alpha&#39;: 6.047288450805627}. 
<span class=" -Color -Color-Green">[I 2021-06-24 20:48:04,187]</span> Trial 4 finished with values: [0.7907219480285735, 0.7253270149230957] and parameters: {&#39;learning_rate&#39;: 0.0007950956451248601, &#39;n_estimators&#39;: 72, &#39;gamma&#39;: 0.2588302475297265, &#39;max_depth&#39;: 20, &#39;min_child_weight&#39;: 9.980249211870053, &#39;max_delta_step&#39;: 0.5070052767412336, &#39;subsample&#39;: 0.4427839167037173, &#39;lambda&#39;: 9.18342131974135, &#39;alpha&#39;: 6.142891353891624}. 
<span class=" -Color -Color-Green">[I 2021-06-24 20:48:05,755]</span> Trial 5 finished with values: [0.8372239335890732, 0.5506250858306885] and parameters: {&#39;learning_rate&#39;: 1.093648713084533e-05, &#39;n_estimators&#39;: 3, &#39;gamma&#39;: 3.9664972554432026e-05, &#39;max_depth&#39;: 13, &#39;min_child_weight&#39;: 3.432275441737101, &#39;max_delta_step&#39;: 1.8554617684534436, &#39;subsample&#39;: 0.9681059122367377, &#39;lambda&#39;: 5.933343041458691, &#39;alpha&#39;: 5.453249163711615}. 
<span class=" -Color -Color-Green">[I 2021-06-24 20:48:32,244]</span> Trial 6 finished with values: [0.8819907134791205, 0.9833920001983643] and parameters: {&#39;learning_rate&#39;: 0.821029533602367, &#39;n_estimators&#39;: 369, &#39;gamma&#39;: 2.2161757014229365e-05, &#39;max_depth&#39;: 15, &#39;min_child_weight&#39;: 1.4307189206386086, &#39;max_delta_step&#39;: 7.301499127258594, &#39;subsample&#39;: 0.5187790874065326, &#39;lambda&#39;: 2.090188681418923, &#39;alpha&#39;: 9.087315864563003}. 
<span class=" -Color -Color-Green">[I 2021-06-24 20:49:37,820]</span> Trial 7 finished with values: [0.8164952608616804, 1.7470707893371582] and parameters: {&#39;learning_rate&#39;: 6.737309426800943e-06, &#39;n_estimators&#39;: 430, &#39;gamma&#39;: 0.6868115423128701, &#39;max_depth&#39;: 19, &#39;min_child_weight&#39;: 2.581291915040649, &#39;max_delta_step&#39;: 2.8543903924150884, &#39;subsample&#39;: 0.6653646048159492, &#39;lambda&#39;: 9.484410488892946, &#39;alpha&#39;: 8.072584857843378}. 
<span class=" -Color -Color-Green">[I 2021-06-24 20:49:57,615]</span> Trial 8 finished with values: [0.8110071208354481, 0.9040791988372803] and parameters: {&#39;learning_rate&#39;: 0.0031677284739884772, &#39;n_estimators&#39;: 163, &#39;gamma&#39;: 0.00010702421330864722, &#39;max_depth&#39;: 18, &#39;min_child_weight&#39;: 8.856112044580582, &#39;max_delta_step&#39;: 5.093373981255441, &#39;subsample&#39;: 0.6003872671567319, &#39;lambda&#39;: 8.046720356388816, &#39;alpha&#39;: 6.356458751623912}. 
<span class=" -Color -Color-Green">[I 2021-06-24 20:50:08,799]</span> Trial 9 finished with values: [0.8611497321132064, 0.7889628410339355] and parameters: {&#39;learning_rate&#39;: 0.17112557188756733, &#39;n_estimators&#39;: 72, &#39;gamma&#39;: 0.001117269613116781, &#39;max_depth&#39;: 9, &#39;min_child_weight&#39;: 2.822529905930525, &#39;max_delta_step&#39;: 9.712416583652026, &#39;subsample&#39;: 0.5366657462471061, &#39;lambda&#39;: 6.346963249441826, &#39;alpha&#39;: 5.722158639756152}. 
<span class=" -Color -Color-Green">[I 2021-06-24 20:50:12,772]</span> Trial 10 finished with values: [0.8190593634257831, 0.6103942394256592] and parameters: {&#39;learning_rate&#39;: 2.8328548258503566e-05, &#39;n_estimators&#39;: 22, &#39;gamma&#39;: 1.4401765478644015e-06, &#39;max_depth&#39;: 3, &#39;min_child_weight&#39;: 6.3228651430797616, &#39;max_delta_step&#39;: 2.2128374822198724, &#39;subsample&#39;: 0.9875156223384347, &#39;lambda&#39;: 3.72546222321344, &#39;alpha&#39;: 2.6837190840802956}. 
<span class=" -Color -Color-Green">[I 2021-06-24 20:50:14,820]</span> Trial 11 finished with values: [0.8212108153131968, 0.5293130874633789] and parameters: {&#39;learning_rate&#39;: 1.3169956839423796e-06, &#39;n_estimators&#39;: 6, &#39;gamma&#39;: 3.7679523765754575e-06, &#39;max_depth&#39;: 12, &#39;min_child_weight&#39;: 4.408260493980597, &#39;max_delta_step&#39;: 5.321975475095146, &#39;subsample&#39;: 0.9906490751020187, &#39;lambda&#39;: 7.084676807945309, &#39;alpha&#39;: 0.02885143092349729}. 
<span class=" -Color -Color-Green">[I 2021-06-24 20:51:53,340]</span> Trial 12 finished with values: [0.8096040355096351, 2.123795986175537] and parameters: {&#39;learning_rate&#39;: 3.574248230763069e-05, &#39;n_estimators&#39;: 488, &#39;gamma&#39;: 0.010071406562763044, &#39;max_depth&#39;: 5, &#39;min_child_weight&#39;: 7.081831477143929, &#39;max_delta_step&#39;: 1.469450495185323, &#39;subsample&#39;: 0.8626282231677532, &#39;lambda&#39;: 0.11549233033703388, &#39;alpha&#39;: 2.8966693486582793}. 
<span class=" -Color -Color-Green">[I 2021-06-24 20:52:23,776]</span> Trial 13 finished with values: [0.8217102835322941, 1.382354736328125] and parameters: {&#39;learning_rate&#39;: 0.005362613188277339, &#39;n_estimators&#39;: 339, &#39;gamma&#39;: 1.29554214578235e-05, &#39;max_depth&#39;: 14, &#39;min_child_weight&#39;: 4.27102010974796, &#39;max_delta_step&#39;: 3.876823085247566, &#39;subsample&#39;: 0.33643603206419503, &#39;lambda&#39;: 4.168418113270792, &#39;alpha&#39;: 3.114876523744128}. 
<span class=" -Color -Color-Green">[I 2021-06-24 20:52:36,083]</span> Trial 14 finished with values: [0.8154368154679541, 0.7646341323852539] and parameters: {&#39;learning_rate&#39;: 2.494051364512187e-05, &#39;n_estimators&#39;: 89, &#39;gamma&#39;: 0.0018306237881515531, &#39;max_depth&#39;: 9, &#39;min_child_weight&#39;: 7.732851360809009, &#39;max_delta_step&#39;: 0.02361847155997676, &#39;subsample&#39;: 0.8516146614316329, &#39;lambda&#39;: 7.572027410560815, &#39;alpha&#39;: 1.6380365775549208}. 
<span class=" -Color -Color-Green">[I 2021-06-24 20:53:26,097]</span> Trial 15 finished with values: [0.8633044164736345, 1.428790807723999] and parameters: {&#39;learning_rate&#39;: 0.013784803227981588, &#39;n_estimators&#39;: 308, &#39;gamma&#39;: 0.05031346438224321, &#39;max_depth&#39;: 17, &#39;min_child_weight&#39;: 3.374623811387655, &#39;max_delta_step&#39;: 1.4581067498184277, &#39;subsample&#39;: 0.7359146331941624, &#39;lambda&#39;: 4.719590009902535, &#39;alpha&#39;: 4.472362077695003}. 
<span class=" -Color -Color-Green">[I 2021-06-24 20:54:58,279]</span> Trial 16 finished with values: [0.8235868808857589, 2.006478786468506] and parameters: {&#39;learning_rate&#39;: 6.082728364930827e-06, &#39;n_estimators&#39;: 498, &#39;gamma&#39;: 1.0991666927093917e-06, &#39;max_depth&#39;: 12, &#39;min_child_weight&#39;: 5.245900719048517, &#39;max_delta_step&#39;: 6.279545736187093, &#39;subsample&#39;: 0.8811278835093631, &#39;lambda&#39;: 3.359483893211821, &#39;alpha&#39;: 7.247337373259028}. 
<span class=" -Color -Color-Green">[I 2021-06-24 20:55:29,456]</span> Trial 17 finished with values: [0.8296553899158493, 0.9752237796783447] and parameters: {&#39;learning_rate&#39;: 0.0001558669988145356, &#39;n_estimators&#39;: 119, &#39;gamma&#39;: 9.687002571688614, &#39;max_depth&#39;: 7, &#39;min_child_weight&#39;: 1.042021283438359, &#39;max_delta_step&#39;: 3.977586096516638, &#39;subsample&#39;: 0.9949142096312764, &#39;lambda&#39;: 0.49316790110574615, &#39;alpha&#39;: 9.81870345582193}. 
<span class=" -Color -Color-Green">[I 2021-06-24 20:55:30,578]</span> Trial 18 finished with values: [0.8029202507075638, 0.519097089767456] and parameters: {&#39;learning_rate&#39;: 0.00013030941908511435, &#39;n_estimators&#39;: 2, &#39;gamma&#39;: 1.241334065455237e-05, &#39;max_depth&#39;: 14, &#39;min_child_weight&#39;: 3.479010449552889, &#39;max_delta_step&#39;: 1.5375076391481943, &#39;subsample&#39;: 0.31936821435437807, &#39;lambda&#39;: 5.662345591728402, &#39;alpha&#39;: 3.8642884050868407}. 
<span class=" -Color -Color-Green">[I 2021-06-24 20:55:53,963]</span> Trial 19 finished with values: [0.7736580986027985, 0.8894929885864258] and parameters: {&#39;learning_rate&#39;: 6.431816304724322e-06, &#39;n_estimators&#39;: 425, &#39;gamma&#39;: 0.00024478965727701486, &#39;max_depth&#39;: 1, &#39;min_child_weight&#39;: 5.684328554585848, &#39;max_delta_step&#39;: 3.8753853187933225, &#39;subsample&#39;: 0.9203137783278178, &#39;lambda&#39;: 8.381754095265858, &#39;alpha&#39;: 1.4735484394877063}. 
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span><span class=" -Color -Color-Green">[I 2021-06-24 20:56:02,791]</span> Trial 20 finished with values: [0.8633044164736345, 0.7527987957000732] and parameters: {&#39;learning_rate&#39;: 0.8943000035466402, &#39;n_estimators&#39;: 39, &#39;gamma&#39;: 0.0009374759777297474, &#39;max_depth&#39;: 10, &#39;min_child_weight&#39;: 2.7972723804638853, &#39;max_delta_step&#39;: 9.715767719674412, &#39;subsample&#39;: 0.7345387773887169, &#39;lambda&#39;: 6.438201070530747, &#39;alpha&#39;: 5.291548608701597}. 
<span class=" -Color -Color-Green">[I 2021-06-24 20:56:11,339]</span> Trial 21 finished with values: [0.8242881215957178, 0.66837477684021] and parameters: {&#39;learning_rate&#39;: 0.01570830514132692, &#39;n_estimators&#39;: 44, &#39;gamma&#39;: 0.0005843419549110167, &#39;max_depth&#39;: 10, &#39;min_child_weight&#39;: 3.451703160815641, &#39;max_delta_step&#39;: 5.970869095086018, &#39;subsample&#39;: 0.7855819304167373, &#39;lambda&#39;: 6.640877751460141, &#39;alpha&#39;: 9.931482531919947}. 
<span class=" -Color -Color-Green">[I 2021-06-24 20:56:31,464]</span> Trial 22 finished with values: [0.8190593634257831, 0.9080891609191895] and parameters: {&#39;learning_rate&#39;: 1.2132474088146525e-06, &#39;n_estimators&#39;: 123, &#39;gamma&#39;: 5.6973247272204155e-05, &#39;max_depth&#39;: 7, &#39;min_child_weight&#39;: 5.001152424429211, &#39;max_delta_step&#39;: 2.4380920807775857, &#39;subsample&#39;: 0.6873597883200264, &#39;lambda&#39;: 5.724053366517921, &#39;alpha&#39;: 7.556686631049391}. 
<span class=" -Color -Color-Green">[I 2021-06-24 20:56:38,140]</span> Trial 23 finished with values: [0.8190593634257831, 0.6599571704864502] and parameters: {&#39;learning_rate&#39;: 7.97444360904522e-05, &#39;n_estimators&#39;: 42, &#39;gamma&#39;: 0.00463988389756205, &#39;max_depth&#39;: 3, &#39;min_child_weight&#39;: 2.0569415146958194, &#39;max_delta_step&#39;: 8.341904333971137, &#39;subsample&#39;: 0.9414736049764031, &#39;lambda&#39;: 4.819674774917272, &#39;alpha&#39;: 4.278570488827559}. 
<span class=" -Color -Color-Green">[I 2021-06-24 20:56:39,255]</span> Trial 24 finished with values: [0.8199460677866314, 0.5046272277832031] and parameters: {&#39;learning_rate&#39;: 0.0019307574955714929, &#39;n_estimators&#39;: 1, &#39;gamma&#39;: 4.942631410160456e-06, &#39;max_depth&#39;: 20, &#39;min_child_weight&#39;: 3.134099946280175, &#39;max_delta_step&#39;: 0.7215327368090307, &#39;subsample&#39;: 0.8129750730286442, &#39;lambda&#39;: 6.33253296445254, &#39;alpha&#39;: 7.04070926317071}. 
<span class=" -Color -Color-Green">[I 2021-06-24 20:57:21,569]</span> Trial 25 finished with values: [0.8497067443220636, 1.444654941558838] and parameters: {&#39;learning_rate&#39;: 0.0004909423405341595, &#39;n_estimators&#39;: 209, &#39;gamma&#39;: 4.8563679895476455e-05, &#39;max_depth&#39;: 13, &#39;min_child_weight&#39;: 4.272166163907865, &#39;max_delta_step&#39;: 4.430029921923916, &#39;subsample&#39;: 0.9327084797783503, &#39;lambda&#39;: 2.678936190904497, &#39;alpha&#39;: 3.623891336142108}. 
<span class=" -Color -Color-Green">[I 2021-06-24 20:57:37,138]</span> Trial 26 finished with values: [0.8190593634257831, 0.8482670783996582] and parameters: {&#39;learning_rate&#39;: 1.1199666931371499e-05, &#39;n_estimators&#39;: 107, &#39;gamma&#39;: 3.3637346884893065e-06, &#39;max_depth&#39;: 5, &#39;min_child_weight&#39;: 7.261268727187913, &#39;max_delta_step&#39;: 3.1825340112399374, &#39;subsample&#39;: 0.629615493773783, &#39;lambda&#39;: 7.6206509350579585, &#39;alpha&#39;: 5.216504727063977}. 
<span class=" -Color -Color-Green">[I 2021-06-24 20:57:46,338]</span> Trial 27 finished with values: [0.857311265788703, 0.793402910232544] and parameters: {&#39;learning_rate&#39;: 0.0644316254401455, &#39;n_estimators&#39;: 45, &#39;gamma&#39;: 0.0002949552355248947, &#39;max_depth&#39;: 17, &#39;min_child_weight&#39;: 2.352103041588548, &#39;max_delta_step&#39;: 1.882347763159959, &#39;subsample&#39;: 0.7221470931042703, &#39;lambda&#39;: 8.65235184265372, &#39;alpha&#39;: 1.7276626749666697}. 
<span class=" -Color -Color-Green">[I 2021-06-24 20:58:12,755]</span> Trial 28 finished with values: [0.8869709470943627, 1.0466270446777344] and parameters: {&#39;learning_rate&#39;: 0.4108095470017982, &#39;n_estimators&#39;: 147, &#39;gamma&#39;: 0.05414824732217044, &#39;max_depth&#39;: 10, &#39;min_child_weight&#39;: 1.0645669753059692, &#39;max_delta_step&#39;: 1.1311077246249992, &#39;subsample&#39;: 0.8457226912084972, &#39;lambda&#39;: 9.993710370110481, &#39;alpha&#39;: 9.209188200810011}. 
<span class=" -Color -Color-Green">[I 2021-06-24 20:58:21,987]</span> Trial 29 finished with values: [0.500866074458996, 0.5441570281982422] and parameters: {&#39;learning_rate&#39;: 2.180103124467189e-06, &#39;n_estimators&#39;: 59, &#39;gamma&#39;: 7.99081955663191, &#39;max_depth&#39;: 15, &#39;min_child_weight&#39;: 3.7720054097935156, &#39;max_delta_step&#39;: 0.021239392868065643, &#39;subsample&#39;: 0.9102785270729872, &#39;lambda&#39;: 1.1128511336253855, &#39;alpha&#39;: 0.8995571016424462}. 
<span class=" -Color -Color-Green">[I 2021-06-24 20:58:23,074]</span> Trial 30 finished with values: [0.8199460677866314, 0.5142290592193604] and parameters: {&#39;learning_rate&#39;: 0.0025504654904952837, &#39;n_estimators&#39;: 1, &#39;gamma&#39;: 4.513420559952353e-06, &#39;max_depth&#39;: 19, &#39;min_child_weight&#39;: 2.92422655260822, &#39;max_delta_step&#39;: 0.8035071237303693, &#39;subsample&#39;: 0.7993979237817694, &#39;lambda&#39;: 6.359724385518489, &#39;alpha&#39;: 6.9503641082501915}. 
<span class=" -Color -Color-Green">[I 2021-06-24 20:58:28,555]</span> Trial 31 finished with values: [0.8097309773814971, 0.6845550537109375] and parameters: {&#39;learning_rate&#39;: 0.028757184452310628, &#39;n_estimators&#39;: 29, &#39;gamma&#39;: 2.1944782544787574e-05, &#39;max_depth&#39;: 19, &#39;min_child_weight&#39;: 4.938365438163685, &#39;max_delta_step&#39;: 0.9101695250225518, &#39;subsample&#39;: 0.8114961543616174, &#39;lambda&#39;: 7.022438508376041, &#39;alpha&#39;: 5.371175703048151}. 
<span class=" -Color -Color-Green">[I 2021-06-24 20:58:31,026]</span> Trial 32 finished with values: [0.8190593634257831, 0.6061441898345947] and parameters: {&#39;learning_rate&#39;: 0.0016038015739477413, &#39;n_estimators&#39;: 9, &#39;gamma&#39;: 1.0135263447792603e-06, &#39;max_depth&#39;: 20, &#39;min_child_weight&#39;: 2.892024674267672, &#39;max_delta_step&#39;: 2.773643252655238, &#39;subsample&#39;: 0.7207211472647927, &#39;lambda&#39;: 5.256681086543765, &#39;alpha&#39;: 6.650864446166134}. 
<span class=" -Color -Color-Green">[I 2021-06-24 20:58:47,796]</span> Trial 33 finished with values: [0.8190593634257831, 0.8620870113372803] and parameters: {&#39;learning_rate&#39;: 0.00031879124643030386, &#39;n_estimators&#39;: 93, &#39;gamma&#39;: 5.991089809999583e-06, &#39;max_depth&#39;: 17, &#39;min_child_weight&#39;: 1.8411171580201566, &#39;max_delta_step&#39;: 1.8766777209429502, &#39;subsample&#39;: 0.6559622516630184, &#39;lambda&#39;: 4.442033136536418, &#39;alpha&#39;: 7.652974118877559}. 
<span class=" -Color -Color-Green">[I 2021-06-24 20:59:32,776]</span> Trial 34 finished with values: [0.8827483715689888, 1.3534259796142578] and parameters: {&#39;learning_rate&#39;: 0.7916257651279365, &#39;n_estimators&#39;: 218, &#39;gamma&#39;: 4.329166016405834e-05, &#39;max_depth&#39;: 11, &#39;min_child_weight&#39;: 3.2825110387115055, &#39;max_delta_step&#39;: 0.0962671918383069, &#39;subsample&#39;: 0.9556980538686872, &#39;lambda&#39;: 6.068578747893616, &#39;alpha&#39;: 4.971002836535026}. 
<span class=" -Color -Color-Green">[I 2021-06-24 20:59:33,955]</span> Trial 35 finished with values: [0.7966857621125422, 0.537714958190918] and parameters: {&#39;learning_rate&#39;: 0.005859284348518846, &#39;n_estimators&#39;: 1, &#39;gamma&#39;: 0.0007332978316777311, &#39;max_depth&#39;: 8, &#39;min_child_weight&#39;: 3.850350178542057, &#39;max_delta_step&#39;: 7.474503872597144, &#39;subsample&#39;: 0.8228098868533305, &#39;lambda&#39;: 7.432639787221948, &#39;alpha&#39;: 5.78252420266546}. 
<span class=" -Color -Color-Green">[I 2021-06-24 20:59:55,898]</span> Trial 36 finished with values: [0.8701491652197825, 0.8639569282531738] and parameters: {&#39;learning_rate&#39;: 0.29714892650024366, &#39;n_estimators&#39;: 133, &#39;gamma&#39;: 0.004252167407159966, &#39;max_depth&#39;: 15, &#39;min_child_weight&#39;: 2.368673704193263, &#39;max_delta_step&#39;: 5.8250803685449535, &#39;subsample&#39;: 0.8842370825434066, &#39;lambda&#39;: 3.802320232127904, &#39;alpha&#39;: 8.694646943576233}. 
<span class=" -Color -Color-Green">[I 2021-06-24 21:00:10,075]</span> Trial 37 finished with values: [0.8216567660231857, 0.8082258701324463] and parameters: {&#39;learning_rate&#39;: 6.797131487983354e-05, &#39;n_estimators&#39;: 81, &#39;gamma&#39;: 1.5926585650552506e-06, &#39;max_depth&#39;: 13, &#39;min_child_weight&#39;: 5.627724516702568, &#39;max_delta_step&#39;: 9.856860405668833, &#39;subsample&#39;: 0.7670057355473066, &#39;lambda&#39;: 5.12872382811452, &#39;alpha&#39;: 6.7747210712520864}. 
<span class=" -Color -Color-Green">[I 2021-06-24 21:00:13,819]</span> Trial 38 finished with values: [0.8055692573300804, 0.6419839859008789] and parameters: {&#39;learning_rate&#39;: 0.02410415306516871, &#39;n_estimators&#39;: 26, &#39;gamma&#39;: 9.57517500900782e-06, &#39;max_depth&#39;: 3, &#39;min_child_weight&#39;: 4.797654735196051, &#39;max_delta_step&#39;: 3.3970562278814262, &#39;subsample&#39;: 0.5717919189012426, &#39;lambda&#39;: 6.787224894794189, &#39;alpha&#39;: 4.524024807533664}. 
<span class=" -Color -Color-Green">[I 2021-06-24 21:00:25,197]</span> Trial 39 finished with values: [0.8376127606607264, 0.7177431583404541] and parameters: {&#39;learning_rate&#39;: 1.1272591941437295e-05, &#39;n_estimators&#39;: 58, &#39;gamma&#39;: 8.132298515672763e-05, &#39;max_depth&#39;: 18, &#39;min_child_weight&#39;: 4.01178671217895, &#39;max_delta_step&#39;: 4.645635561759342, &#39;subsample&#39;: 0.7509392888542791, &#39;lambda&#39;: 3.2376877510253173, &#39;alpha&#39;: 2.316974592922121}. 
<span class=" -Color -Color-Green">[I 2021-06-24 21:00:55,507]</span> Trial 40 finished with values: [0.8938399771147919, 0.9719398021697998] and parameters: {&#39;learning_rate&#39;: 0.8666663645154254, &#39;n_estimators&#39;: 285, &#39;gamma&#39;: 2.7458056279217076e-05, &#39;max_depth&#39;: 15, &#39;min_child_weight&#39;: 1.4213403025683085, &#39;max_delta_step&#39;: 9.21203626032315, &#39;subsample&#39;: 0.6311193631746275, &#39;lambda&#39;: 5.925508887012758, &#39;alpha&#39;: 5.411290667010976}. 
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span><span class=" -Color -Color-Green">[I 2021-06-24 21:01:44,028]</span> Trial 41 finished with values: [0.8960887077034426, 1.3205640316009521] and parameters: {&#39;learning_rate&#39;: 0.35255531082952624, &#39;n_estimators&#39;: 299, &#39;gamma&#39;: 0.0022554428603927803, &#39;max_depth&#39;: 11, &#39;min_child_weight&#39;: 1.4326160029393387, &#39;max_delta_step&#39;: 9.23833054166008, &#39;subsample&#39;: 0.5924575269082281, &#39;lambda&#39;: 6.279056012760253, &#39;alpha&#39;: 3.900550166331861}. 
<span class=" -Color -Color-Green">[I 2021-06-24 21:02:18,902]</span> Trial 42 finished with values: [0.8869709470943627, 0.8983578681945801] and parameters: {&#39;learning_rate&#39;: 0.9164021354316638, &#39;n_estimators&#39;: 420, &#39;gamma&#39;: 2.836435332224698e-05, &#39;max_depth&#39;: 16, &#39;min_child_weight&#39;: 1.0383833612085256, &#39;max_delta_step&#39;: 8.23276279240983, &#39;subsample&#39;: 0.6925163909114869, &#39;lambda&#39;: 5.934725798512234, &#39;alpha&#39;: 5.597046742001425}. 
<span class=" -Color -Color-Green">[I 2021-06-24 21:03:36,769]</span> Trial 43 finished with values: [0.8790418265455769, 2.0225112438201904] and parameters: {&#39;learning_rate&#39;: 0.055299118251327245, &#39;n_estimators&#39;: 453, &#39;gamma&#39;: 0.000260614052481391, &#39;max_depth&#39;: 16, &#39;min_child_weight&#39;: 2.558140412940894, &#39;max_delta_step&#39;: 8.558258517995641, &#39;subsample&#39;: 0.6969057507617269, &#39;lambda&#39;: 8.30503748657117, &#39;alpha&#39;: 3.3362374885062955}. 
<span class=" -Color -Color-Green">[I 2021-06-24 21:04:35,850]</span> Trial 44 finished with values: [0.886011475371661, 1.5756418704986572] and parameters: {&#39;learning_rate&#39;: 0.1674018527636666, &#39;n_estimators&#39;: 383, &#39;gamma&#39;: 2.625435841431077e-06, &#39;max_depth&#39;: 13, &#39;min_child_weight&#39;: 3.075127181163143, &#39;max_delta_step&#39;: 8.312056307610552, &#39;subsample&#39;: 0.8230128566364131, &#39;lambda&#39;: 5.142493761565072, &#39;alpha&#39;: 4.7660844480470494}. 
<span class=" -Color -Color-Green">[I 2021-06-24 21:04:40,288]</span> Trial 45 finished with values: [0.8216567660231857, 0.575991153717041] and parameters: {&#39;learning_rate&#39;: 0.007183698468875329, &#39;n_estimators&#39;: 23, &#39;gamma&#39;: 8.954724979599804e-06, &#39;max_depth&#39;: 20, &#39;min_child_weight&#39;: 2.226546587185025, &#39;max_delta_step&#39;: 6.764165706442737, &#39;subsample&#39;: 0.6964440456128205, &#39;lambda&#39;: 7.311009598668012, &#39;alpha&#39;: 6.481960784477348}. 
<span class=" -Color -Color-Green">[I 2021-06-24 21:05:27,142]</span> Trial 46 finished with values: [0.8256868369018574, 1.2599658966064453] and parameters: {&#39;learning_rate&#39;: 0.00032301087441987693, &#39;n_estimators&#39;: 229, &#39;gamma&#39;: 0.00018448407886833315, &#39;max_depth&#39;: 14, &#39;min_child_weight&#39;: 1.7966357215668838, &#39;max_delta_step&#39;: 7.9123078274182905, &#39;subsample&#39;: 0.8988846493365852, &#39;lambda&#39;: 6.697729860030265, &#39;alpha&#39;: 5.731621419458763}. 
<span class=" -Color -Color-Green">[I 2021-06-24 21:05:42,795]</span> Trial 47 finished with values: [0.817647621215583, 0.793281078338623] and parameters: {&#39;learning_rate&#39;: 0.0014834529444652292, &#39;n_estimators&#39;: 106, &#39;gamma&#39;: 0.0006748016100794333, &#39;max_depth&#39;: 16, &#39;min_child_weight&#39;: 3.666738435093685, &#39;max_delta_step&#39;: 9.862126303229266, &#39;subsample&#39;: 0.6432963310834884, &#39;lambda&#39;: 4.43066917068923, &#39;alpha&#39;: 7.086477237087129}. 
<span class=" -Color -Color-Green">[I 2021-06-24 21:07:06,792]</span> Trial 48 finished with values: [0.896317585463515, 1.991610050201416] and parameters: {&#39;learning_rate&#39;: 0.07055705272110428, &#39;n_estimators&#39;: 386, &#39;gamma&#39;: 2.7075445643830907e-05, &#39;max_depth&#39;: 17, &#39;min_child_weight&#39;: 2.694797059003313, &#39;max_delta_step&#39;: 8.891916820631465, &#39;subsample&#39;: 0.9595277923034844, &#39;lambda&#39;: 8.992140034504448, &#39;alpha&#39;: 4.059528790968905}. 
<span class=" -Color -Color-Green">[I 2021-06-24 21:07:32,407]</span> Trial 49 finished with values: [0.7977261200751783, 1.0730280876159668] and parameters: {&#39;learning_rate&#39;: 2.4327789499872114e-06, &#39;n_estimators&#39;: 277, &#39;gamma&#39;: 0.00012756997354334903, &#39;max_depth&#39;: 11, &#39;min_child_weight&#39;: 9.49056309499084, &#39;max_delta_step&#39;: 6.675906796625968, &#39;subsample&#39;: 0.4400139035111046, &#39;lambda&#39;: 5.949457200880867, &#39;alpha&#39;: 5.571534740484487}. 
</pre></div>
</div>
</div>
</div>
<p>Our second experiment will use random sampling for optimization. The only thing that needs to change is the sampling strategy.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">random_sampler</span> <span class="o">=</span> <span class="n">optuna</span><span class="o">.</span><span class="n">samplers</span><span class="o">.</span><span class="n">RandomSampler</span><span class="p">()</span>
<span class="n">random_experiment</span> <span class="o">=</span> <span class="n">optuna</span><span class="o">.</span><span class="n">create_study</span><span class="p">(</span><span class="n">sampler</span><span class="o">=</span><span class="n">random_sampler</span><span class="p">,</span> <span class="n">directions</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;maximize&quot;</span><span class="p">,</span> <span class="s2">&quot;minimize&quot;</span><span class="p">])</span>
<span class="n">random_experiment</span><span class="o">.</span><span class="n">optimize</span><span class="p">(</span><span class="n">train</span><span class="p">,</span> <span class="n">n_trials</span><span class="o">=</span><span class="n">N_TRIAL</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span><span class=" -Color -Color-Green">[I 2021-06-24 21:07:32,414]</span> A new study created in memory with name: no-name-c5e5c759-7e1d-4441-a31f-7819463f106d
<span class=" -Color -Color-Green">[I 2021-06-24 21:08:08,251]</span> Trial 0 finished with values: [0.8242881215957178, 1.2239470481872559] and parameters: {&#39;learning_rate&#39;: 1.9507239259377863e-06, &#39;n_estimators&#39;: 271, &#39;gamma&#39;: 3.203084164333884e-05, &#39;max_depth&#39;: 12, &#39;min_child_weight&#39;: 3.9081575799327846, &#39;max_delta_step&#39;: 4.117412038341683, &#39;subsample&#39;: 0.5762772304746615, &#39;lambda&#39;: 5.405463628427972, &#39;alpha&#39;: 3.864109226147682}. 
<span class=" -Color -Color-Green">[I 2021-06-24 21:08:18,362]</span> Trial 1 finished with values: [0.8296505703197883, 0.6600246429443359] and parameters: {&#39;learning_rate&#39;: 0.01655164453347855, &#39;n_estimators&#39;: 58, &#39;gamma&#39;: 3.9044873735601247, &#39;max_depth&#39;: 13, &#39;min_child_weight&#39;: 7.984927170467948, &#39;max_delta_step&#39;: 8.09500795905697, &#39;subsample&#39;: 0.849813279677957, &#39;lambda&#39;: 7.3365891345582455, &#39;alpha&#39;: 2.2537196445796215}. 
<span class=" -Color -Color-Green">[I 2021-06-24 21:08:24,360]</span> Trial 2 finished with values: [0.7962364659054924, 0.6288399696350098] and parameters: {&#39;learning_rate&#39;: 7.144946052169984e-06, &#39;n_estimators&#39;: 55, &#39;gamma&#39;: 0.0005938900211062816, &#39;max_depth&#39;: 16, &#39;min_child_weight&#39;: 8.29773094793812, &#39;max_delta_step&#39;: 2.788855163589783, &#39;subsample&#39;: 0.4420523447596358, &#39;lambda&#39;: 5.066121241361202, &#39;alpha&#39;: 0.3988522258882743}. 
<span class=" -Color -Color-Green">[I 2021-06-24 21:09:28,410]</span> Trial 3 finished with values: [0.8205210619242312, 1.6159019470214844] and parameters: {&#39;learning_rate&#39;: 0.0005145376531234654, &#39;n_estimators&#39;: 407, &#39;gamma&#39;: 8.19479783583658, &#39;max_depth&#39;: 13, &#39;min_child_weight&#39;: 4.493654845616417, &#39;max_delta_step&#39;: 7.226690315481411, &#39;subsample&#39;: 0.7932310802055202, &#39;lambda&#39;: 3.130885424940356, &#39;alpha&#39;: 9.040552220313188}. 
<span class=" -Color -Color-Green">[I 2021-06-24 21:09:48,963]</span> Trial 4 finished with values: [0.8496149189813387, 1.0410358905792236] and parameters: {&#39;learning_rate&#39;: 0.013849555482491996, &#39;n_estimators&#39;: 247, &#39;gamma&#39;: 0.004894973036916711, &#39;max_depth&#39;: 9, &#39;min_child_weight&#39;: 6.9009694714379695, &#39;max_delta_step&#39;: 7.3663949572847685, &#39;subsample&#39;: 0.31946222683020187, &#39;lambda&#39;: 0.5603893246244229, &#39;alpha&#39;: 4.450235208975947}. 
<span class=" -Color -Color-Green">[I 2021-06-24 21:10:14,033]</span> Trial 5 finished with values: [0.8230729155907346, 0.8897931575775146] and parameters: {&#39;learning_rate&#39;: 0.00013362494466989306, &#39;n_estimators&#39;: 141, &#39;gamma&#39;: 0.000329157827823119, &#39;max_depth&#39;: 20, &#39;min_child_weight&#39;: 7.797581565153617, &#39;max_delta_step&#39;: 4.03179167233131, &#39;subsample&#39;: 0.9227920340824296, &#39;lambda&#39;: 1.6106526818819378, &#39;alpha&#39;: 6.601091473609549}. 
<span class=" -Color -Color-Green">[I 2021-06-24 21:10:48,359]</span> Trial 6 finished with values: [0.8003060994353435, 1.498732089996338] and parameters: {&#39;learning_rate&#39;: 1.168241229250612e-06, &#39;n_estimators&#39;: 439, &#39;gamma&#39;: 7.818060971555486e-05, &#39;max_depth&#39;: 17, &#39;min_child_weight&#39;: 6.41934097447878, &#39;max_delta_step&#39;: 7.810546223442011, &#39;subsample&#39;: 0.31300197310918787, &#39;lambda&#39;: 3.109239024530824, &#39;alpha&#39;: 1.3833741423925527}. 
<span class=" -Color -Color-Green">[I 2021-06-24 21:11:39,786]</span> Trial 7 finished with values: [0.8701491652197825, 1.512773036956787] and parameters: {&#39;learning_rate&#39;: 0.02790630764374101, &#39;n_estimators&#39;: 243, &#39;gamma&#39;: 0.32616502309400197, &#39;max_depth&#39;: 5, &#39;min_child_weight&#39;: 3.446731702524492, &#39;max_delta_step&#39;: 8.85814993654472, &#39;subsample&#39;: 0.8905978197038209, &#39;lambda&#39;: 0.2896918954362726, &#39;alpha&#39;: 6.0978187521867335}. 
<span class=" -Color -Color-Green">[I 2021-06-24 21:12:20,530]</span> Trial 8 finished with values: [0.8746975062494932, 1.8283360004425049] and parameters: {&#39;learning_rate&#39;: 0.06365863535683362, &#39;n_estimators&#39;: 377, &#39;gamma&#39;: 0.01206542578602256, &#39;max_depth&#39;: 4, &#39;min_child_weight&#39;: 7.580066616388424, &#39;max_delta_step&#39;: 2.9636879058903407, &#39;subsample&#39;: 0.48901772292932044, &#39;lambda&#39;: 2.163451159114259, &#39;alpha&#39;: 5.531900357691034}. 
<span class=" -Color -Color-Green">[I 2021-06-24 21:12:46,343]</span> Trial 9 finished with values: [0.7689478730324634, 1.2750110626220703] and parameters: {&#39;learning_rate&#39;: 1.0363857232566384e-06, &#39;n_estimators&#39;: 341, &#39;gamma&#39;: 0.0005049586032749233, &#39;max_depth&#39;: 9, &#39;min_child_weight&#39;: 9.265481309572648, &#39;max_delta_step&#39;: 5.236168392595397, &#39;subsample&#39;: 0.30241181083214913, &#39;lambda&#39;: 9.288411149593697, &#39;alpha&#39;: 1.4806249551779593}. 
<span class=" -Color -Color-Green">[I 2021-06-24 21:13:15,668]</span> Trial 10 finished with values: [0.786973939042972, 1.3024439811706543] and parameters: {&#39;learning_rate&#39;: 0.002335840721200959, &#39;n_estimators&#39;: 471, &#39;gamma&#39;: 2.732162626655945, &#39;max_depth&#39;: 11, &#39;min_child_weight&#39;: 8.446409864456264, &#39;max_delta_step&#39;: 8.414460270670364, &#39;subsample&#39;: 0.24599288277867146, &#39;lambda&#39;: 7.630680346954753, &#39;alpha&#39;: 7.164191512507909}. 
<span class=" -Color -Color-Green">[I 2021-06-24 21:13:26,994]</span> Trial 11 finished with values: [0.8694663883046205, 0.9429812431335449] and parameters: {&#39;learning_rate&#39;: 0.07049569809401308, &#39;n_estimators&#39;: 111, &#39;gamma&#39;: 0.010400062352022907, &#39;max_depth&#39;: 17, &#39;min_child_weight&#39;: 1.682818031984671, &#39;max_delta_step&#39;: 7.9353700622048295, &#39;subsample&#39;: 0.24055469032404275, &#39;lambda&#39;: 4.362461895127384, &#39;alpha&#39;: 1.429810512766334}. 
<span class=" -Color -Color-Green">[I 2021-06-24 21:13:51,049]</span> Trial 12 finished with values: [0.8556790817356423, 0.9278888702392578] and parameters: {&#39;learning_rate&#39;: 0.8521133971884453, &#39;n_estimators&#39;: 399, &#39;gamma&#39;: 0.05765190927287146, &#39;max_depth&#39;: 2, &#39;min_child_weight&#39;: 1.2005285447449363, &#39;max_delta_step&#39;: 8.347295791594432, &#39;subsample&#39;: 0.7032450156575869, &#39;lambda&#39;: 2.1712736719047876, &#39;alpha&#39;: 9.85013897044308}. 
<span class=" -Color -Color-Green">[I 2021-06-24 21:14:11,533]</span> Trial 13 finished with values: [0.8164952608616804, 0.991943359375] and parameters: {&#39;learning_rate&#39;: 0.00013974596048266447, &#39;n_estimators&#39;: 146, &#39;gamma&#39;: 9.639906431167176e-05, &#39;max_depth&#39;: 7, &#39;min_child_weight&#39;: 1.953471213218589, &#39;max_delta_step&#39;: 2.248534923591489, &#39;subsample&#39;: 0.5004616260874354, &#39;lambda&#39;: 3.475261912395701, &#39;alpha&#39;: 7.982277238304221}. 
<span class=" -Color -Color-Green">[I 2021-06-24 21:14:34,379]</span> Trial 14 finished with values: [0.8656943012972265, 0.9568576812744141] and parameters: {&#39;learning_rate&#39;: 0.6888988306460377, &#39;n_estimators&#39;: 240, &#39;gamma&#39;: 1.0297392319062661e-05, &#39;max_depth&#39;: 4, &#39;min_child_weight&#39;: 5.324504122542551, &#39;max_delta_step&#39;: 2.050103538295925, &#39;subsample&#39;: 0.84766005209242, &#39;lambda&#39;: 7.0135771183941396, &#39;alpha&#39;: 8.876460643992045}. 
<span class=" -Color -Color-Green">[I 2021-06-24 21:15:12,327]</span> Trial 15 finished with values: [0.7893394252069803, 1.2191379070281982] and parameters: {&#39;learning_rate&#39;: 1.9910319756644723e-05, &#39;n_estimators&#39;: 359, &#39;gamma&#39;: 0.1342591040715406, &#39;max_depth&#39;: 17, &#39;min_child_weight&#39;: 6.900616762527168, &#39;max_delta_step&#39;: 0.6841686028973304, &#39;subsample&#39;: 0.5191495086852524, &#39;lambda&#39;: 1.2145082775703975, &#39;alpha&#39;: 8.108235121968528}. 
<span class=" -Color -Color-Green">[I 2021-06-24 21:15:44,691]</span> Trial 16 finished with values: [0.8388877369513583, 1.5050408840179443] and parameters: {&#39;learning_rate&#39;: 0.00899052560815236, &#39;n_estimators&#39;: 291, &#39;gamma&#39;: 0.38349350195905285, &#39;max_depth&#39;: 7, &#39;min_child_weight&#39;: 8.996764763115241, &#39;max_delta_step&#39;: 8.343203615416073, &#39;subsample&#39;: 0.4937619595582944, &#39;lambda&#39;: 9.718912302234466, &#39;alpha&#39;: 1.556797069857253}. 
<span class=" -Color -Color-Green">[I 2021-06-24 21:16:15,047]</span> Trial 17 finished with values: [0.8620796708637679, 1.2882869243621826] and parameters: {&#39;learning_rate&#39;: 0.12042438603720615, &#39;n_estimators&#39;: 278, &#39;gamma&#39;: 0.13085929093618276, &#39;max_depth&#39;: 7, &#39;min_child_weight&#39;: 6.9412935626173455, &#39;max_delta_step&#39;: 1.017209248991896, &#39;subsample&#39;: 0.5695175622187703, &#39;lambda&#39;: 3.507755980106151, &#39;alpha&#39;: 7.6981528068144875}. 
<span class=" -Color -Color-Green">[I 2021-06-24 21:16:31,255]</span> Trial 18 finished with values: [0.8190593634257831, 0.8593380451202393] and parameters: {&#39;learning_rate&#39;: 3.360137322801189e-05, &#39;n_estimators&#39;: 145, &#39;gamma&#39;: 2.87781881000497e-06, &#39;max_depth&#39;: 20, &#39;min_child_weight&#39;: 4.313886958053366, &#39;max_delta_step&#39;: 7.783780042252879, &#39;subsample&#39;: 0.47965760708350813, &#39;lambda&#39;: 9.125929370528876, &#39;alpha&#39;: 2.075047118219052}. 
<span class=" -Color -Color-Green">[I 2021-06-24 21:17:17,009]</span> Trial 19 finished with values: [0.8149820523103819, 1.215108871459961] and parameters: {&#39;learning_rate&#39;: 0.002466037895045117, &#39;n_estimators&#39;: 291, &#39;gamma&#39;: 0.0001762100831698647, &#39;max_depth&#39;: 15, &#39;min_child_weight&#39;: 6.559990265810391, &#39;max_delta_step&#39;: 1.1303452422384253, &#39;subsample&#39;: 0.9616536564638809, &#39;lambda&#39;: 3.187000624434526, &#39;alpha&#39;: 8.279609591369342}. 
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span><span class=" -Color -Color-Green">[I 2021-06-24 21:17:56,898]</span> Trial 20 finished with values: [0.8110071208354481, 1.5800440311431885] and parameters: {&#39;learning_rate&#39;: 2.0884008896526032e-06, &#39;n_estimators&#39;: 455, &#39;gamma&#39;: 2.891243616832569e-06, &#39;max_depth&#39;: 15, &#39;min_child_weight&#39;: 5.035721547860057, &#39;max_delta_step&#39;: 8.70477440647167, &#39;subsample&#39;: 0.3385504407877148, &#39;lambda&#39;: 2.7435598630534397, &#39;alpha&#39;: 0.6978146783305916}. 
<span class=" -Color -Color-Green">[I 2021-06-24 21:18:00,515]</span> Trial 21 finished with values: [0.8641084498468754, 0.5938787460327148] and parameters: {&#39;learning_rate&#39;: 0.5793789316528507, &#39;n_estimators&#39;: 32, &#39;gamma&#39;: 0.052375508713541845, &#39;max_depth&#39;: 9, &#39;min_child_weight&#39;: 7.430893761258581, &#39;max_delta_step&#39;: 6.395150246442426, &#39;subsample&#39;: 0.4435405009843316, &#39;lambda&#39;: 3.0110480701804763, &#39;alpha&#39;: 6.4509483120277125}. 
<span class=" -Color -Color-Green">[I 2021-06-24 21:18:18,926]</span> Trial 22 finished with values: [0.8084182064818278, 0.9187440872192383] and parameters: {&#39;learning_rate&#39;: 0.0002872486380637588, &#39;n_estimators&#39;: 160, &#39;gamma&#39;: 8.128741037256317e-05, &#39;max_depth&#39;: 4, &#39;min_child_weight&#39;: 2.575461825257871, &#39;max_delta_step&#39;: 9.246070649799462, &#39;subsample&#39;: 0.4899607230522984, &#39;lambda&#39;: 5.578388144432656, &#39;alpha&#39;: 8.099903083090448}. 
<span class=" -Color -Color-Green">[I 2021-06-24 21:18:43,272]</span> Trial 23 finished with values: [0.8110071208354481, 1.1431057453155518] and parameters: {&#39;learning_rate&#39;: 0.001381728822917572, &#39;n_estimators&#39;: 310, &#39;gamma&#39;: 3.7166911468646853, &#39;max_depth&#39;: 2, &#39;min_child_weight&#39;: 1.2941885682925542, &#39;max_delta_step&#39;: 6.5277324400696655, &#39;subsample&#39;: 0.6565476312576366, &#39;lambda&#39;: 9.69454691928472, &#39;alpha&#39;: 3.5948674166461645}. 
<span class=" -Color -Color-Green">[I 2021-06-24 21:19:49,118]</span> Trial 24 finished with values: [0.857311265788703, 2.0020668506622314] and parameters: {&#39;learning_rate&#39;: 0.031346928125215376, &#39;n_estimators&#39;: 419, &#39;gamma&#39;: 0.0006212152718852163, &#39;max_depth&#39;: 5, &#39;min_child_weight&#39;: 1.1202810266401795, &#39;max_delta_step&#39;: 8.801409251027748, &#39;subsample&#39;: 0.5896817648894658, &#39;lambda&#39;: 9.320145510414722, &#39;alpha&#39;: 6.225365754810461}. 
<span class=" -Color -Color-Green">[I 2021-06-24 21:20:26,064]</span> Trial 25 finished with values: [0.8340701441935598, 1.2236511707305908] and parameters: {&#39;learning_rate&#39;: 0.00011182559506367305, &#39;n_estimators&#39;: 248, &#39;gamma&#39;: 0.0010053636955866386, &#39;max_depth&#39;: 16, &#39;min_child_weight&#39;: 4.174266872618048, &#39;max_delta_step&#39;: 2.481259784028845, &#39;subsample&#39;: 0.659566715060743, &#39;lambda&#39;: 0.09586916840113813, &#39;alpha&#39;: 6.807305874708272}. 
<span class=" -Color -Color-Green">[I 2021-06-24 21:20:43,091]</span> Trial 26 finished with values: [0.8611497321132064, 0.8022499084472656] and parameters: {&#39;learning_rate&#39;: 0.09896390141242128, &#39;n_estimators&#39;: 97, &#39;gamma&#39;: 0.2833402629264894, &#39;max_depth&#39;: 11, &#39;min_child_weight&#39;: 5.292013380232245, &#39;max_delta_step&#39;: 6.731804241037651, &#39;subsample&#39;: 0.8644632339936671, &#39;lambda&#39;: 6.524342813771654, &#39;alpha&#39;: 9.471981763135433}. 
<span class=" -Color -Color-Green">[I 2021-06-24 21:21:02,872]</span> Trial 27 finished with values: [0.8901473481580877, 1.3373830318450928] and parameters: {&#39;learning_rate&#39;: 0.14021312820737467, &#39;n_estimators&#39;: 273, &#39;gamma&#39;: 8.146475211466562e-05, &#39;max_depth&#39;: 9, &#39;min_child_weight&#39;: 3.14388930063139, &#39;max_delta_step&#39;: 6.719270941417452, &#39;subsample&#39;: 0.2646046896455692, &#39;lambda&#39;: 6.61184963264053, &#39;alpha&#39;: 0.545784193512614}. 
<span class=" -Color -Color-Green">[I 2021-06-24 21:22:00,733]</span> Trial 28 finished with values: [0.8229716051137779, 1.5625231266021729] and parameters: {&#39;learning_rate&#39;: 0.004625944161007529, &#39;n_estimators&#39;: 425, &#39;gamma&#39;: 0.620112195315808, &#39;max_depth&#39;: 3, &#39;min_child_weight&#39;: 6.905011629893329, &#39;max_delta_step&#39;: 1.851315377470214, &#39;subsample&#39;: 0.9930086009772898, &#39;lambda&#39;: 4.971325051899399, &#39;alpha&#39;: 1.343103796366869}. 
<span class=" -Color -Color-Green">[I 2021-06-24 21:22:15,808]</span> Trial 29 finished with values: [0.8110071208354481, 0.8679680824279785] and parameters: {&#39;learning_rate&#39;: 0.0006005367776356777, &#39;n_estimators&#39;: 188, &#39;gamma&#39;: 0.0003989071140277466, &#39;max_depth&#39;: 2, &#39;min_child_weight&#39;: 7.70048315858797, &#39;max_delta_step&#39;: 3.352499659112773, &#39;subsample&#39;: 0.6362712821712457, &#39;lambda&#39;: 5.602814529170654, &#39;alpha&#39;: 3.5452047426838886}. 
<span class=" -Color -Color-Green">[I 2021-06-24 21:23:00,075]</span> Trial 30 finished with values: [0.7936440992272074, 1.243791103363037] and parameters: {&#39;learning_rate&#39;: 1.950840343464215e-05, &#39;n_estimators&#39;: 380, &#39;gamma&#39;: 0.0036582892083478367, &#39;max_depth&#39;: 5, &#39;min_child_weight&#39;: 7.291069053814144, &#39;max_delta_step&#39;: 0.09464097130660432, &#39;subsample&#39;: 0.7464831886530765, &#39;lambda&#39;: 7.138304276321468, &#39;alpha&#39;: 5.798901912189601}. 
<span class=" -Color -Color-Green">[I 2021-06-24 21:23:40,834]</span> Trial 31 finished with values: [0.8543758450450631, 1.0849800109863281] and parameters: {&#39;learning_rate&#39;: 0.1494159640075728, &#39;n_estimators&#39;: 442, &#39;gamma&#39;: 0.09977049939165789, &#39;max_depth&#39;: 20, &#39;min_child_weight&#39;: 8.983830088285888, &#39;max_delta_step&#39;: 4.813737648631068, &#39;subsample&#39;: 0.9397056707532663, &#39;lambda&#39;: 5.881035571085805, &#39;alpha&#39;: 9.617606737324499}. 
<span class=" -Color -Color-Green">[I 2021-06-24 21:23:48,020]</span> Trial 32 finished with values: [0.8003060994353435, 0.7104589939117432] and parameters: {&#39;learning_rate&#39;: 2.1991147225922148e-05, &#39;n_estimators&#39;: 104, &#39;gamma&#39;: 5.859308313423115e-05, &#39;max_depth&#39;: 2, &#39;min_child_weight&#39;: 8.230050161135958, &#39;max_delta_step&#39;: 7.398397307612075, &#39;subsample&#39;: 0.4257940713857218, &#39;lambda&#39;: 2.459720891739737, &#39;alpha&#39;: 3.755968680189591}. 
<span class=" -Color -Color-Green">[I 2021-06-24 21:24:10,871]</span> Trial 33 finished with values: [0.8753304406743465, 0.8417830467224121] and parameters: {&#39;learning_rate&#39;: 0.9394078194827746, &#39;n_estimators&#39;: 448, &#39;gamma&#39;: 0.000538048130123458, &#39;max_depth&#39;: 14, &#39;min_child_weight&#39;: 1.911829679403519, &#39;max_delta_step&#39;: 1.958940115833785, &#39;subsample&#39;: 0.3774079966016376, &#39;lambda&#39;: 1.1305928187999204, &#39;alpha&#39;: 9.862850414500029}. 
<span class=" -Color -Color-Green">[I 2021-06-24 21:24:50,842]</span> Trial 34 finished with values: [0.7962364659054924, 1.2472569942474365] and parameters: {&#39;learning_rate&#39;: 6.0782982971329043e-05, &#39;n_estimators&#39;: 339, &#39;gamma&#39;: 2.479857463426879e-05, &#39;max_depth&#39;: 19, &#39;min_child_weight&#39;: 2.7749295210233997, &#39;max_delta_step&#39;: 0.1081053253927311, &#39;subsample&#39;: 0.6529027193633793, &#39;lambda&#39;: 3.2047081687757997, &#39;alpha&#39;: 7.336297840995827}. 
<span class=" -Color -Color-Green">[I 2021-06-24 21:25:23,460]</span> Trial 35 finished with values: [0.8003060994353435, 1.1897087097167969] and parameters: {&#39;learning_rate&#39;: 8.802960709377651e-06, &#39;n_estimators&#39;: 335, &#39;gamma&#39;: 7.548534903723408e-05, &#39;max_depth&#39;: 15, &#39;min_child_weight&#39;: 7.913442979457331, &#39;max_delta_step&#39;: 7.273672219782713, &#39;subsample&#39;: 0.48860459391406075, &#39;lambda&#39;: 6.46402630454282, &#39;alpha&#39;: 9.763337730223803}. 
<span class=" -Color -Color-Green">[I 2021-06-24 21:26:07,811]</span> Trial 36 finished with values: [0.827078301823953, 1.4122300148010254] and parameters: {&#39;learning_rate&#39;: 0.0002168616709207641, &#39;n_estimators&#39;: 331, &#39;gamma&#39;: 0.004578134043902739, &#39;max_depth&#39;: 7, &#39;min_child_weight&#39;: 3.4174717554497627, &#39;max_delta_step&#39;: 4.893134554827137, &#39;subsample&#39;: 0.5295827673928973, &#39;lambda&#39;: 1.7403199384813228, &#39;alpha&#39;: 2.1403266705181645}. 
<span class=" -Color -Color-Green">[I 2021-06-24 21:26:29,396]</span> Trial 37 finished with values: [0.8508501231162573, 0.8315458297729492] and parameters: {&#39;learning_rate&#39;: 0.00010526742202047852, &#39;n_estimators&#39;: 114, &#39;gamma&#39;: 0.005805878866544669, &#39;max_depth&#39;: 10, &#39;min_child_weight&#39;: 2.468175236382814, &#39;max_delta_step&#39;: 6.204230102338428, &#39;subsample&#39;: 0.908156115347069, &#39;lambda&#39;: 1.0607574667062847, &#39;alpha&#39;: 4.465313958685507}. 
<span class=" -Color -Color-Green">[I 2021-06-24 21:26:30,534]</span> Trial 38 finished with values: [0.7746126649789082, 0.49856996536254883] and parameters: {&#39;learning_rate&#39;: 9.788402705802285e-06, &#39;n_estimators&#39;: 3, &#39;gamma&#39;: 1.8636442808909454e-06, &#39;max_depth&#39;: 20, &#39;min_child_weight&#39;: 9.537986914349867, &#39;max_delta_step&#39;: 5.27845522605244, &#39;subsample&#39;: 0.39015531862016606, &#39;lambda&#39;: 1.6367258439730936, &#39;alpha&#39;: 2.989343046650145}. 
<span class=" -Color -Color-Green">[I 2021-06-24 21:26:47,650]</span> Trial 39 finished with values: [0.892981124197745, 0.9046058654785156] and parameters: {&#39;learning_rate&#39;: 0.17959875151705093, &#39;n_estimators&#39;: 125, &#39;gamma&#39;: 0.0001109584364222337, &#39;max_depth&#39;: 12, &#39;min_child_weight&#39;: 5.39606378787268, &#39;max_delta_step&#39;: 1.9758010476326593, &#39;subsample&#39;: 0.577284400072523, &#39;lambda&#39;: 0.6219184183554771, &#39;alpha&#39;: 0.672004307953783}. 
<span class=" -Color -Color-Green">[I 2021-06-24 21:27:16,047]</span> Trial 40 finished with values: [0.7974728529308776, 1.4177539348602295] and parameters: {&#39;learning_rate&#39;: 0.0011845465569698794, &#39;n_estimators&#39;: 442, &#39;gamma&#39;: 0.7620830145090745, &#39;max_depth&#39;: 5, &#39;min_child_weight&#39;: 2.271722360863892, &#39;max_delta_step&#39;: 7.0210318813574375, &#39;subsample&#39;: 0.2116384393121508, &#39;lambda&#39;: 8.371442319574717, &#39;alpha&#39;: 5.882796103914561}. 
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span><span class=" -Color -Color-Green">[I 2021-06-24 21:27:28,994]</span> Trial 41 finished with values: [0.7948569290121553, 0.8213651180267334] and parameters: {&#39;learning_rate&#39;: 8.4280873592244e-05, &#39;n_estimators&#39;: 268, &#39;gamma&#39;: 2.277792842070888e-06, &#39;max_depth&#39;: 20, &#39;min_child_weight&#39;: 7.4531713488192235, &#39;max_delta_step&#39;: 0.8172735909486628, &#39;subsample&#39;: 0.21085097355333302, &#39;lambda&#39;: 2.025124138341253, &#39;alpha&#39;: 2.1176634964253815}. 
<span class=" -Color -Color-Green">[I 2021-06-24 21:27:53,899]</span> Trial 42 finished with values: [0.8694663883046205, 1.0063731670379639] and parameters: {&#39;learning_rate&#39;: 0.34136880083925847, &#39;n_estimators&#39;: 177, &#39;gamma&#39;: 0.015021676606379956, &#39;max_depth&#39;: 8, &#39;min_child_weight&#39;: 1.5736368640199103, &#39;max_delta_step&#39;: 6.250758290164357, &#39;subsample&#39;: 0.5598802274964483, &#39;lambda&#39;: 6.967950869039663, &#39;alpha&#39;: 7.643500856478671}. 
<span class=" -Color -Color-Green">[I 2021-06-24 21:28:01,544]</span> Trial 43 finished with values: [0.8633044164736345, 0.6240670680999756] and parameters: {&#39;learning_rate&#39;: 0.1258658923810024, &#39;n_estimators&#39;: 41, &#39;gamma&#39;: 0.10648563780481408, &#39;max_depth&#39;: 19, &#39;min_child_weight&#39;: 5.343821672160628, &#39;max_delta_step&#39;: 6.653114723346393, &#39;subsample&#39;: 0.9141831943543757, &#39;lambda&#39;: 7.407423551110383, &#39;alpha&#39;: 6.118577237820352}. 
<span class=" -Color -Color-Green">[I 2021-06-24 21:28:28,956]</span> Trial 44 finished with values: [0.8216567660231857, 0.9390499591827393] and parameters: {&#39;learning_rate&#39;: 3.604195658289106e-06, &#39;n_estimators&#39;: 162, &#39;gamma&#39;: 4.2243333751987855, &#39;max_depth&#39;: 19, &#39;min_child_weight&#39;: 5.488203389834896, &#39;max_delta_step&#39;: 7.269422248948586, &#39;subsample&#39;: 0.8505629688396659, &#39;lambda&#39;: 8.688476930570168, &#39;alpha&#39;: 3.704597825216797}. 
<span class=" -Color -Color-Green">[I 2021-06-24 21:28:42,839]</span> Trial 45 finished with values: [0.8110071208354481, 0.800206184387207] and parameters: {&#39;learning_rate&#39;: 0.002129730178703707, &#39;n_estimators&#39;: 135, &#39;gamma&#39;: 0.023677990145689862, &#39;max_depth&#39;: 12, &#39;min_child_weight&#39;: 5.732403366160122, &#39;max_delta_step&#39;: 9.466053339973318, &#39;subsample&#39;: 0.447815873780944, &#39;lambda&#39;: 9.468174408899921, &#39;alpha&#39;: 0.5095257253610441}. 
<span class=" -Color -Color-Green">[I 2021-06-24 21:29:11,197]</span> Trial 46 finished with values: [0.7789888066300412, 0.692234992980957] and parameters: {&#39;learning_rate&#39;: 0.1259440314628567, &#39;n_estimators&#39;: 260, &#39;gamma&#39;: 2.0109008620769058, &#39;max_depth&#39;: 10, &#39;min_child_weight&#39;: 1.8364105493733527, &#39;max_delta_step&#39;: 0.05313929774694248, &#39;subsample&#39;: 0.4692861310587977, &#39;lambda&#39;: 3.1389419565946755, &#39;alpha&#39;: 3.168205632605723}. 
<span class=" -Color -Color-Green">[I 2021-06-24 21:29:14,291]</span> Trial 47 finished with values: [0.8150374862525068, 0.5811522006988525] and parameters: {&#39;learning_rate&#39;: 8.301101637578672e-05, &#39;n_estimators&#39;: 16, &#39;gamma&#39;: 0.0034515795349725265, &#39;max_depth&#39;: 16, &#39;min_child_weight&#39;: 2.9582455884170322, &#39;max_delta_step&#39;: 5.887308426635012, &#39;subsample&#39;: 0.5652222841444268, &#39;lambda&#39;: 6.388128240316052, &#39;alpha&#39;: 3.435226928525281}. 
<span class=" -Color -Color-Green">[I 2021-06-24 21:30:03,799]</span> Trial 48 finished with values: [0.7977261200751783, 1.4499850273132324] and parameters: {&#39;learning_rate&#39;: 2.754807472403577e-06, &#39;n_estimators&#39;: 445, &#39;gamma&#39;: 0.00014554001611299385, &#39;max_depth&#39;: 7, &#39;min_child_weight&#39;: 9.013766158049954, &#39;max_delta_step&#39;: 6.44510764082963, &#39;subsample&#39;: 0.5879644676886469, &#39;lambda&#39;: 4.4464742938363155, &#39;alpha&#39;: 7.019515902540286}. 
<span class=" -Color -Color-Green">[I 2021-06-24 21:30:11,376]</span> Trial 49 finished with values: [0.7736580986027985, 0.6159448623657227] and parameters: {&#39;learning_rate&#39;: 0.00017294564714432752, &#39;n_estimators&#39;: 145, &#39;gamma&#39;: 0.008710444065983784, &#39;max_depth&#39;: 1, &#39;min_child_weight&#39;: 5.96632723369129, &#39;max_delta_step&#39;: 2.9422256549284667, &#39;subsample&#39;: 0.849683050335331, &#39;lambda&#39;: 3.0456432415007315, &#39;alpha&#39;: 7.492133503450251}. 
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="id8">
<h3>Visualize Results<a class="headerlink" href="#id8" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">cast_as_df</span><span class="p">(</span><span class="n">optuna_experiment</span><span class="p">,</span> <span class="n">experiment_type</span><span class="p">):</span>
    <span class="n">df</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">optuna_experiment</span>
        <span class="o">.</span><span class="n">trials_dataframe</span><span class="p">()</span>
        <span class="o">.</span><span class="n">rename</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;values_0&quot;</span><span class="p">:</span> <span class="s2">&quot;auprc&quot;</span><span class="p">,</span> <span class="s2">&quot;values_1&quot;</span><span class="p">:</span> <span class="s2">&quot;latency&quot;</span><span class="p">})</span>
        <span class="o">.</span><span class="n">assign</span><span class="p">(</span><span class="n">experiment</span> <span class="o">=</span> <span class="n">experiment_type</span><span class="p">)</span>
    <span class="p">)</span>
    <span class="c1"># the following lines create a column identifing which of the trials resulted in a</span>
    <span class="c1"># Best configuration -- a model with these HPs lies along the empirical Pareto frontier</span>
    <span class="n">best_trials</span> <span class="o">=</span> <span class="p">[</span><span class="n">trial</span><span class="o">.</span><span class="n">number</span> <span class="k">for</span> <span class="n">trial</span> <span class="ow">in</span> <span class="n">optuna_experiment</span><span class="o">.</span><span class="n">best_trials</span><span class="p">]</span>
    <span class="n">df</span><span class="p">[</span><span class="s1">&#39;best_trial&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">number</span><span class="o">.</span><span class="n">isin</span><span class="p">(</span><span class="n">best_trials</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">df</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">three_panel_fig</span><span class="p">(</span><span class="n">dfs</span><span class="p">):</span>
    <span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span><span class="mi">5</span><span class="p">))</span>

    <span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">131</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">df</span> <span class="ow">in</span> <span class="n">dfs</span><span class="p">:</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s2">&quot;auprc&quot;</span><span class="p">],</span> <span class="n">df</span><span class="p">[</span><span class="s2">&quot;latency&quot;</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">df</span><span class="o">.</span><span class="n">reset_index</span><span class="p">()</span><span class="o">.</span><span class="n">experiment</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;AUPRC&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Latency (1M points)&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Comparison&quot;</span><span class="p">)</span>

    <span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">132</span><span class="p">,</span> <span class="n">sharey</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">sharex</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">dfs</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;auprc&quot;</span><span class="p">],</span> <span class="n">dfs</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;latency&quot;</span><span class="p">],</span> 
               <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">dfs</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;number&quot;</span><span class="p">],</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;Blues&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;AUPRC&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Latency (1M points)&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="n">dfs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">reset_index</span><span class="p">()</span><span class="o">.</span><span class="n">experiment</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

    <span class="k">try</span><span class="p">:</span>
        <span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">133</span><span class="p">,</span> <span class="n">sharey</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">sharex</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">dfs</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="s2">&quot;auprc&quot;</span><span class="p">],</span> <span class="n">dfs</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="s2">&quot;latency&quot;</span><span class="p">],</span> 
                   <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">dfs</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="s2">&quot;number&quot;</span><span class="p">],</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;Oranges&#39;</span><span class="p">,</span>
                   <span class="n">label</span><span class="o">=</span><span class="n">dfs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">reset_index</span><span class="p">()</span><span class="o">.</span><span class="n">experiment</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;AUPRC&quot;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Latency (1M points)&quot;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="n">dfs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">reset_index</span><span class="p">()</span><span class="o">.</span><span class="n">experiment</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="k">except</span><span class="p">:</span>
        <span class="k">pass</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">motpe_df</span> <span class="o">=</span> <span class="n">cast_as_df</span><span class="p">(</span><span class="n">motpe_experiment</span><span class="p">,</span> <span class="n">experiment_type</span><span class="o">=</span><span class="s2">&quot;MOTPE&quot;</span><span class="p">)</span>
<span class="n">random_df</span> <span class="o">=</span> <span class="n">cast_as_df</span><span class="p">(</span><span class="n">random_experiment</span><span class="p">,</span> <span class="n">experiment_type</span><span class="o">=</span><span class="s2">&quot;Random&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">motpe_df</span><span class="o">.</span><span class="n">to_csv</span><span class="p">(</span><span class="s2">&quot;optuna_fraud_motpe_100trials.csv&quot;</span><span class="p">)</span>
<span class="n">random_df</span><span class="o">.</span><span class="n">to_csv</span><span class="p">(</span><span class="s2">&quot;optuna_fraud_random_100trials.csv&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># motpe_df = pd.read_csv(&quot;optuna_fraud_motpe_100trials.csv&quot;)</span>
<span class="c1"># random_df = pd.read_csv(&quot;optuna_fraud_random_100trials.csv&quot;)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">three_panel_fig</span><span class="p">([</span><span class="n">motpe_df</span><span class="p">,</span> <span class="n">random_df</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/T892775_Exploring_Multi_Objective_Hyperparameter_Optimization_134_0.png" src="_images/T892775_Exploring_Multi_Objective_Hyperparameter_Optimization_134_0.png" />
</div>
</div>
<p>The figures above (if you loaded ours) show the results of 100 trials using MOTPE and 100 trials of random search to optimize the hyperparameters of our XGBoost model for the fraud dataset. The leftmost panel shows a comparison between the two optimization experiments. We see that the MOTPE strategy more densely samples in the desired region (low latency - high AUPRC) and finds many models that dominate in terms of latency (fastest models). However, random search is no slouch and ends up finding several models that dominate in terms of AUPRC (most predictive accuracy).</p>
<p>The middle and right panels show each experiment separately with points colored according to the trial in which they were sampled, with lighter points representing earlier trials and darker points, later trials. In the case of MOTPE, we see the lightest points are spread out over the latency-AUPRC space (this makes sense since the MOTPE algorithm is initially seeded with a few random trials), while the darker points cluster in the desirable corner. This indicates that the MOTPE algorithm is sampling strategically, attempting to identify hyperparameter configurations that optimize both objectives.</p>
<p>On the other hand, the random search strategy is random throughout! Both lighter and darker orange points are widely dispersed and it was by sheer luck that this strategy landed on some of the most predictive models. But this is exactly the point – given enough random samples, you’re bound to cover a good portion of the space, including desirable regions of low latency and high AUPRC.</p>
<p>Optuna also provides some nice visualizations.  Though we can’t plot both studies simultaneously, these figures allow one to quickly identify the trials on the Pareto frontier (in red). Hovering over each point provides the details for that trial, including the sampled hyperparameters.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span> <span class="o">=</span> <span class="n">optuna</span><span class="o">.</span><span class="n">visualization</span><span class="o">.</span><span class="n">plot_pareto_front</span><span class="p">(</span><span class="n">motpe_experiment</span><span class="p">,</span> <span class="n">target_names</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;AUPRC&quot;</span><span class="p">,</span> <span class="s2">&quot;latency&quot;</span><span class="p">])</span>
<span class="n">fig</span><span class="o">.</span><span class="n">layout</span><span class="o">.</span><span class="n">title</span> <span class="o">=</span> <span class="s1">&#39;Pareto-front MOTPE&#39;</span>
<span class="n">fig</span>
</pre></div>
</div>
</div>
</div>
<p><center><img src='https://github.com/sparsh-ai/multiobjective-optimizations/blob/main/docs/_images/T892775_6.png?raw=1'></center></p></div>
<div class="section" id="conclusion-or-something">
<h3>Conclusion or something<a class="headerlink" href="#conclusion-or-something" title="Permalink to this headline">¶</a></h3>
<p>In this section we considered a simple, yet real-world use case in which one might wish to consider multi-objective Bayesian optimization to select the hyperparameters of a machine learning model. We found that although BO techniques more densely sampled the most desirable region of our objective space (low latency, high AUPRC), it was only marginally better than a random sampling approach. We suspect that, given additional trials (for both experiments), BO would eventually provide a better Pareto frontier than random sampling. If your compute budget is considerable, this may be a viable option.  However, for most use cases, random sampling is likely sufficient. Either way, run as many trials as your compute budget allows.</p>
</div>
</div>
<div class="section" id="multi-objective-bayesian-optimization-for-approximate-nearest-neighbors-search-annoy-hnsw-lsh">
<h2>Multi-objective Bayesian optimization for Approximate Nearest Neighbors search (Annoy, HNSW, LSH)<a class="headerlink" href="#multi-objective-bayesian-optimization-for-approximate-nearest-neighbors-search-annoy-hnsw-lsh" title="Permalink to this headline">¶</a></h2>
<p>Some of the ANN algorithms explored in this section take a considerable amount of time and memory at install. They also take a long time to train during hyperparameter optimization. We’ve provided estimates of cell run times within the section. While we encourage experimentation and exploration, we include saved results so that execution of each cell is not strictly required.</p>
<blockquote>
<div><p>This section makes use of a GloVe word embeddings dataset reformatted by the ANN Benchmarks repository, a project that provides tools, code, and data for performing comparative benchmarking between various approximate neareset neighbor algorithms. However, the ANN Benchmarks library is a heavy install, designed to provide a suite of Docker containers for benchmarking purposes. We do not wish to belabor the already lengthy install for this section so we have borrowed and gently modified their dataset fetching functionality. The original can be found <a class="reference external" href="https://github.com/erikbern/ann-benchmarks/blob/18ba39b6a4af1243b26bf5fedc6c5c7ced791d70/ann_benchmarks/datasets.py">here</a>.</p>
</div></blockquote>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">code gently modified from original source at </span>
<span class="sd">https://github.com/erikbern/ann-benchmarks/blob/18ba39b6a4af1243b26bf5fedc6c5c7ced791d70/ann_benchmarks/datasets.py</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="kn">import</span> <span class="nn">h5py</span>
<span class="kn">import</span> <span class="nn">os</span>

<span class="kn">from</span> <span class="nn">urllib.request</span> <span class="kn">import</span> <span class="n">urlretrieve</span>


<span class="k">def</span> <span class="nf">download</span><span class="p">(</span><span class="n">src</span><span class="p">,</span> <span class="n">dst</span><span class="p">):</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">dst</span><span class="p">):</span>
        <span class="c1"># TODO: should be atomic</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;downloading </span><span class="si">%s</span><span class="s1"> -&gt; </span><span class="si">%s</span><span class="s1">...&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">src</span><span class="p">,</span> <span class="n">dst</span><span class="p">))</span>
        <span class="n">urlretrieve</span><span class="p">(</span><span class="n">src</span><span class="p">,</span> <span class="n">dst</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">get_dataset_fn</span><span class="p">(</span><span class="n">dataset</span><span class="p">):</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="s1">&#39;data&#39;</span><span class="p">):</span>
        <span class="n">os</span><span class="o">.</span><span class="n">mkdir</span><span class="p">(</span><span class="s1">&#39;data&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="s1">&#39;data&#39;</span><span class="p">,</span> <span class="s1">&#39;</span><span class="si">%s</span><span class="s1">.hdf5&#39;</span> <span class="o">%</span> <span class="n">dataset</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">get_dataset</span><span class="p">(</span><span class="n">which</span><span class="p">):</span>
    <span class="n">hdf5_fn</span> <span class="o">=</span> <span class="n">get_dataset_fn</span><span class="p">(</span><span class="n">which</span><span class="p">)</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">url</span> <span class="o">=</span> <span class="s1">&#39;http://ann-benchmarks.com/</span><span class="si">%s</span><span class="s1">.hdf5&#39;</span> <span class="o">%</span> <span class="n">which</span>
        <span class="n">download</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="n">hdf5_fn</span><span class="p">)</span>
    <span class="k">except</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Cannot download </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">url</span><span class="p">)</span>
    <span class="n">hdf5_f</span> <span class="o">=</span> <span class="n">h5py</span><span class="o">.</span><span class="n">File</span><span class="p">(</span><span class="n">hdf5_fn</span><span class="p">,</span> <span class="s1">&#39;r&#39;</span><span class="p">)</span>

    <span class="c1"># here for backward compatibility, to ensure old datasets can still be used with newer versions</span>
    <span class="n">dimension</span> <span class="o">=</span> <span class="n">hdf5_f</span><span class="o">.</span><span class="n">attrs</span><span class="p">[</span><span class="s1">&#39;dimension&#39;</span><span class="p">]</span> <span class="k">if</span> <span class="s1">&#39;dimension&#39;</span> <span class="ow">in</span> <span class="n">hdf5_f</span><span class="o">.</span><span class="n">attrs</span> <span class="k">else</span> <span class="nb">len</span><span class="p">(</span><span class="n">hdf5_f</span><span class="p">[</span><span class="s1">&#39;train&#39;</span><span class="p">][</span><span class="mi">0</span><span class="p">])</span>

    <span class="k">return</span> <span class="n">hdf5_f</span><span class="p">,</span> <span class="n">dimension</span>
</pre></div>
</div>
</div>
</div>
<p>In our previous sections we explored multi-objective Bayesian optimization (MOBO) applied to a <span class="xref myst">synthetic dataset</span> and to the canonical <span class="xref myst">Credit Card Fraud dataset</span>. While we learned a lot from these toy experiments, we wanted to consider a more realistic use case. A good candidate for MOBO has several qualities:</p>
<ol class="simple">
<li><p>it should be an expensive-to-run function that could potentially benefit from efficient optimization</p></li>
<li><p>it should have multiple objectives that one might realistically strive to optimize</p></li>
<li><p>and, ideally, it should have real-world utility and a more challenging dataset</p></li>
</ol>
<p>One such application that fits the bill is nearest neighbor search. In nearest neighbor search we typically have an entity represented by a vector and our goal is to indentify similar entities by finding those vectors closest to our query vector. In what follows, we explore several classes of approximate nearest neighbor search algorithms and share what we learned.</p>
<div class="section" id="nearest-neighbor-search">
<h3>Nearest Neighbor Search<a class="headerlink" href="#nearest-neighbor-search" title="Permalink to this headline">¶</a></h3>
<p>Nearest neighbor search is a ubiquitous and fundamental tool. It can be used to detect spam, find similar images, recognize handwritten digits, find semantically similar pieces of text, and recommend articles, music, or videos. Just about any interaction on the internet involves the application of a nearest neighbor search! A brute force solution computes the exact distance between every single pair of objects in our database. However, modern applications typically require searching over millions of objects, rendering the brute force solution impractically slow.  This has spawned a huge area of research and the development of a wide variety of Approximate Nearest Neighbor (ANN) algorithms that make neighbor searching more efficient - at a price. In many applications an exact solution, while nice to have, is not a necessity when a good approximation will work well for most instances. ANN algorithms thus sacrifice accuracy (and sometimes memory) to increase the efficiency of the search.
Therefore, these algorithms naturally demand trade-offs between latency, accuracy and memory footprint.</p>
<div class="section" id="ann-primer">
<h4>ANN Primer<a class="headerlink" href="#ann-primer" title="Permalink to this headline">¶</a></h4>
<p>The core idea behind the majority of ANN algorithms is that we can find an approximate solution by comparing our query point to a <em>subset</em> of points in our database, rather than exhaustively comparing to <em>all</em> points in our database (the brute force solution). The larger the subset we search over, the more likely we are to find the true set of nearest neighbors – but it will take more time. If we allow the subset to equal the size of our database, we should retrieve the exact solution – but the query would take so long that we’d lose all the users on our website who would get bored of waiting and churn off!</p>
<p>Thus, the main objective for many ANN algorithms is to devise clever ways to reduce the number of items to be scored in search of the best neighbors. Often this is accomplished by proficiently partitioning the points in our database. This partitioning can be accomplished in several ways, which we can roughly group into three families of ANN algorithms.</p>
</div>
<div class="section" id="all-in-the-fannmily">
<h4>All in the fANNmily<a class="headerlink" href="#all-in-the-fannmily" title="Permalink to this headline">¶</a></h4>
<ol class="simple">
<li><p><strong>Tree-based</strong> <br />
Trees are another classic approach to nearest neighbor search. Data points are divided hierarchically into cells and neighbors are found by traversing the tree(s). This category includes classics like k-d trees and ball trees. Like graph methods, tree-based methods have a long construction time which is especially noticeable with very large datasets.</p></li>
<li><p><strong>Graph-based</strong> <br />
These algorithms first build a graph in which each item in the database is a node with the vertices representing the distance between nodes. Fast neighbor search is typically performed through greedy graph traversal. Dozens of algorithms exist in this category, including variations on graph construction and greedy  traversal methods.</p></li>
<li><p><strong>Hashing-based</strong> <br />
These algorithms work by projecting the data points into lower-dimensional space using hashing. These hashes are “locality-sensitive” such that nearby points fall into the same hash bucket with high probability. Finding neighbors requires identifying bucket(s) that match the query hash.</p></li>
</ol>
</div>
</div>
<div class="section" id="the-game-plann">
<h3>The game plANN<a class="headerlink" href="#the-game-plann" title="Permalink to this headline">¶</a></h3>
<p>(Alright, we’ll stop with the puns.) In this section we’ll walk through how to apply multi-objective Bayesian optimization techniques to several ANN algorithms: <strong>Annoy</strong> (tree-based), <strong>HNSW</strong> (graph-based), and <strong>LSH</strong> (hashing-based), one from each of the ANN families above.  In each experiment we’ll consider three objectives: accuracy, latency, and memory footprint. Our goal will be to find the 3D Pareto frontier that defines the trade-offs between these objectives. For each experiment, we’ll perform multi-objective Bayesian optimization (MOBO) with the Optuna library and compare that to a quasi-random search.</p>
<p>First, let’s take care of some installs, imports, and the data that we’ll use for this section.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">psutil</span>
<span class="kn">import</span> <span class="nn">time</span>

<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="kn">import</span> <span class="nn">annoy</span>
<span class="kn">import</span> <span class="nn">hnswlib</span>
<span class="kn">import</span> <span class="nn">falconn</span>
<span class="kn">import</span> <span class="nn">optuna</span>
</pre></div>
</div>
</div>
</div>
<p>The next cell will take care of our data needs. We’ll be using GloVe word embeddings generated from tweets. In order to compare our predicted (approximate) nearest neighbors to ground truth, we’ll need to use a brute force method to compute what the ground truth neighbors are for a test set. Luckily, this work has already been done by the authors of the <a class="reference external" href="https://github.com/erikbern/ann-benchmarks/tree/3aedc8a314aba1c7d42b64e7ff9f3944e6822a93">ANN_benchmarks</a> library. They even include a helpful function for automatically downloading a train and test set along with ground truth neighbors to compare against.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Load data for all experiments</span>
<span class="n">dataset</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">get_dataset</span><span class="p">(</span><span class="s1">&#39;glove-100-angular&#39;</span><span class="p">)</span>

<span class="c1"># the data is structured as hd5f so we cast them to np.arrays</span>
<span class="c1"># there are 1.2M word embeddings in the train set</span>
<span class="n">X_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dataset</span><span class="p">[</span><span class="s1">&#39;train&#39;</span><span class="p">])</span>
<span class="c1"># there are 10K word embeddings in the test set</span>
<span class="n">X_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dataset</span><span class="p">[</span><span class="s1">&#39;test&#39;</span><span class="p">])</span>
<span class="c1"># these are the brute-force-computed ground truth neighbors for each item in the test set</span>
<span class="n">X_test_neighbors</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dataset</span><span class="p">[</span><span class="s1">&#39;neighbors&#39;</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<p>These constants will be used throughout the section. For each experiment we’ll look for the 10 nearest neighbors to a given query point. Each experiment will run for a minimum of 50 trials.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># for all experiments, we&#39;ll look for the 10 nearest neighbors</span>
<span class="n">N_NEIGHBORS</span> <span class="o">=</span> <span class="mi">10</span>
<span class="c1"># each experiment will also be composed fo 50 rounds of optimization</span>
<span class="n">N_TRIALS</span> <span class="o">=</span> <span class="mi">50</span>
</pre></div>
</div>
</div>
</div>
<p>Next, we have a couple of utility functions for measuring the memory usage of our algorithms, computing recall, and formatting the results of an experiment.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">get_memory_usage</span><span class="p">(</span><span class="n">pid</span><span class="p">):</span>
    <span class="c1"># given a process ID, return the memory usage in MB</span>
    <span class="k">return</span> <span class="n">psutil</span><span class="o">.</span><span class="n">Process</span><span class="p">(</span><span class="n">pid</span><span class="p">)</span><span class="o">.</span><span class="n">memory_info</span><span class="p">()</span><span class="o">.</span><span class="n">rss</span> <span class="o">/</span> <span class="mi">1024</span> <span class="o">**</span> <span class="mi">2</span>

<span class="k">def</span> <span class="nf">compute_recall</span><span class="p">(</span><span class="n">predicted_neighbors</span><span class="p">,</span> <span class="n">true_neighbors</span><span class="p">):</span>
    <span class="c1"># given a list of neighbor predictions and ground truth neighbors</span>
    <span class="c1"># compute the average recall</span>
    <span class="n">score</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">prediction</span><span class="p">,</span> <span class="n">truth</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">predicted_neighbors</span><span class="p">,</span> <span class="n">true_neighbors</span><span class="p">):</span>
        <span class="n">score</span> <span class="o">+=</span> <span class="nb">len</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">prediction</span><span class="p">)</span><span class="o">.</span><span class="n">intersection</span><span class="p">(</span><span class="n">truth</span><span class="p">))</span> <span class="o">/</span> <span class="n">N_NEIGHBORS</span>
    <span class="k">return</span> <span class="n">score</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">true_neighbors</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">cast_as_df</span><span class="p">(</span><span class="n">optuna_experiment</span><span class="p">,</span> <span class="n">experiment_type</span><span class="p">,</span> <span class="n">algorithm_name</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">(</span>
        <span class="n">optuna_experiment</span>
        <span class="o">.</span><span class="n">trials_dataframe</span><span class="p">()</span>
        <span class="o">.</span><span class="n">rename</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;values_0&quot;</span><span class="p">:</span> <span class="s2">&quot;recall&quot;</span><span class="p">,</span> <span class="s2">&quot;values_1&quot;</span><span class="p">:</span> <span class="s2">&quot;query_time&quot;</span><span class="p">,</span> <span class="s2">&quot;values_2&quot;</span><span class="p">:</span><span class="s2">&quot;index_size&quot;</span><span class="p">})</span>
        <span class="o">.</span><span class="n">assign</span><span class="p">(</span><span class="n">experiment</span> <span class="o">=</span> <span class="n">experiment_type</span><span class="p">,</span>
                <span class="n">algorithm</span> <span class="o">=</span> <span class="n">algorithm_name</span>
               <span class="p">)</span>
    <span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>We also include a few plotting routines.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">threeD_fig</span><span class="p">(</span><span class="n">dfs</span><span class="p">):</span>
    <span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">8</span><span class="p">))</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">axes</span><span class="p">(</span><span class="n">projection</span><span class="o">=</span><span class="s1">&#39;3d&#39;</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">df</span> <span class="ow">in</span> <span class="n">dfs</span><span class="p">:</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">scatter3D</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s2">&quot;recall&quot;</span><span class="p">],</span> <span class="n">df</span><span class="p">[</span><span class="s2">&quot;query_time&quot;</span><span class="p">]</span><span class="o">*</span><span class="mf">1e3</span><span class="p">,</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;index_size&#39;</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">df</span><span class="o">.</span><span class="n">reset_index</span><span class="p">()</span><span class="o">.</span><span class="n">experiment</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Recall&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Query time (ms)&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_zlabel</span><span class="p">(</span><span class="s2">&quot;Index size (MB)&quot;</span><span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">df</span><span class="o">.</span><span class="n">reset_index</span><span class="p">()</span><span class="o">.</span><span class="n">algorithm</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2"> Results&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>

    
<span class="k">def</span> <span class="nf">three_panel_fig</span><span class="p">(</span><span class="n">dfs</span><span class="p">):</span>
    <span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span><span class="mi">6</span><span class="p">))</span>

    <span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">131</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">df</span> <span class="ow">in</span> <span class="n">dfs</span><span class="p">:</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s2">&quot;recall&quot;</span><span class="p">],</span> <span class="n">df</span><span class="p">[</span><span class="s2">&quot;query_time&quot;</span><span class="p">]</span><span class="o">*</span><span class="mf">1e3</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">df</span><span class="o">.</span><span class="n">reset_index</span><span class="p">()</span><span class="o">.</span><span class="n">experiment</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Recall&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Query Time (ms)&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>

    <span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">132</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">df</span> <span class="ow">in</span> <span class="n">dfs</span><span class="p">:</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s2">&quot;recall&quot;</span><span class="p">],</span> <span class="n">df</span><span class="p">[</span><span class="s2">&quot;index_size&quot;</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">df</span><span class="o">.</span><span class="n">reset_index</span><span class="p">()</span><span class="o">.</span><span class="n">experiment</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Recall&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Index size (MB)&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">();</span>

    <span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">133</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">df</span> <span class="ow">in</span> <span class="n">dfs</span><span class="p">:</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s2">&quot;query_time&quot;</span><span class="p">]</span><span class="o">*</span><span class="mf">1e3</span><span class="p">,</span> <span class="n">df</span><span class="p">[</span><span class="s2">&quot;index_size&quot;</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">df</span><span class="o">.</span><span class="n">reset_index</span><span class="p">()</span><span class="o">.</span><span class="n">experiment</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Query time (ms)&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Index size (MB)&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">();</span>
    
    <span class="n">plt</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">df</span><span class="o">.</span><span class="n">reset_index</span><span class="p">()</span><span class="o">.</span><span class="n">algorithm</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2"> Results&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="approximate-nearest-neighbors-oh-yeah">
<h3>Approximate Nearest Neighbors, Oh Yeah!<a class="headerlink" href="#approximate-nearest-neighbors-oh-yeah" title="Permalink to this headline">¶</a></h3>
<p>No kidding, those words compose the delightful acronym <a class="reference external" href="https://github.com/spotify/annoy">Annoy</a>. Developed by the good people at Spotify for music recommendations, the algorithm behind this library falls under the tree-based methods. While the library’s author does a fantastic job describing the algorithm in <a class="reference external" href="https://erikbern.com/2015/10/01/nearest-neighbors-and-vector-models-part-2-how-to-search-in-high-dimensional-spaces.html">this</a> blog post, we provide a brief overview here (borrowing their figures for illustration).</p>
<p>At a high level, Annoy works by randomly selecting pairs of datapoints and creates a hyperplane equidistant between them to split the dataset. It does this recursively until the whole space is partitioned and each partition contains <code class="docutils literal notranslate"><span class="pre">k</span></code> datapoints.</p>
<p><center><img src='https://github.com/sparsh-ai/multiobjective-optimizations/blob/main/docs/_images/T892775_7.png?raw=1' style='width:500px;height:400px;'></center></p>
<p>Next, a binary tree is constructed from these partitions and points that are close to each other in space are likely to be close to each other in the tree as well.</p>
<p><center><img src='https://github.com/sparsh-ai/multiobjective-optimizations/blob/main/docs/_images/T892775_8.png?raw=1' style='width:800px;height:300px;'></center></p>
<p>This is performed many times until an entire forest is constructed. Neighbors are found by traversing the trees and only computing distances on those datapoints that fall in the same (or nearby) branches.</p>
</div>
<div class="section" id="hyperparameters">
<h3>Hyperparameters<a class="headerlink" href="#hyperparameters" title="Permalink to this headline">¶</a></h3>
<p>There are two main hyperparameters to consider: the number of trees (<code class="docutils literal notranslate"><span class="pre">n_trees</span></code>) to construct during build time, and the number candidate points to inspect (<code class="docutils literal notranslate"><span class="pre">search_k</span></code>) at query time. Increasing the number of trees results in better accuracy but can take more memory. Increasing the number of points to inspect also increases accuracy but takes longer.</p>
</div>
<div class="section" id="code-it-up">
<h3>Code it up<a class="headerlink" href="#code-it-up" title="Permalink to this headline">¶</a></h3>
<p>We’ll use <span class="xref myst">Optuna</span> to tune these hyperparameters. Below is the function we’ll pass to Optuna’s optimization engine. Optuna will suggest values for the hyperparameters through the <code class="docutils literal notranslate"><span class="pre">trial</span></code> object passed to the function. The function contains all the necessary pieces to build an Annoy Index and evaluate that index on the test set. The evaluation metrics include a simple computation of recall, as well as memory usage and the average query time for inference on the test set.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">train_annoy</span><span class="p">(</span><span class="n">trial</span><span class="p">):</span>    
    <span class="c1"># initialize the Annoy Index - requires the vector length and metric (&#39;angular&#39; is for cosine similarity)</span>
    <span class="n">ann</span> <span class="o">=</span> <span class="n">annoy</span><span class="o">.</span><span class="n">AnnoyIndex</span><span class="p">(</span><span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">metric</span><span class="o">=</span><span class="s1">&#39;angular&#39;</span><span class="p">)</span>

    <span class="c1"># populate the index (build the trees) with train set</span>
    <span class="n">memory_usage_before</span> <span class="o">=</span> <span class="n">get_memory_usage</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">getpid</span><span class="p">())</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">X_train</span><span class="p">):</span>
        <span class="n">ann</span><span class="o">.</span><span class="n">add_item</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">tolist</span><span class="p">())</span>
    <span class="n">ann</span><span class="o">.</span><span class="n">build</span><span class="p">(</span><span class="n">n_trees</span> <span class="o">=</span> <span class="n">trial</span><span class="o">.</span><span class="n">suggest_int</span><span class="p">(</span><span class="s2">&quot;n_trees&quot;</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
    <span class="c1"># measuring memory usage before and after the build </span>
    <span class="c1"># will give us the size of the index in MB</span>
    <span class="n">index_size</span> <span class="o">=</span> <span class="n">get_memory_usage</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">getpid</span><span class="p">())</span> <span class="o">-</span> <span class="n">memory_usage_before</span>

    <span class="c1"># run queries for each example in the test set</span>
    <span class="n">t1</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
    <span class="n">neighbors</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">X_test</span><span class="p">):</span>
        <span class="n">neighbors</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ann</span><span class="o">.</span><span class="n">get_nns_by_vector</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">tolist</span><span class="p">(),</span> 
                                               <span class="n">N_NEIGHBORS</span><span class="p">,</span> 
                                               <span class="n">search_k</span> <span class="o">=</span> <span class="n">trial</span><span class="o">.</span><span class="n">suggest_int</span><span class="p">(</span><span class="s2">&quot;search_k&quot;</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>
                                              <span class="p">))</span>
    <span class="c1"># noting the time before and after running our queries will give an</span>
    <span class="c1"># approximation of the average query_time</span>
    <span class="n">query_time</span> <span class="o">=</span> <span class="p">(</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">t1</span><span class="p">)</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

    <span class="n">recall</span> <span class="o">=</span> <span class="n">compute_recall</span><span class="p">(</span><span class="n">neighbors</span><span class="p">,</span> <span class="n">X_test_neighbors</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">recall</span><span class="p">,</span> <span class="n">query_time</span><span class="p">,</span> <span class="n">index_size</span>
</pre></div>
</div>
</div>
</div>
<p>Below we specify the MOTPE (Multi-Objective Trees of Parzen Estimators) algorithm to sample hyperparameter values. In Optuna, the optimization process is called a <code class="docutils literal notranslate"><span class="pre">study</span></code>, and we pass it our sampling strategy and the <code class="docutils literal notranslate"><span class="pre">direction</span></code> we wish to optimize our metrics.  We have three objectives: predictive performance (<code class="docutils literal notranslate"><span class="pre">recall</span></code>), speed (<code class="docutils literal notranslate"><span class="pre">query_time</span></code>), and memory consumption (<code class="docutils literal notranslate"><span class="pre">index_size</span></code>). We want to <code class="docutils literal notranslate"><span class="pre">maximize</span></code> recall and <code class="docutils literal notranslate"><span class="pre">minimize</span></code> the other two. The order should align with the order in which these variables are returned from the training function above. The third line begins the optimization exercise and here we pass it the function we wish to optimize and the number of trials it should run for.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Note: this cell takes 20-30 minutes to run</span>
<span class="n">motpe_sampler</span> <span class="o">=</span> <span class="n">optuna</span><span class="o">.</span><span class="n">samplers</span><span class="o">.</span><span class="n">MOTPESampler</span><span class="p">()</span>
<span class="n">motpe_experiment</span> <span class="o">=</span> <span class="n">optuna</span><span class="o">.</span><span class="n">create_study</span><span class="p">(</span><span class="n">sampler</span><span class="o">=</span><span class="n">motpe_sampler</span><span class="p">,</span>
                                       <span class="n">directions</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;maximize&quot;</span><span class="p">,</span> <span class="s2">&quot;minimize&quot;</span><span class="p">,</span> <span class="s2">&quot;minimize&quot;</span><span class="p">])</span>
<span class="n">motpe_experiment</span><span class="o">.</span><span class="n">optimize</span><span class="p">(</span><span class="n">train_annoy</span><span class="p">,</span> <span class="n">n_trials</span><span class="o">=</span><span class="n">N_TRIALS</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;ipython-input-8-c8caf536b12c&gt;:1: ExperimentalWarning: MOTPESampler is experimental (supported from v2.4.0). The interface can change in the future.
  motpe_sampler = optuna.samplers.MOTPESampler()
<span class=" -Color -Color-Green">[I 2021-06-24 18:02:42,741]</span> A new study created in memory with name: no-name-bb880729-97b0-4c77-8803-e339148d4e64
<span class=" -Color -Color-Green">[I 2021-06-24 18:03:05,170]</span> Trial 0 finished with values: [0.7299600000000024, 0.00014450559616088868, 801.37109375] and parameters: {&#39;n_trees&#39;: 25, &#39;search_k&#39;: 569}. 
<span class=" -Color -Color-Green">[I 2021-06-24 18:03:23,703]</span> Trial 1 finished with values: [0.6031800000000012, 9.578409194946289e-05, 909.98046875] and parameters: {&#39;n_trees&#39;: 24, &#39;search_k&#39;: 283}. 
<span class=" -Color -Color-Green">[I 2021-06-24 18:04:25,494]</span> Trial 2 finished with values: [0.8044399999999984, 0.00023747572898864745, 1981.5859375] and parameters: {&#39;n_trees&#39;: 98, &#39;search_k&#39;: 635}. 
<span class=" -Color -Color-Green">[I 2021-06-24 18:05:13,503]</span> Trial 3 finished with values: [0.7083799999999989, 0.00016117212772369385, 1998.16015625] and parameters: {&#39;n_trees&#39;: 74, &#39;search_k&#39;: 372}. 
<span class=" -Color -Color-Green">[I 2021-06-24 18:06:07,839]</span> Trial 4 finished with values: [0.5720700000000017, 0.000121159029006958, 1983.8359375] and parameters: {&#39;n_trees&#39;: 87, &#39;search_k&#39;: 164}. 
<span class=" -Color -Color-Green">[I 2021-06-24 18:07:04,034]</span> Trial 5 finished with values: [0.7731599999999986, 0.00021204431056976318, 1927.82421875] and parameters: {&#39;n_trees&#39;: 91, &#39;search_k&#39;: 525}. 
<span class=" -Color -Color-Green">[I 2021-06-24 18:07:45,555]</span> Trial 6 finished with values: [0.5702700000000009, 0.00011700329780578614, 1538.8125] and parameters: {&#39;n_trees&#39;: 65, &#39;search_k&#39;: 172}. 
<span class=" -Color -Color-Green">[I 2021-06-24 18:08:07,496]</span> Trial 7 finished with values: [0.8193899999999997, 0.0002068059206008911, 827.57421875] and parameters: {&#39;n_trees&#39;: 25, &#39;search_k&#39;: 989}. 
<span class=" -Color -Color-Green">[I 2021-06-24 18:08:29,636]</span> Trial 8 finished with values: [0.7788799999999998, 0.00017602019309997558, 840.84765625] and parameters: {&#39;n_trees&#39;: 27, &#39;search_k&#39;: 751}. 
<span class=" -Color -Color-Green">[I 2021-06-24 18:08:58,419]</span> Trial 9 finished with values: [0.6633800000000014, 0.0001228365182876587, 1097.82421875] and parameters: {&#39;n_trees&#39;: 42, &#39;search_k&#39;: 332}. 
<span class=" -Color -Color-Green">[I 2021-06-24 18:09:10,916]</span> Trial 10 finished with values: [0.7732999999999985, 0.00017564597129821776, 650.73046875] and parameters: {&#39;n_trees&#39;: 12, &#39;search_k&#39;: 917}. 
<span class=" -Color -Color-Green">[I 2021-06-24 18:09:22,751]</span> Trial 11 finished with values: [0.7667299999999982, 0.00017196671962738038, 687.89453125] and parameters: {&#39;n_trees&#39;: 10, &#39;search_k&#39;: 955}. 
<span class=" -Color -Color-Green">[I 2021-06-24 18:09:54,971]</span> Trial 12 finished with values: [0.8209599999999975, 0.00021422030925750732, 1527.29296875] and parameters: {&#39;n_trees&#39;: 47, &#39;search_k&#39;: 843}. 
<span class=" -Color -Color-Green">[I 2021-06-24 18:10:18,855]</span> Trial 13 finished with values: [0.7366799999999991, 0.00013359060287475587, 1176.86328125] and parameters: {&#39;n_trees&#39;: 38, &#39;search_k&#39;: 523}. 
<span class=" -Color -Color-Green">[I 2021-06-24 18:10:55,671]</span> Trial 14 finished with values: [0.7936599999999968, 0.0001958622932434082, 1544.11328125] and parameters: {&#39;n_trees&#39;: 56, &#39;search_k&#39;: 675}. 
<span class=" -Color -Color-Green">[I 2021-06-24 18:11:11,346]</span> Trial 15 finished with values: [0.6666200000000009, 0.00011324610710144043, 833.74609375] and parameters: {&#39;n_trees&#39;: 16, &#39;search_k&#39;: 454}. 
<span class=" -Color -Color-Green">[I 2021-06-24 18:11:36,741]</span> Trial 16 finished with values: [0.8027699999999978, 0.00019883131980895995, 1105.79296875] and parameters: {&#39;n_trees&#39;: 35, &#39;search_k&#39;: 802}. 
<span class=" -Color -Color-Green">[I 2021-06-24 18:12:12,620]</span> Trial 17 finished with values: [0.7831899999999992, 0.00018567397594451904, 1530.0625] and parameters: {&#39;n_trees&#39;: 52, &#39;search_k&#39;: 645}. 
<span class=" -Color -Color-Green">[I 2021-06-24 18:12:56,596]</span> Trial 18 finished with values: [0.6328900000000038, 0.00012887351512908937, 1471.1015625] and parameters: {&#39;n_trees&#39;: 68, &#39;search_k&#39;: 245}. 
<span class=" -Color -Color-Green">[I 2021-06-24 18:13:12,974]</span> Trial 19 finished with values: [0.6562900000000014, 0.00011083180904388428, 899.1171875] and parameters: {&#39;n_trees&#39;: 18, &#39;search_k&#39;: 412}. 
<span class=" -Color -Color-Green">[I 2021-06-24 18:13:24,791]</span> Trial 20 finished with values: [0.7583599999999977, 0.0001661931037902832, 694.23828125] and parameters: {&#39;n_trees&#39;: 10, &#39;search_k&#39;: 907}. 
<span class=" -Color -Color-Green">[I 2021-06-24 18:13:36,380]</span> Trial 21 finished with values: [0.7242000000000005, 0.0001424143075942993, 714.40625] and parameters: {&#39;n_trees&#39;: 10, &#39;search_k&#39;: 736}. 
<span class=" -Color -Color-Green">[I 2021-06-24 18:13:52,048]</span> Trial 22 finished with values: [0.7798900000000005, 0.00017392451763153076, 693.46875] and parameters: {&#39;n_trees&#39;: 15, &#39;search_k&#39;: 887}. 
<span class=" -Color -Color-Green">[I 2021-06-24 18:14:03,510]</span> Trial 23 finished with values: [0.7234700000000004, 0.0001459075927734375, 653.05859375] and parameters: {&#39;n_trees&#39;: 10, &#39;search_k&#39;: 733}. 
<span class=" -Color -Color-Green">[I 2021-06-24 18:14:26,743]</span> Trial 24 finished with values: [0.8045899999999995, 0.00019436302185058593, 1120.30859375] and parameters: {&#39;n_trees&#39;: 30, &#39;search_k&#39;: 849}. 
<span class=" -Color -Color-Green">[I 2021-06-24 18:14:43,709]</span> Trial 25 finished with values: [0.7545700000000019, 0.00015453028678894044, 890.98046875] and parameters: {&#39;n_trees&#39;: 19, &#39;search_k&#39;: 717}. 
<span class=" -Color -Color-Green">[I 2021-06-24 18:15:07,508]</span> Trial 26 finished with values: [0.7474999999999997, 0.00015871598720550536, 1181.91796875] and parameters: {&#39;n_trees&#39;: 32, &#39;search_k&#39;: 589}. 
<span class=" -Color -Color-Green">[I 2021-06-24 18:15:24,201]</span> Trial 27 finished with values: [0.7669500000000011, 0.0001666718006134033, 834.72265625] and parameters: {&#39;n_trees&#39;: 18, &#39;search_k&#39;: 778}. 
<span class=" -Color -Color-Green">[I 2021-06-24 18:15:53,404]</span> Trial 28 finished with values: [0.728969999999999, 0.00015100059509277344, 1112.0078125] and parameters: {&#39;n_trees&#39;: 42, &#39;search_k&#39;: 483}. 
<span class=" -Color -Color-Green">[I 2021-06-24 18:16:44,129]</span> Trial 29 finished with values: [0.8400099999999983, 0.00025642433166503907, 1993.1796875] and parameters: {&#39;n_trees&#39;: 81, &#39;search_k&#39;: 845}. 
<span class=" -Color -Color-Green">[I 2021-06-24 18:17:01,106]</span> Trial 30 finished with values: [0.8037499999999994, 0.00018991279602050782, 911.359375] and parameters: {&#39;n_trees&#39;: 18, &#39;search_k&#39;: 976}. 
<span class=" -Color -Color-Green">[I 2021-06-24 18:17:18,537]</span> Trial 31 finished with values: [0.6878500000000018, 0.00012064528465270997, 927.83984375] and parameters: {&#39;n_trees&#39;: 21, &#39;search_k&#39;: 472}. 
<span class=" -Color -Color-Green">[I 2021-06-24 18:17:42,684]</span> Trial 32 finished with values: [0.8288600000000006, 0.00021294629573822023, 1178.1328125] and parameters: {&#39;n_trees&#39;: 31, &#39;search_k&#39;: 982}. 
<span class=" -Color -Color-Green">[I 2021-06-24 18:17:57,691]</span> Trial 33 finished with values: [0.6510400000000003, 0.00011054368019104004, 620.92578125] and parameters: {&#39;n_trees&#39;: 15, &#39;search_k&#39;: 428}. 
<span class=" -Color -Color-Green">[I 2021-06-24 18:18:11,695]</span> Trial 34 finished with values: [0.42728999999999095, 5.519480705261231e-05, 630.40625] and parameters: {&#39;n_trees&#39;: 14, &#39;search_k&#39;: 122}. 
<span class=" -Color -Color-Green">[I 2021-06-24 18:18:29,810]</span> Trial 35 finished with values: [0.6087200000000014, 9.948570728302002e-05, 830.0234375] and parameters: {&#39;n_trees&#39;: 24, &#39;search_k&#39;: 293}. 
<span class=" -Color -Color-Green">[I 2021-06-24 18:18:54,496]</span> Trial 36 finished with values: [0.6104800000000002, 0.0001061837911605835, 1185.5625] and parameters: {&#39;n_trees&#39;: 36, &#39;search_k&#39;: 257}. 
<span class=" -Color -Color-Green">[I 2021-06-24 18:19:12,280]</span> Trial 37 finished with values: [0.6568100000000034, 0.00010924019813537598, 903.703125] and parameters: {&#39;n_trees&#39;: 23, &#39;search_k&#39;: 387}. 
<span class=" -Color -Color-Green">[I 2021-06-24 18:19:34,060]</span> Trial 38 finished with values: [0.6419700000000035, 0.00010734679698944091, 911.609375] and parameters: {&#39;n_trees&#39;: 28, &#39;search_k&#39;: 335}. 
<span class=" -Color -Color-Green">[I 2021-06-24 18:20:11,530]</span> Trial 39 finished with values: [0.7220300000000007, 0.00015792412757873535, 1465.9609375] and parameters: {&#39;n_trees&#39;: 58, &#39;search_k&#39;: 428}. 
<span class=" -Color -Color-Green">[I 2021-06-24 18:20:29,550]</span> Trial 40 finished with values: [0.6341900000000029, 0.000102762770652771, 902.51953125] and parameters: {&#39;n_trees&#39;: 23, &#39;search_k&#39;: 342}. 
<span class=" -Color -Color-Green">[I 2021-06-24 18:20:44,062]</span> Trial 41 finished with values: [0.6855100000000016, 0.00011883602142333984, 718.1796875] and parameters: {&#39;n_trees&#39;: 15, &#39;search_k&#39;: 518}. 
<span class=" -Color -Color-Green">[I 2021-06-24 18:20:56,515]</span> Trial 42 finished with values: [0.7197200000000007, 0.00013703577518463133, 710.81640625] and parameters: {&#39;n_trees&#39;: 12, &#39;search_k&#39;: 669}. 
<span class=" -Color -Color-Green">[I 2021-06-24 18:21:08,755]</span> Trial 43 finished with values: [0.7029600000000004, 0.0001284583330154419, 690.0546875] and parameters: {&#39;n_trees&#39;: 12, &#39;search_k&#39;: 613}. 
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span><span class=" -Color -Color-Green">[I 2021-06-24 18:21:23,797]</span> Trial 44 finished with values: [0.7817899999999973, 0.00017605998516082765, 695.63671875] and parameters: {&#39;n_trees&#39;: 13, &#39;search_k&#39;: 936}. 
<span class=" -Color -Color-Green">[I 2021-06-24 18:21:35,373]</span> Trial 45 finished with values: [0.7119300000000002, 0.00014063560962677003, 706.6796875] and parameters: {&#39;n_trees&#39;: 10, &#39;search_k&#39;: 687}. 
<span class=" -Color -Color-Green">[I 2021-06-24 18:21:53,367]</span> Trial 46 finished with values: [0.7334200000000008, 0.00014655680656433105, 902.859375] and parameters: {&#39;n_trees&#39;: 21, &#39;search_k&#39;: 612}. 
<span class=" -Color -Color-Green">[I 2021-06-24 18:22:16,122]</span> Trial 47 finished with values: [0.7953299999999996, 0.0001865869998931885, 842.125] and parameters: {&#39;n_trees&#39;: 28, &#39;search_k&#39;: 817}. 
<span class=" -Color -Color-Green">[I 2021-06-24 18:22:31,404]</span> Trial 48 finished with values: [0.7773999999999993, 0.00017059760093688964, 697.41796875] and parameters: {&#39;n_trees&#39;: 14, &#39;search_k&#39;: 893}. 
<span class=" -Color -Color-Green">[I 2021-06-24 18:22:55,605]</span> Trial 49 finished with values: [0.7425200000000012, 0.0001503593921661377, 1178.1640625] and parameters: {&#39;n_trees&#39;: 33, &#39;search_k&#39;: 566}. 
</pre></div>
</div>
</div>
</div>
<p>Want to run more trials? Optuna makes it easy. Simply keep running the experiment!</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># (OPTIONAL) run an additional N_TRIALS </span>
<span class="c1"># motpe_experiment.optimize(train_annoy, n_trials=N_TRIALS)</span>
</pre></div>
</div>
</div>
</div>
<p>Setting up an experiment that uses random sampling (rather than the fancy MOTPE strategy) is similarly straightforward. Just change the sampler!</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Note: this cell takes 20-30 minutes to run</span>
<span class="n">random_sampler</span> <span class="o">=</span> <span class="n">optuna</span><span class="o">.</span><span class="n">samplers</span><span class="o">.</span><span class="n">RandomSampler</span><span class="p">()</span>
<span class="n">random_experiment</span> <span class="o">=</span> <span class="n">optuna</span><span class="o">.</span><span class="n">create_study</span><span class="p">(</span><span class="n">sampler</span><span class="o">=</span><span class="n">random_sampler</span><span class="p">,</span> <span class="n">directions</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;maximize&quot;</span><span class="p">,</span> <span class="s2">&quot;minimize&quot;</span><span class="p">,</span> <span class="s2">&quot;minimize&quot;</span><span class="p">])</span>
<span class="n">random_experiment</span><span class="o">.</span><span class="n">optimize</span><span class="p">(</span><span class="n">train_annoy</span><span class="p">,</span> <span class="n">n_trials</span><span class="o">=</span><span class="n">N_TRIALS</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span><span class=" -Color -Color-Green">[I 2021-06-24 18:25:38,503]</span> A new study created in memory with name: no-name-4b99c314-d172-4807-80a7-e6272f5bec81
<span class=" -Color -Color-Green">[I 2021-06-24 18:26:29,808]</span> Trial 0 finished with values: [0.6405900000000024, 0.0001361593723297119, 1994.2421875] and parameters: {&#39;n_trees&#39;: 81, &#39;search_k&#39;: 244}. 
<span class=" -Color -Color-Green">[I 2021-06-24 18:27:28,174]</span> Trial 1 finished with values: [0.610740000000003, 0.0001340374231338501, 1989.5] and parameters: {&#39;n_trees&#39;: 95, &#39;search_k&#39;: 195}. 
<span class=" -Color -Color-Green">[I 2021-06-24 18:27:44,941]</span> Trial 2 finished with values: [0.775880000000001, 0.0001720470905303955, 909.9296875] and parameters: {&#39;n_trees&#39;: 16, &#39;search_k&#39;: 848}. 
<span class=" -Color -Color-Green">[I 2021-06-24 18:28:10,219]</span> Trial 3 finished with values: [0.7087000000000028, 0.00013290958404541016, 1176.19140625] and parameters: {&#39;n_trees&#39;: 31, &#39;search_k&#39;: 474}. 
<span class=" -Color -Color-Green">[I 2021-06-24 18:29:05,679]</span> Trial 4 finished with values: [0.6768700000000033, 0.00016857590675354003, 1920.81640625] and parameters: {&#39;n_trees&#39;: 88, &#39;search_k&#39;: 295}. 
<span class=" -Color -Color-Green">[I 2021-06-24 18:29:48,169]</span> Trial 5 finished with values: [0.751759999999999, 0.00018195850849151611, 1463.890625] and parameters: {&#39;n_trees&#39;: 63, &#39;search_k&#39;: 505}. 
<span class=" -Color -Color-Green">[I 2021-06-24 18:30:38,833]</span> Trial 6 finished with values: [0.5292199999999988, 0.0001224360227584839, 1995.67578125] and parameters: {&#39;n_trees&#39;: 82, &#39;search_k&#39;: 130}. 
<span class=" -Color -Color-Green">[I 2021-06-24 18:31:20,187]</span> Trial 7 finished with values: [0.5912800000000008, 0.00010979037284851074, 1530.92578125] and parameters: {&#39;n_trees&#39;: 63, &#39;search_k&#39;: 193}. 
<span class=" -Color -Color-Green">[I 2021-06-24 18:32:17,489]</span> Trial 8 finished with values: [0.73897, 0.00019058051109313966, 1922.4765625] and parameters: {&#39;n_trees&#39;: 94, &#39;search_k&#39;: 420}. 
<span class=" -Color -Color-Green">[I 2021-06-24 18:32:49,072]</span> Trial 9 finished with values: [0.8240199999999973, 0.00021805870532989503, 1168.44140625] and parameters: {&#39;n_trees&#39;: 46, &#39;search_k&#39;: 863}. 
<span class=" -Color -Color-Green">[I 2021-06-24 18:33:17,053]</span> Trial 10 finished with values: [0.75525, 0.00016548652648925782, 1187.34375] and parameters: {&#39;n_trees&#39;: 37, &#39;search_k&#39;: 590}. 
<span class=" -Color -Color-Green">[I 2021-06-24 18:33:41,866]</span> Trial 11 finished with values: [0.692780000000003, 0.00012705209255218505, 1178.59375] and parameters: {&#39;n_trees&#39;: 32, &#39;search_k&#39;: 430}. 
<span class=" -Color -Color-Green">[I 2021-06-24 18:34:32,207]</span> Trial 12 finished with values: [0.6727800000000022, 0.0001531501054763794, 1994.86328125] and parameters: {&#39;n_trees&#39;: 76, &#39;search_k&#39;: 299}. 
<span class=" -Color -Color-Green">[I 2021-06-24 18:34:53,423]</span> Trial 13 finished with values: [0.4582099999999953, 6.970939636230469e-05, 904.04296875] and parameters: {&#39;n_trees&#39;: 25, &#39;search_k&#39;: 119}. 
<span class=" -Color -Color-Green">[I 2021-06-24 18:35:21,704]</span> Trial 14 finished with values: [0.4853999999999963, 8.091049194335938e-05, 1172.0859375] and parameters: {&#39;n_trees&#39;: 38, &#39;search_k&#39;: 119}. 
<span class=" -Color -Color-Green">[I 2021-06-24 18:35:58,778]</span> Trial 15 finished with values: [0.6589500000000013, 0.0001283493995666504, 1466.59765625] and parameters: {&#39;n_trees&#39;: 52, &#39;search_k&#39;: 307}. 
<span class=" -Color -Color-Green">[I 2021-06-24 18:36:29,325]</span> Trial 16 finished with values: [0.6731800000000011, 0.0001264847993850708, 1109.1953125] and parameters: {&#39;n_trees&#39;: 42, &#39;search_k&#39;: 352}. 
<span class=" -Color -Color-Green">[I 2021-06-24 18:37:15,711]</span> Trial 17 finished with values: [0.5399799999999996, 0.0001023080825805664, 1990.76953125] and parameters: {&#39;n_trees&#39;: 73, &#39;search_k&#39;: 145}. 
<span class=" -Color -Color-Green">[I 2021-06-24 18:38:05,826]</span> Trial 18 finished with values: [0.8563199999999987, 0.0002738676786422729, 1991.54296875] and parameters: {&#39;n_trees&#39;: 78, &#39;search_k&#39;: 960}. 
<span class=" -Color -Color-Green">[I 2021-06-24 18:38:25,671]</span> Trial 19 finished with values: [0.7578100000000022, 0.00015554838180541992, 897.015625] and parameters: {&#39;n_trees&#39;: 24, &#39;search_k&#39;: 676}. 
<span class=" -Color -Color-Green">[I 2021-06-24 18:39:09,510]</span> Trial 20 finished with values: [0.7968899999999983, 0.00020785210132598877, 1543.46484375] and parameters: {&#39;n_trees&#39;: 68, &#39;search_k&#39;: 657}. 
<span class=" -Color -Color-Green">[I 2021-06-24 18:40:00,529]</span> Trial 21 finished with values: [0.5247599999999989, 0.00010677750110626221, 1910.546875] and parameters: {&#39;n_trees&#39;: 84, &#39;search_k&#39;: 124}. 
<span class=" -Color -Color-Green">[I 2021-06-24 18:40:43,683]</span> Trial 22 finished with values: [0.8408699999999976, 0.000244869589805603, 1538.390625] and parameters: {&#39;n_trees&#39;: 66, &#39;search_k&#39;: 890}. 
<span class=" -Color -Color-Green">[I 2021-06-24 18:41:25,844]</span> Trial 23 finished with values: [0.5076299999999963, 9.353542327880859e-05, 1469.1328125] and parameters: {&#39;n_trees&#39;: 68, &#39;search_k&#39;: 108}. 
<span class=" -Color -Color-Green">[I 2021-06-24 18:41:51,836]</span> Trial 24 finished with values: [0.8174099999999969, 0.00021065123081207276, 1173.39453125] and parameters: {&#39;n_trees&#39;: 36, &#39;search_k&#39;: 876}. 
<span class=" -Color -Color-Green">[I 2021-06-24 18:42:54,072]</span> Trial 25 finished with values: [0.8683199999999982, 0.0003084920883178711, 2590.90234375] and parameters: {&#39;n_trees&#39;: 100, &#39;search_k&#39;: 994}. 
<span class=" -Color -Color-Green">[I 2021-06-24 18:43:39,445]</span> Trial 26 finished with values: [0.8083099999999973, 0.00022102899551391602, 1986.6796875] and parameters: {&#39;n_trees&#39;: 72, &#39;search_k&#39;: 703}. 
<span class=" -Color -Color-Green">[I 2021-06-24 18:44:35,354]</span> Trial 27 finished with values: [0.7047000000000015, 0.00016818852424621582, 1994.76953125] and parameters: {&#39;n_trees&#39;: 91, &#39;search_k&#39;: 344}. 
<span class=" -Color -Color-Green">[I 2021-06-24 18:45:26,462]</span> Trial 28 finished with values: [0.6810300000000028, 0.000154721999168396, 1922.58203125] and parameters: {&#39;n_trees&#39;: 84, &#39;search_k&#39;: 307}. 
<span class=" -Color -Color-Green">[I 2021-06-24 18:46:04,125]</span> Trial 29 finished with values: [0.71714, 0.00015302631855010986, 1523.015625] and parameters: {&#39;n_trees&#39;: 59, &#39;search_k&#39;: 413}. 
<span class=" -Color -Color-Green">[I 2021-06-24 18:46:48,840]</span> Trial 30 finished with values: [0.7963199999999983, 0.00020971572399139404, 1988.1953125] and parameters: {&#39;n_trees&#39;: 70, &#39;search_k&#39;: 652}. 
<span class=" -Color -Color-Green">[I 2021-06-24 18:47:10,429]</span> Trial 31 finished with values: [0.5947800000000004, 9.425170421600341e-05, 847.65625] and parameters: {&#39;n_trees&#39;: 28, &#39;search_k&#39;: 256}. 
<span class=" -Color -Color-Green">[I 2021-06-24 18:47:46,931]</span> Trial 32 finished with values: [0.7523000000000003, 0.00016569981575012207, 1535.12890625] and parameters: {&#39;n_trees&#39;: 55, &#39;search_k&#39;: 521}. 
<span class=" -Color -Color-Green">[I 2021-06-24 18:48:30,519]</span> Trial 33 finished with values: [0.8513299999999985, 0.00026622021198272706, 1452.90625] and parameters: {&#39;n_trees&#39;: 64, &#39;search_k&#39;: 966}. 
<span class=" -Color -Color-Green">[I 2021-06-24 18:49:01,470]</span> Trial 34 finished with values: [0.7229899999999989, 0.0001468843698501587, 1178.17578125] and parameters: {&#39;n_trees&#39;: 42, &#39;search_k&#39;: 467}. 
<span class=" -Color -Color-Green">[I 2021-06-24 18:49:31,540]</span> Trial 35 finished with values: [0.789849999999998, 0.00018884520530700684, 1108.00390625] and parameters: {&#39;n_trees&#39;: 41, &#39;search_k&#39;: 710}. 
<span class=" -Color -Color-Green">[I 2021-06-24 18:50:20,978]</span> Trial 36 finished with values: [0.8119899999999985, 0.00023058078289031982, 1916.4453125] and parameters: {&#39;n_trees&#39;: 73, &#39;search_k&#39;: 718}. 
<span class=" -Color -Color-Green">[I 2021-06-24 18:50:56,868]</span> Trial 37 finished with values: [0.6314399999999996, 0.0001175858974456787, 1457.734375] and parameters: {&#39;n_trees&#39;: 53, &#39;search_k&#39;: 259}. 
<span class=" -Color -Color-Green">[I 2021-06-24 18:51:59,828]</span> Trial 38 finished with values: [0.8107799999999986, 0.00024778900146484376, 2001.27734375] and parameters: {&#39;n_trees&#39;: 97, &#39;search_k&#39;: 667}. 
<span class=" -Color -Color-Green">[I 2021-06-24 18:52:16,289]</span> Trial 39 finished with values: [0.7687400000000012, 0.00016920382976531984, 834.99609375] and parameters: {&#39;n_trees&#39;: 16, &#39;search_k&#39;: 813}. 
<span class=" -Color -Color-Green">[I 2021-06-24 18:52:30,916]</span> Trial 40 finished with values: [0.53536, 7.796227931976318e-05, 631.41796875] and parameters: {&#39;n_trees&#39;: 15, &#39;search_k&#39;: 227}. 
<span class=" -Color -Color-Green">[I 2021-06-24 18:53:02,560]</span> Trial 41 finished with values: [0.6267400000000025, 0.00011182148456573487, 1170.63671875] and parameters: {&#39;n_trees&#39;: 46, &#39;search_k&#39;: 262}. 
<span class=" -Color -Color-Green">[I 2021-06-24 18:53:51,157]</span> Trial 42 finished with values: [0.5177099999999976, 9.809987545013427e-05, 1998.7421875] and parameters: {&#39;n_trees&#39;: 77, &#39;search_k&#39;: 118}. 
<span class=" -Color -Color-Green">[I 2021-06-24 18:54:03,948]</span> Trial 43 finished with values: [0.7686799999999996, 0.0001807464361190796, 619.0390625] and parameters: {&#39;n_trees&#39;: 12, &#39;search_k&#39;: 889}. 
<span class=" -Color -Color-Green">[I 2021-06-24 18:54:39,651]</span> Trial 44 finished with values: [0.785409999999997, 0.00020225830078125, 1543.421875] and parameters: {&#39;n_trees&#39;: 51, &#39;search_k&#39;: 659}. 
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span><span class=" -Color -Color-Green">[I 2021-06-24 18:54:58,392]</span> Trial 45 finished with values: [0.7507200000000016, 0.00015626652240753174, 824.8046875] and parameters: {&#39;n_trees&#39;: 23, &#39;search_k&#39;: 659}. 
<span class=" -Color -Color-Green">[I 2021-06-24 18:55:15,463]</span> Trial 46 finished with values: [0.7031799999999997, 0.00013080110549926758, 946.61328125] and parameters: {&#39;n_trees&#39;: 19, &#39;search_k&#39;: 528}. 
<span class=" -Color -Color-Green">[I 2021-06-24 18:56:17,086]</span> Trial 47 finished with values: [0.7725799999999986, 0.00021990971565246582, 1964.546875] and parameters: {&#39;n_trees&#39;: 97, &#39;search_k&#39;: 515}. 
<span class=" -Color -Color-Green">[I 2021-06-24 18:56:46,749]</span> Trial 48 finished with values: [0.7408799999999989, 0.00015285589694976807, 1171.859375] and parameters: {&#39;n_trees&#39;: 41, &#39;search_k&#39;: 525}. 
<span class=" -Color -Color-Green">[I 2021-06-24 18:57:42,778]</span> Trial 49 finished with values: [0.533529999999999, 0.00011064469814300537, 1928.34765625] and parameters: {&#39;n_trees&#39;: 92, &#39;search_k&#39;: 130}. 
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># (OPTIONAL) run an additional N_TRIALS</span>
<span class="c1"># random_experiment.optimize(train_annoy, n_trials=N_TRIALS)</span>
</pre></div>
</div>
</div>
</div>
<p>The output of these experiments include a Pandas dataframe summarizing the results. We created a wrapper to format that output in a nicer way.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">annoy_motpe_df</span> <span class="o">=</span> <span class="n">cast_as_df</span><span class="p">(</span><span class="n">motpe_experiment</span><span class="p">,</span> <span class="n">experiment_type</span><span class="o">=</span><span class="s2">&quot;MOTPE&quot;</span><span class="p">,</span> <span class="n">algorithm_name</span><span class="o">=</span><span class="s2">&quot;Annoy&quot;</span><span class="p">)</span>
<span class="n">annoy_random_df</span> <span class="o">=</span> <span class="n">cast_as_df</span><span class="p">(</span><span class="n">random_experiment</span><span class="p">,</span> <span class="n">experiment_type</span><span class="o">=</span><span class="s2">&quot;Random&quot;</span><span class="p">,</span> <span class="n">algorithm_name</span><span class="o">=</span><span class="s2">&quot;Annoy&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Let’s look at the results!</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">three_panel_fig</span><span class="p">([</span><span class="n">annoy_motpe_df</span><span class="p">,</span> <span class="n">annoy_random_df</span><span class="p">])</span><span class="c1">#</span>
<span class="c1">#threeD_fig([motpe_df, random_df])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/T892775_Exploring_Multi_Objective_Hyperparameter_Optimization_165_0.png" src="_images/T892775_Exploring_Multi_Objective_Hyperparameter_Optimization_165_0.png" />
</div>
</div>
</div>
<div class="section" id="discussion-time">
<h3>Discussion time<a class="headerlink" href="#discussion-time" title="Permalink to this headline">¶</a></h3>
<p>Let’s unpack this a bit. We had three objectives to optimize: recall, latency, and memory. In the figure above we show the relationship between all possible combinations of objectives (rather than showing a 3D scatter plot, which, while cool, is harder to glean useful information from. If you’re hankering for some 3D, uncomment the other fig command and have at it).</p>
<p>The leftmost panel shows the resulting <code class="docutils literal notranslate"><span class="pre">query_time</span></code> vs <code class="docutils literal notranslate"><span class="pre">recall</span></code> and a lovely curve demonstrating the trade-off between these two objectives.  The blue MOTPE points seem to be more densely clustered closer to the edge of the latency-recall Pareto frontier. However, the orange points sample it well enough. That these two distributions are so similar is not actually a surprise. It turns out that the <code class="docutils literal notranslate"><span class="pre">search_k</span></code> parameter is explicitly designed to directly control this trade-off: larger values of <code class="docutils literal notranslate"><span class="pre">search_k</span></code> result in higher accuracy <em>and</em> longer query times. This means that tracing out the Pareto frontier is almost entirely a matter of fixing <code class="docutils literal notranslate"><span class="pre">n_trees</span></code> (our other hyperparameter), and sweeping over <code class="docutils literal notranslate"><span class="pre">search_k</span></code>. The results would yield a similar curve.</p>
<p>If those were the only two objectives we cared about, we’d be done! But since we also care about memory, the problem is a bit more complex. In the middle and third panels, we see Index size (in MB) vs the other two objectives, and here we see a bit more difference between the MOTPE algorithm and randomly sampled hyperparameter values. The MOTPE algorithm better samples the regions we care about (low memory, fast query times, and high recall), but it’s not always a significant difference, even when we run for many more trials!</p>
<p><center><img src='https://github.com/sparsh-ai/multiobjective-optimizations/blob/main/docs/_images/T892775_9.png?raw=1'></center></p><p>Some other things to note:</p>
<ol class="simple">
<li><p>To run the optimization in a realistic amount of time, we provided the Annoy algorithm with artifically low values for its hyperparameters. The algorithm is fairly competitive and can achieve much better recall on the GloVe dataset. However, the build and query times associated with these preferrable hyperparameter values result in an optimization time on the order of several hours. It a pesky, double-edged sword: while we want to demonstrate realistic use cases / algorithms / datasets, they come with realistic compute times which are <em>considerable</em>.</p></li>
<li><p>It’s important to keep in mind that it may not be physically possible to densely sample the entire 3D output objective volume – there is no guarantee that combinations of hyperparameters exist such that all white space could be covered by orange or blue points.</p></li>
</ol>
<p>So why didn’t MOBO run circles around random search? Maybe Annoy doesn’t have enough hyperparameters to optimize over. Maybe it’s just too simple. We’ll come back to this discussion towards the end. For now, let’s try our next ANN, which has three hyperparameters.</p>
</div>
<div class="section" id="hierarchical-navigable-small-worlds">
<h3>Hierarchical Navigable Small Worlds<a class="headerlink" href="#hierarchical-navigable-small-worlds" title="Permalink to this headline">¶</a></h3>
<p>A classic brute-force method for computing nearest neighbors to a query point is to construct a KNN graph. While these work great for small datasets, they don’t scale well to millions of points. Enter Hierarchical Navigable Small Worlds (HNSW) graphs. The name says a lot so let’s break it down.</p>
<p>A “small world” graph (or network) is one in which the nodes are highly clustered (most nodes are not neighbors of one another), but the <em>neighbors</em> of any give node are likely to be neighbors of <em>each other</em>, resulting in a graph in which most nodes can be reached by every other node in only a few steps. While that might seem paradoxical, think of the distribution of air traffic. Planes fly mostly from busy, centralized hubs (highly clustered), and it only takes a few stop-overs to go from your local airport to just about any major city in the world.</p>
<p>If we could structure our dataset in such a way that it has the properties of a small world graph, we’d be in decent shape because we could quickly find neighbors anywhere in the graph in a small number of steps. In practice, though, that’s not enough. Tracing along the nodes in a graph still takes linear time (it takes <em>n</em> time steps to move <em>n</em> nodes). However, if a small world graph is also <em>navigable</em> then it possesses an additional property: routing through the graph can be done on a logarathimic scale (those same <em>n</em> nodes can now be traversed in only <em>log(n)</em> time), which is much faster! But we can do even better.</p>
<p>We finally come to the “heirarchical” part. The HNSW algorithm constructs a hierarchy of NSW graphs. The first layer is a graph of all the points in our database, where each point is connected to others on a small, characteristic scale length. Next, a subset of these points is kicked up to a second layer, where these points are connected via a slightly longer scale length. Then a subset of this subset is kicked up to a third layer with an even larger scale length connection, and so on.</p>
<p><center><img src='https://github.com/sparsh-ai/multiobjective-optimizations/blob/main/docs/_images/T892775_10.png?raw=1'></center></p><p>When a query comes in, the algorithm starts at the top layer, greedily searching for nearest neighbors on the graph with the longest scale length. The results of this layer are then passed down to the layer below, where new, closer neighbors are found. This continues until the algorithm reaches lowest layer, finally returning the closest neighbors.</p>
</div>
<div class="section" id="id9">
<h3>Hyperparameters<a class="headerlink" href="#id9" title="Permalink to this headline">¶</a></h3>
<p>We’ll use the <a class="reference external" href="https://github.com/nmslib/hnswlib">hnswlib</a> library for this algorithm. This implementation has three tunable hyperparameters.</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">ef</span></code> sets the size of the dynamic list of candidates to evaluate at query time. This parameter affects the latency-recall trade-off, where large values of <code class="docutils literal notranslate"><span class="pre">ef</span></code> yield better accuracy but longer query times.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">ef_construction</span></code>: This has the same meaning as <code class="docutils literal notranslate"><span class="pre">ef</span></code> but during the construction phase. It influences the construction time as well as the quality of the index. Larger values result in a better index but take longer to build.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">M</span></code>: this sets the number of bidirectional connections between nodes in the graph. More nodes mean more memory usage but also better connections between other nodes. Larger values of <code class="docutils literal notranslate"><span class="pre">M</span></code> are often better for high-dimensional vectors (like the word embeddings we’re using).</p></li>
</ul>
</div>
<div class="section" id="id10">
<h3>Code it up<a class="headerlink" href="#id10" title="Permalink to this headline">¶</a></h3>
<p>Very little changes from our previous function! We still initialize and populate an index even though, behind the scenes, the algorithm is constructing a very different object from the Annoy algorithm.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">train_hnsw</span><span class="p">(</span><span class="n">trial</span><span class="p">):</span> 
    
    <span class="n">ann</span> <span class="o">=</span> <span class="n">hnswlib</span><span class="o">.</span><span class="n">Index</span><span class="p">(</span><span class="n">space</span><span class="o">=</span><span class="s1">&#39;cosine&#39;</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
    
    <span class="n">num_elements</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
    
    <span class="c1"># Initializing index - the maximum number of elements should be known beforehand</span>
    <span class="n">memory_usage_before</span> <span class="o">=</span> <span class="n">get_memory_usage</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">getpid</span><span class="p">())</span>
    <span class="n">ann</span><span class="o">.</span><span class="n">init_index</span><span class="p">(</span><span class="n">max_elements</span> <span class="o">=</span> <span class="n">num_elements</span><span class="p">,</span> 
                   <span class="n">ef_construction</span> <span class="o">=</span> <span class="n">trial</span><span class="o">.</span><span class="n">suggest_int</span><span class="p">(</span><span class="s2">&quot;ef_construction&quot;</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">100</span><span class="p">),</span> 
                   <span class="n">M</span> <span class="o">=</span> <span class="n">trial</span><span class="o">.</span><span class="n">suggest_int</span><span class="p">(</span><span class="s2">&quot;M&quot;</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">50</span><span class="p">)</span>
                  <span class="p">)</span>
    <span class="c1"># Element insertion </span>
    <span class="n">ann</span><span class="o">.</span><span class="n">add_items</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">num_elements</span><span class="p">))</span>
    <span class="n">index_size</span> <span class="o">=</span> <span class="n">get_memory_usage</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">getpid</span><span class="p">())</span> <span class="o">-</span> <span class="n">memory_usage_before</span>

    <span class="c1"># control the recall (ef should always be &gt;= N_NEIGHBORS)</span>
    <span class="n">ann</span><span class="o">.</span><span class="n">set_ef</span><span class="p">(</span><span class="n">trial</span><span class="o">.</span><span class="n">suggest_int</span><span class="p">(</span><span class="s2">&quot;ef&quot;</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span> 

    <span class="c1"># run queries for each example in the test set</span>
    <span class="n">t1</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
    <span class="c1"># Query dataset, k - number of closest elements (returns 2 numpy arrays)</span>
    <span class="n">neighbors</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">ann</span><span class="o">.</span><span class="n">knn_query</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">k</span> <span class="o">=</span> <span class="n">N_NEIGHBORS</span><span class="p">)</span>
    <span class="n">query_time</span> <span class="o">=</span> <span class="p">(</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">t1</span><span class="p">)</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

    <span class="n">recall</span> <span class="o">=</span> <span class="n">compute_recall</span><span class="p">(</span><span class="n">neighbors</span><span class="p">,</span> <span class="n">X_test_neighbors</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">recall</span><span class="p">,</span> <span class="n">query_time</span><span class="p">,</span> <span class="n">index_size</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># NOTE: this cell takes about an hour to run</span>
<span class="n">motpe_sampler</span> <span class="o">=</span> <span class="n">optuna</span><span class="o">.</span><span class="n">samplers</span><span class="o">.</span><span class="n">MOTPESampler</span><span class="p">()</span>
<span class="n">motpe_experiment</span> <span class="o">=</span> <span class="n">optuna</span><span class="o">.</span><span class="n">create_study</span><span class="p">(</span><span class="n">sampler</span><span class="o">=</span><span class="n">motpe_sampler</span><span class="p">,</span>
                                       <span class="n">directions</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;maximize&quot;</span><span class="p">,</span> <span class="s2">&quot;minimize&quot;</span><span class="p">,</span> <span class="s2">&quot;minimize&quot;</span><span class="p">])</span>
<span class="n">motpe_experiment</span><span class="o">.</span><span class="n">optimize</span><span class="p">(</span><span class="n">train_hnsw</span><span class="p">,</span> <span class="n">n_trials</span><span class="o">=</span><span class="n">N_TRIALS</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;ipython-input-13-1db9ede7d8eb&gt;:1: ExperimentalWarning: MOTPESampler is experimental (supported from v2.4.0). The interface can change in the future.
  motpe_sampler = optuna.samplers.MOTPESampler()
<span class=" -Color -Color-Green">[I 2021-06-24 19:03:32,672]</span> A new study created in memory with name: no-name-95a55001-4570-4d8f-9543-c0a88604b11a
<span class=" -Color -Color-Green">[I 2021-06-24 19:03:43,053]</span> Trial 0 finished with values: [0.26542999999999495, 8.561468124389648e-06, 674.1875] and parameters: {&#39;ef_construction&#39;: 51, &#39;M&#39;: 2, &#39;ef&#39;: 71}. 
<span class=" -Color -Color-Green">[I 2021-06-24 19:06:18,716]</span> Trial 1 finished with values: [0.991349999999999, 7.11815357208252e-05, 1399.38671875] and parameters: {&#39;ef_construction&#39;: 32, &#39;M&#39;: 96, &#39;ef&#39;: 33}. 
<span class=" -Color -Color-Green">[I 2021-06-24 19:06:45,629]</span> Trial 2 finished with values: [0.784400000000004, 1.891188621520996e-05, 622.87890625] and parameters: {&#39;ef_construction&#39;: 39, &#39;M&#39;: 10, &#39;ef&#39;: 33}. 
<span class=" -Color -Color-Green">[I 2021-06-24 19:08:59,683]</span> Trial 3 finished with values: [0.9920599999999989, 7.491040229797363e-05, 1309.55078125] and parameters: {&#39;ef_construction&#39;: 79, &#39;M&#39;: 86, &#39;ef&#39;: 38}. 
<span class=" -Color -Color-Green">[I 2021-06-24 19:10:08,138]</span> Trial 4 finished with values: [0.9696199999999997, 5.083022117614746e-05, 909.734375] and parameters: {&#39;ef_construction&#39;: 57, &#39;M&#39;: 42, &#39;ef&#39;: 42}. 
<span class=" -Color -Color-Green">[I 2021-06-24 19:11:23,925]</span> Trial 5 finished with values: [0.98393, 6.899902820587158e-05, 1053.5859375] and parameters: {&#39;ef_construction&#39;: 10, &#39;M&#39;: 58, &#39;ef&#39;: 49}. 
<span class=" -Color -Color-Green">[I 2021-06-24 19:12:14,173]</span> Trial 6 finished with values: [0.9458000000000004, 4.4464731216430667e-05, 632.8125] and parameters: {&#39;ef_construction&#39;: 69, &#39;M&#39;: 11, &#39;ef&#39;: 76}. 
<span class=" -Color -Color-Green">[I 2021-06-24 19:13:48,047]</span> Trial 7 finished with values: [0.9961099999999997, 8.779120445251465e-05, 902.34375] and parameters: {&#39;ef_construction&#39;: 73, &#39;M&#39;: 41, &#39;ef&#39;: 75}. 
<span class=" -Color -Color-Green">[I 2021-06-24 19:14:37,028]</span> Trial 8 finished with values: [0.9817699999999995, 7.159368991851807e-05, 899.18359375] and parameters: {&#39;ef_construction&#39;: 29, &#39;M&#39;: 41, &#39;ef&#39;: 77}. 
<span class=" -Color -Color-Green">[I 2021-06-24 19:15:17,245]</span> Trial 9 finished with values: [0.8106700000000042, 1.9858622550964354e-05, 600.875] and parameters: {&#39;ef_construction&#39;: 72, &#39;M&#39;: 8, &#39;ef&#39;: 43}. 
<span class=" -Color -Color-Green">[I 2021-06-24 19:17:04,467]</span> Trial 10 finished with values: [0.8870600000000007, 2.0579004287719726e-05, 760.6328125] and parameters: {&#39;ef_construction&#39;: 100, &#39;M&#39;: 25, &#39;ef&#39;: 13}. 
<span class=" -Color -Color-Green">[I 2021-06-24 19:19:34,461]</span> Trial 11 finished with values: [0.9361299999999999, 3.240420818328857e-05, 1113.859375] and parameters: {&#39;ef_construction&#39;: 98, &#39;M&#39;: 65, &#39;ef&#39;: 13}. 
<span class=" -Color -Color-Green">[I 2021-06-24 19:21:04,091]</span> Trial 12 finished with values: [0.9972699999999997, 8.905429840087891e-05, 751.75390625] and parameters: {&#39;ef_construction&#39;: 87, &#39;M&#39;: 25, &#39;ef&#39;: 91}. 
<span class=" -Color -Color-Green">[I 2021-06-24 19:22:57,720]</span> Trial 13 finished with values: [0.9957799999999998, 9.269151687622071e-05, 1077.234375] and parameters: {&#39;ef_construction&#39;: 58, &#39;M&#39;: 74, &#39;ef&#39;: 61}. 
<span class=" -Color -Color-Green">[I 2021-06-24 19:23:21,257]</span> Trial 14 finished with values: [0.9499800000000002, 5.801699161529541e-05, 759.20703125] and parameters: {&#39;ef_construction&#39;: 11, &#39;M&#39;: 24, &#39;ef&#39;: 98}. 
<span class=" -Color -Color-Green">[I 2021-06-24 19:24:03,114]</span> Trial 15 finished with values: [0.674530000000004, 1.0708022117614746e-05, 590.74609375] and parameters: {&#39;ef_construction&#39;: 90, &#39;M&#39;: 7, &#39;ef&#39;: 23}. 
<span class=" -Color -Color-Green">[I 2021-06-24 19:25:07,843]</span> Trial 16 finished with values: [0.974189999999999, 6.696519851684571e-05, 705.09765625] and parameters: {&#39;ef_construction&#39;: 67, &#39;M&#39;: 20, &#39;ef&#39;: 57}. 
<span class=" -Color -Color-Green">[I 2021-06-24 19:26:32,009]</span> Trial 17 finished with values: [0.93601, 3.7243080139160155e-05, 998.546875] and parameters: {&#39;ef_construction&#39;: 45, &#39;M&#39;: 53, &#39;ef&#39;: 24}. 
<span class=" -Color -Color-Green">[I 2021-06-24 19:28:36,717]</span> Trial 18 finished with values: [0.9979299999999998, 0.00010821943283081054, 1211.60546875] and parameters: {&#39;ef_construction&#39;: 20, &#39;M&#39;: 77, &#39;ef&#39;: 65}. 
<span class=" -Color -Color-Green">[I 2021-06-24 19:30:23,788]</span> Trial 19 finished with values: [0.9888799999999998, 6.816809177398682e-05, 846.74609375] and parameters: {&#39;ef_construction&#39;: 82, &#39;M&#39;: 36, &#39;ef&#39;: 48}. 
<span class=" -Color -Color-Green">[I 2021-06-24 19:31:22,360]</span> Trial 20 finished with values: [0.9815699999999997, 5.994198322296142e-05, 656.48046875] and parameters: {&#39;ef_construction&#39;: 68, &#39;M&#39;: 15, &#39;ef&#39;: 83}. 
<span class=" -Color -Color-Green">[I 2021-06-24 19:31:49,498]</span> Trial 21 finished with values: [0.3025199999999958, 4.8305749893188475e-06, 555.96875] and parameters: {&#39;ef_construction&#39;: 94, &#39;M&#39;: 3, &#39;ef&#39;: 22}. 
<span class=" -Color -Color-Green">[I 2021-06-24 19:33:08,601]</span> Trial 22 finished with values: [0.99571, 0.00011474599838256836, 817.26171875] and parameters: {&#39;ef_construction&#39;: 64, &#39;M&#39;: 33, &#39;ef&#39;: 87}. 
<span class=" -Color -Color-Green">[I 2021-06-24 19:34:22,053]</span> Trial 23 finished with values: [0.9444899999999993, 3.7832713127136234e-05, 644.46484375] and parameters: {&#39;ef_construction&#39;: 74, &#39;M&#39;: 14, &#39;ef&#39;: 51}. 
<span class=" -Color -Color-Green">[I 2021-06-24 19:35:31,800]</span> Trial 24 finished with values: [0.96041, 4.2540788650512696e-05, 662.98828125] and parameters: {&#39;ef_construction&#39;: 76, &#39;M&#39;: 16, &#39;ef&#39;: 49}. 
<span class=" -Color -Color-Green">[I 2021-06-24 19:35:46,371]</span> Trial 25 finished with values: [0.2832999999999954, 9.058833122253418e-06, 546.078125] and parameters: {&#39;ef_construction&#39;: 82, &#39;M&#39;: 2, &#39;ef&#39;: 65}. 
<span class=" -Color -Color-Green">[I 2021-06-24 19:36:49,873]</span> Trial 26 finished with values: [0.9546300000000004, 4.5300817489624026e-05, 797.609375] and parameters: {&#39;ef_construction&#39;: 49, &#39;M&#39;: 31, &#39;ef&#39;: 43}. 
<span class=" -Color -Color-Green">[I 2021-06-24 19:37:56,812]</span> Trial 27 finished with values: [0.9163800000000026, 4.813470840454102e-05, 677.3359375] and parameters: {&#39;ef_construction&#39;: 63, &#39;M&#39;: 18, &#39;ef&#39;: 31}. 
<span class=" -Color -Color-Green">[I 2021-06-24 19:38:40,450]</span> Trial 28 finished with values: [0.7715000000000035, 3.0777311325073245e-05, 570.08984375] and parameters: {&#39;ef_construction&#39;: 74, &#39;M&#39;: 6, &#39;ef&#39;: 56}. 
<span class=" -Color -Color-Green">[I 2021-06-24 19:40:55,689]</span> Trial 29 finished with values: [0.9985599999999999, 0.00015600650310516358, 939.97265625] and parameters: {&#39;ef_construction&#39;: 88, &#39;M&#39;: 47, &#39;ef&#39;: 98}. 
<span class=" -Color -Color-Green">[I 2021-06-24 19:41:45,479]</span> Trial 30 finished with values: [0.7457400000000024, 1.3268184661865234e-05, 597.5234375] and parameters: {&#39;ef_construction&#39;: 89, &#39;M&#39;: 9, &#39;ef&#39;: 21}. 
<span class=" -Color -Color-Green">[I 2021-06-24 19:42:53,529]</span> Trial 31 finished with values: [0.8227500000000011, 1.66964054107666e-05, 632.84375] and parameters: {&#39;ef_construction&#39;: 95, &#39;M&#39;: 13, &#39;ef&#39;: 18}. 
<span class=" -Color -Color-Green">[I 2021-06-24 19:44:15,145]</span> Trial 32 finished with values: [0.9401800000000001, 3.325850963592529e-05, 695.2578125] and parameters: {&#39;ef_construction&#39;: 84, &#39;M&#39;: 20, &#39;ef&#39;: 29}. 
<span class=" -Color -Color-Green">[I 2021-06-24 19:46:02,819]</span> Trial 33 finished with values: [0.9976299999999998, 0.00010521950721740723, 785.35546875] and parameters: {&#39;ef_construction&#39;: 92, &#39;M&#39;: 30, &#39;ef&#39;: 83}. 
<span class=" -Color -Color-Green">[I 2021-06-24 19:47:02,022]</span> Trial 34 finished with values: [0.7081400000000023, 1.0004782676696777e-05, 631.9921875] and parameters: {&#39;ef_construction&#39;: 81, &#39;M&#39;: 13, &#39;ef&#39;: 10}. 
<span class=" -Color -Color-Green">[I 2021-06-24 19:48:09,114]</span> Trial 35 finished with values: [0.9786800000000001, 6.086788177490234e-05, 758.453125] and parameters: {&#39;ef_construction&#39;: 62, &#39;M&#39;: 27, &#39;ef&#39;: 53}. 
<span class=" -Color -Color-Green">[I 2021-06-24 19:48:22,975]</span> Trial 36 finished with values: [0.1972299999999982, 5.773591995239258e-06, 540.2578125] and parameters: {&#39;ef_construction&#39;: 77, &#39;M&#39;: 2, &#39;ef&#39;: 38}. 
<span class=" -Color -Color-Green">[I 2021-06-24 19:49:46,915]</span> Trial 37 finished with values: [0.8625700000000026, 2.913990020751953e-05, 664.20703125] and parameters: {&#39;ef_construction&#39;: 100, &#39;M&#39;: 16, &#39;ef&#39;: 17}. 
<span class=" -Color -Color-Green">[I 2021-06-24 19:51:12,314]</span> Trial 38 finished with values: [0.9406600000000012, 3.159458637237549e-05, 704.14453125] and parameters: {&#39;ef_construction&#39;: 86, &#39;M&#39;: 21, &#39;ef&#39;: 27}. 
<span class=" -Color -Color-Green">[I 2021-06-24 19:52:37,768]</span> Trial 39 finished with values: [0.9921099999999993, 7.47386932373047e-05, 839.58984375] and parameters: {&#39;ef_construction&#39;: 68, &#39;M&#39;: 36, &#39;ef&#39;: 65}. 
<span class=" -Color -Color-Green">[I 2021-06-24 19:53:13,385]</span> Trial 40 finished with values: [0.8203800000000033, 2.3042798042297363e-05, 578.09375] and parameters: {&#39;ef_construction&#39;: 72, &#39;M&#39;: 7, &#39;ef&#39;: 56}. 
<span class=" -Color -Color-Green">[I 2021-06-24 19:53:44,266]</span> Trial 41 finished with values: [0.8121000000000036, 2.2452425956726076e-05, 568.875] and parameters: {&#39;ef_construction&#39;: 70, &#39;M&#39;: 6, &#39;ef&#39;: 71}. 
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span><span class=" -Color -Color-Green">[I 2021-06-24 20:06:49,033]</span> Trial 42 finished with values: [0.9911199999999997, 7.964971065521241e-05, 738.58984375] and parameters: {&#39;ef_construction&#39;: 53, &#39;M&#39;: 25, &#39;ef&#39;: 90}. 
<span class=" -Color -Color-Green">[I 2021-06-24 20:08:01,295]</span> Trial 43 finished with values: [0.9795499999999984, 5.242159366607666e-05, 658.45703125] and parameters: {&#39;ef_construction&#39;: 78, &#39;M&#39;: 16, &#39;ef&#39;: 71}. 
<span class=" -Color -Color-Green">[I 2021-06-24 20:09:23,540]</span> Trial 44 finished with values: [0.9883000000000002, 7.203876972198486e-05, 704.921875] and parameters: {&#39;ef_construction&#39;: 79, &#39;M&#39;: 21, &#39;ef&#39;: 68}. 
<span class=" -Color -Color-Green">[I 2021-06-24 20:12:36,768]</span> Trial 45 finished with values: [0.9993, 0.00015685410499572755, 1380.41796875] and parameters: {&#39;ef_construction&#39;: 62, &#39;M&#39;: 96, &#39;ef&#39;: 82}. 
<span class=" -Color -Color-Green">[I 2021-06-24 20:13:03,193]</span> Trial 46 finished with values: [0.7359600000000042, 2.223978042602539e-05, 560.40234375] and parameters: {&#39;ef_construction&#39;: 57, &#39;M&#39;: 5, &#39;ef&#39;: 71}. 
<span class=" -Color -Color-Green">[I 2021-06-24 20:13:58,171]</span> Trial 47 finished with values: [0.9367900000000003, 4.296810626983643e-05, 623.61328125] and parameters: {&#39;ef_construction&#39;: 66, &#39;M&#39;: 12, &#39;ef&#39;: 60}. 
<span class=" -Color -Color-Green">[I 2021-06-24 20:14:32,596]</span> Trial 48 finished with values: [0.9203600000000007, 3.722789287567139e-05, 604.9765625] and parameters: {&#39;ef_construction&#39;: 46, &#39;M&#39;: 10, &#39;ef&#39;: 80}. 
<span class=" -Color -Color-Green">[I 2021-06-24 20:15:00,791]</span> Trial 49 finished with values: [0.8694400000000009, 6.25748872756958e-05, 578.73046875] and parameters: {&#39;ef_construction&#39;: 45, &#39;M&#39;: 7, &#39;ef&#39;: 94}. 
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Note: this cell can take 1-2 hours to run</span>
<span class="n">random_sampler</span> <span class="o">=</span> <span class="n">optuna</span><span class="o">.</span><span class="n">samplers</span><span class="o">.</span><span class="n">RandomSampler</span><span class="p">()</span>
<span class="n">random_experiment</span> <span class="o">=</span> <span class="n">optuna</span><span class="o">.</span><span class="n">create_study</span><span class="p">(</span><span class="n">sampler</span><span class="o">=</span><span class="n">random_sampler</span><span class="p">,</span> <span class="n">directions</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;maximize&quot;</span><span class="p">,</span> <span class="s2">&quot;minimize&quot;</span><span class="p">,</span> <span class="s2">&quot;minimize&quot;</span><span class="p">])</span>
<span class="n">random_experiment</span><span class="o">.</span><span class="n">optimize</span><span class="p">(</span><span class="n">train_hnsw</span><span class="p">,</span> <span class="n">n_trials</span><span class="o">=</span><span class="n">N_TRIALS</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span><span class=" -Color -Color-Green">[I 2021-06-24 20:25:37,665]</span> A new study created in memory with name: no-name-a68eff9b-eb2b-4f18-bfc7-d4165162ea33
<span class=" -Color -Color-Green">[I 2021-06-24 20:28:06,180]</span> Trial 0 finished with values: [0.983939999999999, 5.984342098236084e-05, 1276.7578125] and parameters: {&#39;ef_construction&#39;: 84, &#39;M&#39;: 85, &#39;ef&#39;: 29}. 
<span class=" -Color -Color-Green">[I 2021-06-24 20:28:44,845]</span> Trial 1 finished with values: [0.979860000000001, 7.119548320770264e-05, 804.00390625] and parameters: {&#39;ef_construction&#39;: 36, &#39;M&#39;: 32, &#39;ef&#39;: 89}. 
<span class=" -Color -Color-Green">[I 2021-06-24 20:30:52,545]</span> Trial 2 finished with values: [0.9822999999999993, 7.502460479736328e-05, 1203.10546875] and parameters: {&#39;ef_construction&#39;: 37, &#39;M&#39;: 77, &#39;ef&#39;: 32}. 
<span class=" -Color -Color-Green">[I 2021-06-24 20:32:44,439]</span> Trial 3 finished with values: [0.9970999999999997, 9.25718069076538e-05, 884.734375] and parameters: {&#39;ef_construction&#39;: 81, &#39;M&#39;: 41, &#39;ef&#39;: 77}. 
<span class=" -Color -Color-Green">[I 2021-06-24 20:32:59,959]</span> Trial 4 finished with values: [0.7970900000000032, 2.7109718322753907e-05, 668.33984375] and parameters: {&#39;ef_construction&#39;: 15, &#39;M&#39;: 17, &#39;ef&#39;: 47}. 
<span class=" -Color -Color-Green">[I 2021-06-24 20:33:32,772]</span> Trial 5 finished with values: [0.96219, 6.515152454376221e-05, 776.95703125] and parameters: {&#39;ef_construction&#39;: 20, &#39;M&#39;: 29, &#39;ef&#39;: 85}. 
<span class=" -Color -Color-Green">[I 2021-06-24 20:36:21,116]</span> Trial 6 finished with values: [0.9994199999999999, 0.00012017207145690918, 1345.3828125] and parameters: {&#39;ef_construction&#39;: 44, &#39;M&#39;: 92, &#39;ef&#39;: 77}. 
<span class=" -Color -Color-Green">[I 2021-06-24 20:36:27,727]</span> Trial 7 finished with values: [0.35915999999999176, 1.0370612144470215e-05, 545.8046875] and parameters: {&#39;ef_construction&#39;: 21, &#39;M&#39;: 3, &#39;ef&#39;: 64}. 
<span class=" -Color -Color-Green">[I 2021-06-24 20:38:29,016]</span> Trial 8 finished with values: [0.9847699999999991, 5.770750045776367e-05, 890.859375] and parameters: {&#39;ef_construction&#39;: 91, &#39;M&#39;: 41, &#39;ef&#39;: 35}. 
<span class=" -Color -Color-Green">[I 2021-06-24 20:40:58,895]</span> Trial 9 finished with values: [0.9894599999999989, 7.027678489685059e-05, 1239.1875] and parameters: {&#39;ef_construction&#39;: 28, &#39;M&#39;: 85, &#39;ef&#39;: 35}. 
<span class=" -Color -Color-Green">[I 2021-06-24 20:41:44,598]</span> Trial 10 finished with values: [0.5599300000000004, 1.3425469398498535e-05, 587.28515625] and parameters: {&#39;ef_construction&#39;: 85, &#39;M&#39;: 8, &#39;ef&#39;: 10}. 
<span class=" -Color -Color-Green">[I 2021-06-24 20:42:14,849]</span> Trial 11 finished with values: [0.766440000000003, 2.0627212524414063e-05, 560.00390625] and parameters: {&#39;ef_construction&#39;: 79, &#39;M&#39;: 5, &#39;ef&#39;: 75}. 
<span class=" -Color -Color-Green">[I 2021-06-24 20:45:07,329]</span> Trial 12 finished with values: [0.9947599999999996, 7.598700523376465e-05, 1381.48046875] and parameters: {&#39;ef_construction&#39;: 25, &#39;M&#39;: 96, &#39;ef&#39;: 39}. 
<span class=" -Color -Color-Green">[I 2021-06-24 20:47:02,777]</span> Trial 13 finished with values: [0.9969599999999995, 0.00010768730640411377, 839.671875] and parameters: {&#39;ef_construction&#39;: 84, &#39;M&#39;: 36, &#39;ef&#39;: 80}. 
<span class=" -Color -Color-Green">[I 2021-06-24 20:49:57,728]</span> Trial 14 finished with values: [0.9990499999999998, 0.00014294648170471192, 1372.265625] and parameters: {&#39;ef_construction&#39;: 45, &#39;M&#39;: 95, &#39;ef&#39;: 67}. 
<span class=" -Color -Color-Green">[I 2021-06-24 20:50:57,472]</span> Trial 15 finished with values: [0.9916900000000003, 9.124739170074462e-05, 921.234375] and parameters: {&#39;ef_construction&#39;: 17, &#39;M&#39;: 45, &#39;ef&#39;: 90}. 
<span class=" -Color -Color-Green">[I 2021-06-24 20:52:30,273]</span> Trial 16 finished with values: [0.9467300000000006, 3.9931797981262205e-05, 1020.1015625] and parameters: {&#39;ef_construction&#39;: 62, &#39;M&#39;: 56, &#39;ef&#39;: 25}. 
<span class=" -Color -Color-Green">[I 2021-06-24 20:54:44,200]</span> Trial 17 finished with values: [0.9365899999999996, 3.886368274688721e-05, 1213.7109375] and parameters: {&#39;ef_construction&#39;: 37, &#39;M&#39;: 78, &#39;ef&#39;: 16}. 
<span class=" -Color -Color-Green">[I 2021-06-24 20:56:03,613]</span> Trial 18 finished with values: [0.9922900000000001, 0.00010780291557312012, 1029.96875] and parameters: {&#39;ef_construction&#39;: 33, &#39;M&#39;: 57, &#39;ef&#39;: 72}. 
<span class=" -Color -Color-Green">[I 2021-06-24 20:56:50,420]</span> Trial 19 finished with values: [0.9814399999999986, 7.200939655303955e-05, 849.078125] and parameters: {&#39;ef_construction&#39;: 26, &#39;M&#39;: 37, &#39;ef&#39;: 86}. 
<span class=" -Color -Color-Green">[I 2021-06-24 20:58:29,528]</span> Trial 20 finished with values: [0.99735, 0.00010547170639038086, 1011.078125] and parameters: {&#39;ef_construction&#39;: 71, &#39;M&#39;: 55, &#39;ef&#39;: 76}. 
<span class=" -Color -Color-Green">[I 2021-06-24 21:00:12,724]</span> Trial 21 finished with values: [0.9983299999999999, 0.00011518931388854981, 1137.484375] and parameters: {&#39;ef_construction&#39;: 17, &#39;M&#39;: 69, &#39;ef&#39;: 90}. 
<span class=" -Color -Color-Green">[I 2021-06-24 21:03:08,907]</span> Trial 22 finished with values: [0.9981299999999997, 0.00012965700626373292, 1372.80078125] and parameters: {&#39;ef_construction&#39;: 50, &#39;M&#39;: 95, &#39;ef&#39;: 58}. 
<span class=" -Color -Color-Green">[I 2021-06-24 21:06:04,751]</span> Trial 23 finished with values: [0.9997099999999999, 0.000155607008934021, 1381.28125] and parameters: {&#39;ef_construction&#39;: 86, &#39;M&#39;: 96, &#39;ef&#39;: 97}. 
<span class=" -Color -Color-Green">[I 2021-06-24 21:06:47,276]</span> Trial 24 finished with values: [0.9844999999999984, 7.136518955230712e-05, 812.45703125] and parameters: {&#39;ef_construction&#39;: 39, &#39;M&#39;: 33, &#39;ef&#39;: 87}. 
<span class=" -Color -Color-Green">[I 2021-06-24 21:08:02,283]</span> Trial 25 finished with values: [0.8973400000000004, 2.432389259338379e-05, 785.3359375] and parameters: {&#39;ef_construction&#39;: 69, &#39;M&#39;: 30, &#39;ef&#39;: 16}. 
<span class=" -Color -Color-Green">[I 2021-06-24 21:09:55,942]</span> Trial 26 finished with values: [0.99724, 8.974881172180176e-05, 803.5] and parameters: {&#39;ef_construction&#39;: 95, &#39;M&#39;: 32, &#39;ef&#39;: 64}. 
<span class=" -Color -Color-Green">[I 2021-06-24 21:41:48,518]</span> Trial 27 finished with values: [0.9955099999999997, 0.00010829892158508301, 1323.47265625] and parameters: {&#39;ef_construction&#39;: 54, &#39;M&#39;: 90, &#39;ef&#39;: 44}. 
<span class=" -Color -Color-Green">[I 2021-06-24 21:43:12,498]</span> Trial 28 finished with values: [0.9964599999999997, 0.00010232760906219483, 1019.7734375] and parameters: {&#39;ef_construction&#39;: 45, &#39;M&#39;: 56, &#39;ef&#39;: 100}. 
<span class=" -Color -Color-Green">[I 2021-06-24 21:45:07,259]</span> Trial 29 finished with values: [0.9922000000000002, 8.648409843444824e-05, 1192.33203125] and parameters: {&#39;ef_construction&#39;: 35, &#39;M&#39;: 75, &#39;ef&#39;: 51}. 
<span class=" -Color -Color-Green">[I 2021-06-24 21:47:06,937]</span> Trial 30 finished with values: [0.9993, 0.0001427665948867798, 1219.0859375] and parameters: {&#39;ef_construction&#39;: 78, &#39;M&#39;: 78, &#39;ef&#39;: 94}. 
<span class=" -Color -Color-Green">[I 2021-06-24 21:47:14,841]</span> Trial 31 finished with values: [0.3878799999999919, 9.720611572265625e-06, 546.96484375] and parameters: {&#39;ef_construction&#39;: 27, &#39;M&#39;: 3, &#39;ef&#39;: 60}. 
<span class=" -Color -Color-Green">[I 2021-06-24 21:48:41,248]</span> Trial 32 finished with values: [0.9938899999999999, 8.57154130935669e-05, 1051.703125] and parameters: {&#39;ef_construction&#39;: 19, &#39;M&#39;: 59, &#39;ef&#39;: 69}. 
<span class=" -Color -Color-Green">[I 2021-06-24 21:49:40,376]</span> Trial 33 finished with values: [0.9426799999999985, 4.706478118896484e-05, 929.80859375] and parameters: {&#39;ef_construction&#39;: 28, &#39;M&#39;: 46, &#39;ef&#39;: 35}. 
<span class=" -Color -Color-Green">[I 2021-06-24 21:52:46,734]</span> Trial 34 finished with values: [0.9902999999999994, 6.719732284545899e-05, 1363.19140625] and parameters: {&#39;ef_construction&#39;: 31, &#39;M&#39;: 94, &#39;ef&#39;: 31}. 
<span class=" -Color -Color-Green">[I 2021-06-24 21:53:44,678]</span> Trial 35 finished with values: [0.99135, 7.035219669342041e-05, 713.5546875] and parameters: {&#39;ef_construction&#39;: 60, &#39;M&#39;: 22, &#39;ef&#39;: 86}. 
<span class=" -Color -Color-Green">[I 2021-06-24 21:54:02,192]</span> Trial 36 finished with values: [0.8536300000000027, 3.54902982711792e-05, 631.8359375] and parameters: {&#39;ef_construction&#39;: 19, &#39;M&#39;: 13, &#39;ef&#39;: 73}. 
<span class=" -Color -Color-Green">[I 2021-06-24 21:54:39,540]</span> Trial 37 finished with values: [0.8390100000000023, 1.992931365966797e-05, 695.04296875] and parameters: {&#39;ef_construction&#39;: 41, &#39;M&#39;: 20, &#39;ef&#39;: 21}. 
<span class=" -Color -Color-Green">[I 2021-06-24 21:55:13,560]</span> Trial 38 finished with values: [0.9566199999999996, 5.2713918685913084e-05, 803.39453125] and parameters: {&#39;ef_construction&#39;: 32, &#39;M&#39;: 32, &#39;ef&#39;: 66}. 
<span class=" -Color -Color-Green">[I 2021-06-24 21:57:08,907]</span> Trial 39 finished with values: [0.9977999999999998, 0.00010398809909820556, 1209.72265625] and parameters: {&#39;ef_construction&#39;: 59, &#39;M&#39;: 77, &#39;ef&#39;: 63}. 
<span class=" -Color -Color-Green">[I 2021-06-24 21:58:56,433]</span> Trial 40 finished with values: [0.9887400000000002, 6.381468772888184e-05, 875.62890625] and parameters: {&#39;ef_construction&#39;: 86, &#39;M&#39;: 40, &#39;ef&#39;: 45}. 
<span class=" -Color -Color-Green">[I 2021-06-24 22:01:16,824]</span> Trial 41 finished with values: [0.9982000000000002, 0.00011196508407592773, 1300.015625] and parameters: {&#39;ef_construction&#39;: 18, &#39;M&#39;: 87, &#39;ef&#39;: 64}. 
<span class=" -Color -Color-Green">[I 2021-06-24 22:01:46,670]</span> Trial 42 finished with values: [0.9379699999999987, 4.261813163757324e-05, 649.89453125] and parameters: {&#39;ef_construction&#39;: 36, &#39;M&#39;: 15, &#39;ef&#39;: 71}. 
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span><span class=" -Color -Color-Green">[I 2021-06-24 22:02:40,147]</span> Trial 43 finished with values: [0.8827200000000018, 2.7303791046142577e-05, 902.71875] and parameters: {&#39;ef_construction&#39;: 25, &#39;M&#39;: 43, &#39;ef&#39;: 20}. 
<span class=" -Color -Color-Green">[I 2021-06-24 22:04:13,108]</span> Trial 44 finished with values: [0.9580099999999994, 4.920849800109863e-05, 1110.40234375] and parameters: {&#39;ef_construction&#39;: 64, &#39;M&#39;: 66, &#39;ef&#39;: 28}. 
<span class=" -Color -Color-Green">[I 2021-06-24 22:06:12,731]</span> Trial 45 finished with values: [0.9991300000000001, 0.0001390671968460083, 1218.75] and parameters: {&#39;ef_construction&#39;: 55, &#39;M&#39;: 78, &#39;ef&#39;: 92}. 
<span class=" -Color -Color-Green">[I 2021-06-24 22:07:12,461]</span> Trial 46 finished with values: [0.9388200000000008, 3.362522125244141e-05, 631.83984375] and parameters: {&#39;ef_construction&#39;: 84, &#39;M&#39;: 13, &#39;ef&#39;: 50}. 
<span class=" -Color -Color-Green">[I 2021-06-24 22:07:54,610]</span> Trial 47 finished with values: [0.795320000000003, 1.5109610557556152e-05, 695.0390625] and parameters: {&#39;ef_construction&#39;: 46, &#39;M&#39;: 20, &#39;ef&#39;: 14}. 
<span class=" -Color -Color-Green">[I 2021-06-24 22:10:18,765]</span> Trial 48 finished with values: [0.9802700000000001, 5.933408737182617e-05, 1327.11328125] and parameters: {&#39;ef_construction&#39;: 71, &#39;M&#39;: 90, &#39;ef&#39;: 26}. 
<span class=" -Color -Color-Green">[I 2021-06-24 22:10:58,440]</span> Trial 49 finished with values: [0.9368900000000001, 3.7510275840759276e-05, 631.8359375] and parameters: {&#39;ef_construction&#39;: 53, &#39;M&#39;: 13, &#39;ef&#39;: 61}. 
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">hnsw_motpe_df</span> <span class="o">=</span> <span class="n">cast_as_df</span><span class="p">(</span><span class="n">motpe_experiment</span><span class="p">,</span> <span class="n">experiment_type</span><span class="o">=</span><span class="s2">&quot;MOTPE&quot;</span><span class="p">,</span> <span class="n">algorithm_name</span><span class="o">=</span><span class="s2">&quot;HNSW&quot;</span><span class="p">)</span>
<span class="n">hnsw_random_df</span> <span class="o">=</span> <span class="n">cast_as_df</span><span class="p">(</span><span class="n">random_experiment</span><span class="p">,</span> <span class="n">experiment_type</span><span class="o">=</span><span class="s2">&quot;Random&quot;</span><span class="p">,</span> <span class="n">algorithm_name</span><span class="o">=</span><span class="s2">&quot;HNSW&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">three_panel_fig</span><span class="p">([</span><span class="n">hnsw_motpe_df</span><span class="p">,</span> <span class="n">hnsw_random_df</span><span class="p">])</span><span class="c1">#</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/T892775_Exploring_Multi_Objective_Hyperparameter_Optimization_177_0.png" src="_images/T892775_Exploring_Multi_Objective_Hyperparameter_Optimization_177_0.png" />
</div>
</div>
</div>
<div class="section" id="id11">
<h3>Discussion time<a class="headerlink" href="#id11" title="Permalink to this headline">¶</a></h3>
<p>Results are essentially the same with this algorithm. Even though we have more hyperparameters (3 for HNSW vs only 2 for Annoy), random sampling is able to effectively cover similar regions of the objective space. Again, we ran this experiment for 100 trials (which took a looong time).</p>
<p><center><img src='https://github.com/sparsh-ai/multiobjective-optimizations/blob/main/docs/_images/T892775_11.png?raw=1'></center></p><p>One intriguing difference is in the middle figure where the MOTPE algorithm identifies a few models that seem to dominate the memory space (having much lower memory than all other models) but this is likely a fluke because the way we  measure memory usage of the algorithm is not exact.</p>
</div>
<div class="section" id="locality-sensitive-hashing">
<h3>Locality-sensitive Hashing<a class="headerlink" href="#locality-sensitive-hashing" title="Permalink to this headline">¶</a></h3>
<p>A <em>hash function</em> maps high-dimensional data to fixed-size low-dimensional representations. The outputs of hash functions are typically used as keys into <em>hash tables</em>.  In cryptography and security applications, hash functions are designed such that two datapoints (ideally) never map to the same output. But <em>locality-sensitive</em> hash functions do just the opposite – they are designed such that similar data points <em>collide</em> with high probability, that is, they land in the same <em>bucket</em>. Thus, hash functions are another method to partition our dataset, reducing the amount of computation.  LSH strives to ensure similar points fall into the same bucket, and dissimilar points land in other buckets. At query time, we apply our hash function to the incoming query, match the hash output to the corresponding bucket, and examine only those database points residing in that same bucket.</p>
<p><center><img src='https://github.com/sparsh-ai/multiobjective-optimizations/blob/main/docs/_images/T892775_12.png?raw=1'></center></p>
<p>In reality, multiple hash functions are used, each resulting in a slightly different partitioning of the space.   When the query comes in, each hash function is applied to the query point and candidates from the matching bucket from <em>each</em> hash table are retrieved.</p>
<p>A hash table can have lots of buckets or only a few buckets, determined by the number of <em>bits</em>. More bits equal more buckets. More buckets result in a finer partitioning of the space. For example, imagine we randomly mapped each point in our database to either 0 or 1 (two bits = two buckets). We’d have approximately half our database in each bucket. When performing a query, we’ll retrieve all the members of, say, bucket 0. We’ll still have to compute the distances between our query and <em>half</em> the points in our database! Therefore, more bits usually reduces the query time by thinning out the number of candidates in each bucket.</p>
</div>
<div class="section" id="id12">
<h3>Hyperparameters<a class="headerlink" href="#id12" title="Permalink to this headline">¶</a></h3>
<p>While there are literally dozens of implementations, most versions (including the version we use) have the following hyperparameters to consider:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">number_of_hash_tables</span></code>: More hash tables lead to better accuracy but increased memory footprint.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">number_of_hash_bits</span></code>: The number of hash bits determines the number of buckets per table; more buckets decreases query time but increases the memory footprint.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">number_of_probes</span></code>: At query time, we can examine multiple buckets from each hash table to find nearest neighbor candidates. More probes leads to great accuracy but the query will take longer.</p></li>
</ul>
</div>
<div class="section" id="id13">
<h3>Code it up<a class="headerlink" href="#id13" title="Permalink to this headline">¶</a></h3>
<p>We’re using the <a class="reference external" href="https://github.com/FALCONN-LIB/FALCONN">FALCONN</a> library for LSH (in case you’re wondering, FALCONN is the decidedly awkward acronym for FAst Lookups of Cosine and Other Nearest Neighbors). We found this library to be a bit finicky, and it requires a sizeable amount of prepwork before index initialization. We borrowed from <a class="reference external" href="https://github.com/FALCONN-LIB/FALCONN/blob/master/src/examples/glove/glove.py">this example</a>, where you can find more details about these additional (non-tunable) parameters.</p>
<p><strong>Special note:</strong> This algorithm is structured in such a way that the <code class="docutils literal notranslate"><span class="pre">number_of_probes</span></code> used at query time <em>must</em> be at least as large as the <code class="docutils literal notranslate"><span class="pre">number_of_tables</span></code>. This means we have one hyperparameter that is directly dependent on another! Thankfully, Optuna handles this use case with aplomb! Optuna’s enables define-by-run optimization which means that each trial is generated entirely independently from others. Instead of assigning our HPs inline as in the previous examples, we’ll handle them at the top of the function.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">train_lsh</span><span class="p">(</span><span class="n">trial</span><span class="p">):</span>
    
    <span class="c1"># Handle hyperparameter suggestions</span>
    <span class="n">num_tables</span> <span class="o">=</span> <span class="n">trial</span><span class="o">.</span><span class="n">suggest_int</span><span class="p">(</span><span class="s2">&quot;number_of_tables&quot;</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">50</span><span class="p">)</span>
    <span class="c1"># num_probes depends on num_tables -- </span>
    <span class="c1"># We can directly pass Optuna&#39;s num_tables suggestion to the num_probes generator</span>
    <span class="n">num_probes</span> <span class="o">=</span> <span class="n">trial</span><span class="o">.</span><span class="n">suggest_int</span><span class="p">(</span><span class="s2">&quot;number_of_probes&quot;</span><span class="p">,</span> <span class="n">num_tables</span><span class="p">,</span> <span class="n">num_tables</span><span class="o">*</span><span class="mi">2</span><span class="p">)</span> 
    <span class="c1"># Don&#39;t allow num_bits &gt; 28 or falconn will throw an error</span>
    <span class="n">num_bits</span> <span class="o">=</span> <span class="n">trial</span><span class="o">.</span><span class="n">suggest_int</span><span class="p">(</span><span class="s2">&quot;number_of_hash_bits&quot;</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">26</span><span class="p">)</span>
    
    <span class="c1"># LSH initialization (For more details see link to example above)</span>
    <span class="n">params</span> <span class="o">=</span> <span class="n">falconn</span><span class="o">.</span><span class="n">LSHConstructionParameters</span><span class="p">()</span>
    <span class="n">params</span><span class="o">.</span><span class="n">dimension</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">X_train</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="n">params</span><span class="o">.</span><span class="n">lsh_family</span> <span class="o">=</span> <span class="n">falconn</span><span class="o">.</span><span class="n">LSHFamily</span><span class="o">.</span><span class="n">CrossPolytope</span>
    <span class="n">params</span><span class="o">.</span><span class="n">distance_function</span> <span class="o">=</span> <span class="n">falconn</span><span class="o">.</span><span class="n">DistanceFunction</span><span class="o">.</span><span class="n">EuclideanSquared</span>
    <span class="n">params</span><span class="o">.</span><span class="n">num_rotations</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">params</span><span class="o">.</span><span class="n">seed</span> <span class="o">=</span> <span class="mi">5721840</span> 
    <span class="n">params</span><span class="o">.</span><span class="n">num_setup_threads</span> <span class="o">=</span> <span class="mi">0</span> <span class="c1"># use all available threads</span>
    <span class="n">params</span><span class="o">.</span><span class="n">storage_hash_table</span> <span class="o">=</span> <span class="n">falconn</span><span class="o">.</span><span class="n">StorageHashTable</span><span class="o">.</span><span class="n">BitPackedFlatHashTable</span>
    <span class="c1"># Set build-related hyperparameters</span>
    <span class="n">params</span><span class="o">.</span><span class="n">l</span> <span class="o">=</span> <span class="n">num_tables</span>
    <span class="n">falconn</span><span class="o">.</span><span class="n">compute_number_of_hash_functions</span><span class="p">(</span><span class="n">num_bits</span><span class="p">,</span> <span class="n">params</span><span class="p">)</span>

    <span class="c1"># LSH index construction</span>
    <span class="n">memory_usage_before</span> <span class="o">=</span> <span class="n">get_memory_usage</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">getpid</span><span class="p">())</span>
    <span class="n">table</span> <span class="o">=</span> <span class="n">falconn</span><span class="o">.</span><span class="n">LSHIndex</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
    <span class="n">table</span><span class="o">.</span><span class="n">setup</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
    <span class="n">index_size</span> <span class="o">=</span> <span class="n">get_memory_usage</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">getpid</span><span class="p">())</span> <span class="o">-</span> <span class="n">memory_usage_before</span>

    <span class="c1"># Set up the query object with num_probes</span>
    <span class="n">query_object</span> <span class="o">=</span> <span class="n">table</span><span class="o">.</span><span class="n">construct_query_object</span><span class="p">(</span><span class="n">num_probes</span><span class="o">=</span><span class="n">num_probes</span><span class="p">)</span> 
    
    <span class="c1"># Perform the queries</span>
    <span class="n">t1</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
    <span class="n">neighbors</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">X_test</span><span class="p">):</span>
        <span class="n">neighbors</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">query_object</span><span class="o">.</span><span class="n">find_k_nearest_neighbors</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">N_NEIGHBORS</span><span class="p">))</span>
    <span class="n">query_time</span> <span class="o">=</span> <span class="p">(</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">t1</span><span class="p">)</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
                         
    <span class="c1"># Evaluate</span>
    <span class="n">recall</span> <span class="o">=</span> <span class="n">compute_recall</span><span class="p">(</span><span class="n">neighbors</span><span class="p">,</span> <span class="n">X_test_neighbors</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">recall</span><span class="p">,</span> <span class="n">query_time</span><span class="p">,</span> <span class="n">index_size</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Note: this cell takes about 20 minutes to run</span>
<span class="n">motpe_sampler</span> <span class="o">=</span> <span class="n">optuna</span><span class="o">.</span><span class="n">samplers</span><span class="o">.</span><span class="n">MOTPESampler</span><span class="p">()</span>
<span class="n">motpe_experiment</span> <span class="o">=</span> <span class="n">optuna</span><span class="o">.</span><span class="n">create_study</span><span class="p">(</span><span class="n">sampler</span><span class="o">=</span><span class="n">motpe_sampler</span><span class="p">,</span>
                                       <span class="n">directions</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;maximize&quot;</span><span class="p">,</span> <span class="s2">&quot;minimize&quot;</span><span class="p">,</span> <span class="s2">&quot;minimize&quot;</span><span class="p">])</span>
<span class="n">motpe_experiment</span><span class="o">.</span><span class="n">optimize</span><span class="p">(</span><span class="n">train_lsh</span><span class="p">,</span> <span class="n">n_trials</span><span class="o">=</span><span class="n">N_TRIALS</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;ipython-input-21-236ef6d9d285&gt;:1: ExperimentalWarning: MOTPESampler is experimental (supported from v2.4.0). The interface can change in the future.
  motpe_sampler = optuna.samplers.MOTPESampler()
<span class=" -Color -Color-Green">[I 2021-06-25 09:51:36,492]</span> A new study created in memory with name: no-name-4bbb0127-2e15-433d-9c12-3219ddfe1e92
<span class=" -Color -Color-Green">[I 2021-06-25 09:52:09,918]</span> Trial 0 finished with values: [0.8797599999999849, 0.002646678829193115, 168.66796875] and parameters: {&#39;number_of_tables&#39;: 48, &#39;number_of_probes&#39;: 65, &#39;number_of_hash_bits&#39;: 15}. 
<span class=" -Color -Color-Green">[I 2021-06-25 09:52:31,149]</span> Trial 1 finished with values: [0.4578599999999973, 0.00046552860736846926, 2718.35546875] and parameters: {&#39;number_of_tables&#39;: 43, &#39;number_of_probes&#39;: 83, &#39;number_of_hash_bits&#39;: 24}. 
<span class=" -Color -Color-Green">[I 2021-06-25 09:52:41,046]</span> Trial 2 finished with values: [0.37286999999998993, 0.00021935951709747316, 742.484375] and parameters: {&#39;number_of_tables&#39;: 23, &#39;number_of_probes&#39;: 44, &#39;number_of_hash_bits&#39;: 23}. 
<span class=" -Color -Color-Green">[I 2021-06-25 09:52:56,552]</span> Trial 3 finished with values: [0.5693900000000017, 0.0004544396162033081, 125.79296875] and parameters: {&#39;number_of_tables&#39;: 40, &#39;number_of_probes&#39;: 46, &#39;number_of_hash_bits&#39;: 20}. 
<span class=" -Color -Color-Green">[I 2021-06-25 09:53:22,439]</span> Trial 4 finished with values: [0.8441099999999884, 0.002014873504638672, 94.87109375] and parameters: {&#39;number_of_tables&#39;: 30, &#39;number_of_probes&#39;: 50, &#39;number_of_hash_bits&#39;: 15}. 
<span class=" -Color -Color-Green">[I 2021-06-25 09:53:34,734]</span> Trial 5 finished with values: [0.529499999999999, 0.0003613719940185547, 374.28515625] and parameters: {&#39;number_of_tables&#39;: 31, &#39;number_of_probes&#39;: 56, &#39;number_of_hash_bits&#39;: 21}. 
<span class=" -Color -Color-Green">[I 2021-06-25 09:53:55,625]</span> Trial 6 finished with values: [0.7873699999999895, 0.0010903979063034057, 135.515625] and parameters: {&#39;number_of_tables&#39;: 37, &#39;number_of_probes&#39;: 62, &#39;number_of_hash_bits&#39;: 17}. 
<span class=" -Color -Color-Green">[I 2021-06-25 09:54:29,715]</span> Trial 7 finished with values: [0.8756799999999847, 0.0028057760953903196, 136.45703125] and parameters: {&#39;number_of_tables&#39;: 35, &#39;number_of_probes&#39;: 48, &#39;number_of_hash_bits&#39;: 14}. 
<span class=" -Color -Color-Green">[I 2021-06-25 09:54:42,316]</span> Trial 8 finished with values: [0.23708999999999172, 0.0001916936159133911, 355.74609375] and parameters: {&#39;number_of_tables&#39;: 19, &#39;number_of_probes&#39;: 35, &#39;number_of_hash_bits&#39;: 25}. 
<span class=" -Color -Color-Green">[I 2021-06-25 09:54:53,391]</span> Trial 9 finished with values: [0.5003900000000019, 0.00031712210178375246, -414.4453125] and parameters: {&#39;number_of_tables&#39;: 31, &#39;number_of_probes&#39;: 33, &#39;number_of_hash_bits&#39;: 20}. 
<span class=" -Color -Color-Green">[I 2021-06-25 09:54:59,475]</span> Trial 10 finished with values: [0.42324999999999463, 0.00021056768894195556, 66.0546875] and parameters: {&#39;number_of_tables&#39;: 13, &#39;number_of_probes&#39;: 14, &#39;number_of_hash_bits&#39;: 18}. 
<span class=" -Color -Color-Green">[I 2021-06-25 09:55:32,093]</span> Trial 11 finished with values: [0.8344199999999918, 0.0030727063179016114, 65.94140625] and parameters: {&#39;number_of_tables&#39;: 12, &#39;number_of_probes&#39;: 23, &#39;number_of_hash_bits&#39;: 12}. 
<span class=" -Color -Color-Green">[I 2021-06-25 09:55:51,691]</span> Trial 12 finished with values: [0.5932699999999993, 0.0005244213104248046, 871.046875] and parameters: {&#39;number_of_tables&#39;: 50, &#39;number_of_probes&#39;: 98, &#39;number_of_hash_bits&#39;: 22}. 
<span class=" -Color -Color-Green">[I 2021-06-25 09:56:40,319]</span> Trial 13 finished with values: [0.8821599999999884, 0.004491440415382385, 49.41015625] and parameters: {&#39;number_of_tables&#39;: 23, &#39;number_of_probes&#39;: 30, &#39;number_of_hash_bits&#39;: 12}. 
<span class=" -Color -Color-Green">[I 2021-06-25 09:57:01,455]</span> Trial 14 finished with values: [0.19335999999999487, 0.0001741429090499878, 983.36328125] and parameters: {&#39;number_of_tables&#39;: 18, &#39;number_of_probes&#39;: 36, &#39;number_of_hash_bits&#39;: 26}. 
<span class=" -Color -Color-Green">[I 2021-06-25 09:57:20,358]</span> Trial 15 finished with values: [0.707599999999994, 0.0007125402212142944, 214.0234375] and parameters: {&#39;number_of_tables&#39;: 44, &#39;number_of_probes&#39;: 78, &#39;number_of_hash_bits&#39;: 19}. 
<span class=" -Color -Color-Green">[I 2021-06-25 09:57:31,710]</span> Trial 16 finished with values: [0.6444599999999981, 0.000518613600730896, 107.1875] and parameters: {&#39;number_of_tables&#39;: 24, &#39;number_of_probes&#39;: 26, &#39;number_of_hash_bits&#39;: 17}. 
<span class=" -Color -Color-Green">[I 2021-06-25 09:57:44,463]</span> Trial 17 finished with values: [0.48950999999999933, 0.0003297036647796631, 385.5703125] and parameters: {&#39;number_of_tables&#39;: 34, &#39;number_of_probes&#39;: 41, &#39;number_of_hash_bits&#39;: 21}. 
<span class=" -Color -Color-Green">[I 2021-06-25 09:58:08,882]</span> Trial 18 finished with values: [0.20163999999999463, 0.0002264434814453125, 4530.6875] and parameters: {&#39;number_of_tables&#39;: 27, &#39;number_of_probes&#39;: 33, &#39;number_of_hash_bits&#39;: 26}. 
<span class=" -Color -Color-Green">[I 2021-06-25 09:58:17,121]</span> Trial 19 finished with values: [0.6207599999999976, 0.0006306215047836304, 31.37109375] and parameters: {&#39;number_of_tables&#39;: 10, &#39;number_of_probes&#39;: 10, &#39;number_of_hash_bits&#39;: 14}. 
<span class=" -Color -Color-Green">[I 2021-06-25 09:58:39,418]</span> Trial 20 finished with values: [0.836029999999988, 0.0014771086931228638, 142.6875] and parameters: {&#39;number_of_tables&#39;: 37, &#39;number_of_probes&#39;: 60, &#39;number_of_hash_bits&#39;: 16}. 
<span class=" -Color -Color-Green">[I 2021-06-25 09:58:49,057]</span> Trial 21 finished with values: [0.31789999999998875, 6.720409393310546e-05, 1113.5] and parameters: {&#39;number_of_tables&#39;: 27, &#39;number_of_probes&#39;: 27, &#39;number_of_hash_bits&#39;: 23}. 
<span class=" -Color -Color-Green">[I 2021-06-25 09:59:02,931]</span> Trial 22 finished with values: [0.6519799999999968, 0.0005425740003585815, -6.04296875] and parameters: {&#39;number_of_tables&#39;: 34, &#39;number_of_probes&#39;: 39, &#39;number_of_hash_bits&#39;: 18}. 
<span class=" -Color -Color-Green">[I 2021-06-25 09:59:15,396]</span> Trial 23 finished with values: [0.5873599999999999, 0.00040602688789367675, 180.46484375] and parameters: {&#39;number_of_tables&#39;: 32, &#39;number_of_probes&#39;: 39, &#39;number_of_hash_bits&#39;: 19}. 
<span class=" -Color -Color-Green">[I 2021-06-25 09:59:38,030]</span> Trial 24 finished with values: [0.8028599999999917, 0.0019501039743423463, 72.15234375] and parameters: {&#39;number_of_tables&#39;: 18, &#39;number_of_probes&#39;: 20, &#39;number_of_hash_bits&#39;: 13}. 
<span class=" -Color -Color-Green">[I 2021-06-25 09:59:50,894]</span> Trial 25 finished with values: [0.3029999999999891, 0.00024087769985198975, 2151.05859375] and parameters: {&#39;number_of_tables&#39;: 28, &#39;number_of_probes&#39;: 33, &#39;number_of_hash_bits&#39;: 24}. 
<span class=" -Color -Color-Green">[I 2021-06-25 10:00:08,143]</span> Trial 26 finished with values: [0.6477399999999978, 0.000591580080986023, -23.33203125] and parameters: {&#39;number_of_tables&#39;: 44, &#39;number_of_probes&#39;: 76, &#39;number_of_hash_bits&#39;: 20}. 
<span class=" -Color -Color-Green">[I 2021-06-25 10:00:22,703]</span> Trial 27 finished with values: [0.6663999999999942, 0.0004115888833999634, 163.83984375] and parameters: {&#39;number_of_tables&#39;: 40, &#39;number_of_probes&#39;: 40, &#39;number_of_hash_bits&#39;: 18}. 
<span class=" -Color -Color-Green">[I 2021-06-25 10:00:30,913]</span> Trial 28 finished with values: [0.36671999999999066, 0.00019075520038604735, 548.8046875] and parameters: {&#39;number_of_tables&#39;: 21, &#39;number_of_probes&#39;: 30, &#39;number_of_hash_bits&#39;: 22}. 
<span class=" -Color -Color-Green">[I 2021-06-25 10:00:40,481]</span> Trial 29 finished with values: [0.1628599999999968, 0.00011824250221252441, 1285.40625] and parameters: {&#39;number_of_tables&#39;: 15, &#39;number_of_probes&#39;: 18, &#39;number_of_hash_bits&#39;: 25}. 
<span class=" -Color -Color-Green">[I 2021-06-25 10:00:59,697]</span> Trial 30 finished with values: [0.7687999999999898, 0.0009250240087509156, -509.484375] and parameters: {&#39;number_of_tables&#39;: 38, &#39;number_of_probes&#39;: 52, &#39;number_of_hash_bits&#39;: 17}. 
<span class=" -Color -Color-Green">[I 2021-06-25 10:01:26,418]</span> Trial 31 finished with values: [0.8571899999999887, 0.0017379024267196656, 186.0] and parameters: {&#39;number_of_tables&#39;: 49, &#39;number_of_probes&#39;: 68, &#39;number_of_hash_bits&#39;: 16}. 
<span class=" -Color -Color-Green">[I 2021-06-25 10:01:56,910]</span> Trial 32 finished with values: [0.8790699999999879, 0.002172318410873413, 180.16796875] and parameters: {&#39;number_of_tables&#39;: 48, &#39;number_of_probes&#39;: 93, &#39;number_of_hash_bits&#39;: 16}. 
<span class=" -Color -Color-Green">[I 2021-06-25 10:02:17,265]</span> Trial 33 finished with values: [0.7831399999999917, 0.0010036286115646363, 187.47265625] and parameters: {&#39;number_of_tables&#39;: 46, &#39;number_of_probes&#39;: 54, &#39;number_of_hash_bits&#39;: 17}. 
<span class=" -Color -Color-Green">[I 2021-06-25 10:02:41,660]</span> Trial 34 finished with values: [0.8533199999999875, 0.0016864903926849366, 160.96875] and parameters: {&#39;number_of_tables&#39;: 41, &#39;number_of_probes&#39;: 70, &#39;number_of_hash_bits&#39;: 16}. 
<span class=" -Color -Color-Green">[I 2021-06-25 10:03:07,516]</span> Trial 35 finished with values: [0.8571399999999869, 0.0019451041221618653, 144.5546875] and parameters: {&#39;number_of_tables&#39;: 38, &#39;number_of_probes&#39;: 53, &#39;number_of_hash_bits&#39;: 15}. 
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span><span class=" -Color -Color-Green">[I 2021-06-25 10:04:02,085]</span> Trial 36 finished with values: [0.9164999999999814, 0.004724530696868896, 174.75] and parameters: {&#39;number_of_tables&#39;: 47, &#39;number_of_probes&#39;: 92, &#39;number_of_hash_bits&#39;: 14}. 
<span class=" -Color -Color-Green">[I 2021-06-25 10:04:17,020]</span> Trial 37 finished with values: [0.6072200000000012, 0.0004701333045959473, 310.43359375] and parameters: {&#39;number_of_tables&#39;: 42, &#39;number_of_probes&#39;: 58, &#39;number_of_hash_bits&#39;: 20}. 
<span class=" -Color -Color-Green">[I 2021-06-25 10:04:30,403]</span> Trial 38 finished with values: [0.659839999999997, 0.0005370358943939208, 139.46484375] and parameters: {&#39;number_of_tables&#39;: 32, &#39;number_of_probes&#39;: 43, &#39;number_of_hash_bits&#39;: 18}. 
<span class=" -Color -Color-Green">[I 2021-06-25 10:05:35,044]</span> Trial 39 finished with values: [0.9189799999999837, 0.005846633505821228, 144.9609375] and parameters: {&#39;number_of_tables&#39;: 38, &#39;number_of_probes&#39;: 74, &#39;number_of_hash_bits&#39;: 13}. 
<span class=" -Color -Color-Green">[I 2021-06-25 10:05:50,321]</span> Trial 40 finished with values: [0.6989299999999935, 0.0006446422576904297, 164.7109375] and parameters: {&#39;number_of_tables&#39;: 39, &#39;number_of_probes&#39;: 51, &#39;number_of_hash_bits&#39;: 18}. 
<span class=" -Color -Color-Green">[I 2021-06-25 10:06:08,516]</span> Trial 41 finished with values: [0.7875899999999928, 0.0010140242099761962, 147.38671875] and parameters: {&#39;number_of_tables&#39;: 35, &#39;number_of_probes&#39;: 63, &#39;number_of_hash_bits&#39;: 17}. 
<span class=" -Color -Color-Green">[I 2021-06-25 10:06:28,754]</span> Trial 42 finished with values: [0.8304999999999891, 0.0013773599863052368, 148.53125] and parameters: {&#39;number_of_tables&#39;: 36, &#39;number_of_probes&#39;: 57, &#39;number_of_hash_bits&#39;: 16}. 
<span class=" -Color -Color-Green">[I 2021-06-25 10:06:45,548]</span> Trial 43 finished with values: [0.6890799999999935, 0.0007108644008636475, 187.8828125] and parameters: {&#39;number_of_tables&#39;: 40, &#39;number_of_probes&#39;: 47, &#39;number_of_hash_bits&#39;: 18}. 
<span class=" -Color -Color-Green">[I 2021-06-25 10:07:04,714]</span> Trial 44 finished with values: [0.7220999999999944, 0.0007699584007263184, 236.6796875] and parameters: {&#39;number_of_tables&#39;: 45, &#39;number_of_probes&#39;: 86, &#39;number_of_hash_bits&#39;: 19}. 
<span class=" -Color -Color-Green">[I 2021-06-25 10:07:23,493]</span> Trial 45 finished with values: [0.7882699999999926, 0.001070155906677246, 138.0390625] and parameters: {&#39;number_of_tables&#39;: 33, &#39;number_of_probes&#39;: 66, &#39;number_of_hash_bits&#39;: 17}. 
<span class=" -Color -Color-Green">[I 2021-06-25 10:07:36,705]</span> Trial 46 finished with values: [0.7410799999999922, 0.0007573356151580811, 126.61328125] and parameters: {&#39;number_of_tables&#39;: 29, &#39;number_of_probes&#39;: 29, &#39;number_of_hash_bits&#39;: 16}. 
<span class=" -Color -Color-Green">[I 2021-06-25 10:07:52,772]</span> Trial 47 finished with values: [0.663209999999996, 0.000581502103805542, 213.94140625] and parameters: {&#39;number_of_tables&#39;: 39, &#39;number_of_probes&#39;: 60, &#39;number_of_hash_bits&#39;: 19}. 
<span class=" -Color -Color-Green">[I 2021-06-25 10:08:20,690]</span> Trial 48 finished with values: [0.855969999999988, 0.0020762288808822632, 150.03515625] and parameters: {&#39;number_of_tables&#39;: 41, &#39;number_of_probes&#39;: 50, &#39;number_of_hash_bits&#39;: 15}. 
<span class=" -Color -Color-Green">[I 2021-06-25 10:08:34,570]</span> Trial 49 finished with values: [0.6240999999999984, 0.0004833122968673706, 274.21484375] and parameters: {&#39;number_of_tables&#39;: 36, &#39;number_of_probes&#39;: 71, &#39;number_of_hash_bits&#39;: 20}. 
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Note: this cell takes about 20 minutes to run</span>
<span class="n">random_sampler</span> <span class="o">=</span> <span class="n">optuna</span><span class="o">.</span><span class="n">samplers</span><span class="o">.</span><span class="n">RandomSampler</span><span class="p">()</span>
<span class="n">random_experiment</span> <span class="o">=</span> <span class="n">optuna</span><span class="o">.</span><span class="n">create_study</span><span class="p">(</span><span class="n">sampler</span><span class="o">=</span><span class="n">random_sampler</span><span class="p">,</span> <span class="n">directions</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;maximize&quot;</span><span class="p">,</span> <span class="s2">&quot;minimize&quot;</span><span class="p">,</span> <span class="s2">&quot;minimize&quot;</span><span class="p">])</span>
<span class="n">random_experiment</span><span class="o">.</span><span class="n">optimize</span><span class="p">(</span><span class="n">train_lsh</span><span class="p">,</span> <span class="n">n_trials</span><span class="o">=</span><span class="n">N_TRIALS</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span><span class=" -Color -Color-Green">[I 2021-06-25 10:16:26,401]</span> A new study created in memory with name: no-name-fba75b23-62b5-45e2-8188-8631152733c7
<span class=" -Color -Color-Green">[I 2021-06-25 10:16:46,174]</span> Trial 0 finished with values: [0.7781999999999899, 0.0010494932889938355, 146.73046875] and parameters: {&#39;number_of_tables&#39;: 40, &#39;number_of_probes&#39;: 55, &#39;number_of_hash_bits&#39;: 17}. 
<span class=" -Color -Color-Green">[I 2021-06-25 10:17:21,762]</span> Trial 1 finished with values: [0.2764099999999897, 0.00046885650157928464, 5098.8984375] and parameters: {&#39;number_of_tables&#39;: 34, &#39;number_of_probes&#39;: 61, &#39;number_of_hash_bits&#39;: 26}. 
<span class=" -Color -Color-Green">[I 2021-06-25 10:17:46,242]</span> Trial 2 finished with values: [0.8394799999999893, 0.0018278647184371947, 114.30078125] and parameters: {&#39;number_of_tables&#39;: 34, &#39;number_of_probes&#39;: 45, &#39;number_of_hash_bits&#39;: 15}. 
<span class=" -Color -Color-Green">[I 2021-06-25 10:18:55,244]</span> Trial 3 finished with values: [0.9209299999999814, 0.0062559912919998165, 155.18359375] and parameters: {&#39;number_of_tables&#39;: 41, &#39;number_of_probes&#39;: 75, &#39;number_of_hash_bits&#39;: 13}. 
<span class=" -Color -Color-Green">[I 2021-06-25 10:19:12,738]</span> Trial 4 finished with values: [0.5966599999999979, 0.000511039400100708, 459.8046875] and parameters: {&#39;number_of_tables&#39;: 47, &#39;number_of_probes&#39;: 73, &#39;number_of_hash_bits&#39;: 21}. 
<span class=" -Color -Color-Green">[I 2021-06-25 10:20:12,425]</span> Trial 5 finished with values: [0.8983199999999877, 0.005467916393280029, 92.921875] and parameters: {&#39;number_of_tables&#39;: 28, &#39;number_of_probes&#39;: 36, &#39;number_of_hash_bits&#39;: 12}. 
<span class=" -Color -Color-Green">[I 2021-06-25 10:20:26,002]</span> Trial 6 finished with values: [0.737279999999992, 0.0011622004985809326, 31.28125] and parameters: {&#39;number_of_tables&#39;: 12, &#39;number_of_probes&#39;: 20, &#39;number_of_hash_bits&#39;: 14}. 
<span class=" -Color -Color-Green">[I 2021-06-25 10:20:40,540]</span> Trial 7 finished with values: [0.5234899999999997, 0.00038179941177368166, 448.53515625] and parameters: {&#39;number_of_tables&#39;: 40, &#39;number_of_probes&#39;: 48, &#39;number_of_hash_bits&#39;: 21}. 
<span class=" -Color -Color-Green">[I 2021-06-25 10:20:58,950]</span> Trial 8 finished with values: [0.7432799999999922, 0.0008508933067321778, 151.6171875] and parameters: {&#39;number_of_tables&#39;: 37, &#39;number_of_probes&#39;: 73, &#39;number_of_hash_bits&#39;: 18}. 
<span class=" -Color -Color-Green">[I 2021-06-25 10:21:14,577]</span> Trial 9 finished with values: [0.697359999999996, 0.0006478900194168091, 162.84375] and parameters: {&#39;number_of_tables&#39;: 35, &#39;number_of_probes&#39;: 53, &#39;number_of_hash_bits&#39;: 18}. 
<span class=" -Color -Color-Green">[I 2021-06-25 10:22:04,708]</span> Trial 10 finished with values: [0.8958599999999822, 0.004526280403137207, 111.49609375] and parameters: {&#39;number_of_tables&#39;: 28, &#39;number_of_probes&#39;: 49, &#39;number_of_hash_bits&#39;: 13}. 
<span class=" -Color -Color-Green">[I 2021-06-25 10:22:27,894]</span> Trial 11 finished with values: [0.7866999999999913, 0.0010530492782592773, 198.65625] and parameters: {&#39;number_of_tables&#39;: 50, &#39;number_of_probes&#39;: 93, &#39;number_of_hash_bits&#39;: 18}. 
<span class=" -Color -Color-Green">[I 2021-06-25 10:22:36,024]</span> Trial 12 finished with values: [0.13494999999999913, 9.886348247528076e-05, 1903.43359375] and parameters: {&#39;number_of_tables&#39;: 11, &#39;number_of_probes&#39;: 22, &#39;number_of_hash_bits&#39;: 26}. 
<span class=" -Color -Color-Green">[I 2021-06-25 10:22:44,446]</span> Trial 13 finished with values: [0.6394099999999963, 0.0005220484972000122, 46.1328125] and parameters: {&#39;number_of_tables&#39;: 13, &#39;number_of_probes&#39;: 20, &#39;number_of_hash_bits&#39;: 16}. 
<span class=" -Color -Color-Green">[I 2021-06-25 10:22:50,463]</span> Trial 14 finished with values: [0.3564299999999888, 0.00016739919185638428, 154.11328125] and parameters: {&#39;number_of_tables&#39;: 14, &#39;number_of_probes&#39;: 18, &#39;number_of_hash_bits&#39;: 20}. 
<span class=" -Color -Color-Green">[I 2021-06-25 10:23:24,463]</span> Trial 15 finished with values: [0.8769899999999843, 0.0028113796949386597, 119.81640625] and parameters: {&#39;number_of_tables&#39;: 36, &#39;number_of_probes&#39;: 48, &#39;number_of_hash_bits&#39;: 14}. 
<span class=" -Color -Color-Green">[I 2021-06-25 10:23:28,896]</span> Trial 16 finished with values: [0.5097599999999995, 0.0002581879615783691, 54.21875] and parameters: {&#39;number_of_tables&#39;: 10, &#39;number_of_probes&#39;: 10, &#39;number_of_hash_bits&#39;: 16}. 
<span class=" -Color -Color-Green">[I 2021-06-25 10:23:39,384]</span> Trial 17 finished with values: [0.18146999999999544, 0.0001444743871688843, 1684.9453125] and parameters: {&#39;number_of_tables&#39;: 19, &#39;number_of_probes&#39;: 20, &#39;number_of_hash_bits&#39;: 25}. 
<span class=" -Color -Color-Green">[I 2021-06-25 10:23:55,141]</span> Trial 18 finished with values: [0.42090999999999484, 0.000324407696723938, 901.9921875] and parameters: {&#39;number_of_tables&#39;: 37, &#39;number_of_probes&#39;: 49, &#39;number_of_hash_bits&#39;: 23}. 
<span class=" -Color -Color-Green">[I 2021-06-25 10:24:17,564]</span> Trial 19 finished with values: [0.8360499999999902, 0.001460992217063904, -7.48828125] and parameters: {&#39;number_of_tables&#39;: 40, &#39;number_of_probes&#39;: 58, &#39;number_of_hash_bits&#39;: 16}. 
<span class=" -Color -Color-Green">[I 2021-06-25 10:24:43,653]</span> Trial 20 finished with values: [0.8224599999999915, 0.0022998151063919068, 85.62109375] and parameters: {&#39;number_of_tables&#39;: 18, &#39;number_of_probes&#39;: 24, &#39;number_of_hash_bits&#39;: 13}. 
<span class=" -Color -Color-Green">[I 2021-06-25 10:25:31,059]</span> Trial 21 finished with values: [0.3131599999999899, 0.0007942313909530639, 4873.1328125] and parameters: {&#39;number_of_tables&#39;: 41, &#39;number_of_probes&#39;: 75, &#39;number_of_hash_bits&#39;: 26}. 
<span class=" -Color -Color-Green">[I 2021-06-25 10:26:01,954]</span> Trial 22 finished with values: [0.8479899999999876, 0.0026165685892105103, 80.04296875] and parameters: {&#39;number_of_tables&#39;: 25, &#39;number_of_probes&#39;: 26, &#39;number_of_hash_bits&#39;: 13}. 
<span class=" -Color -Color-Green">[I 2021-06-25 10:27:04,925]</span> Trial 23 finished with values: [0.2970399999999893, 0.0013944340944290162, 3904.8515625] and parameters: {&#39;number_of_tables&#39;: 48, &#39;number_of_probes&#39;: 61, &#39;number_of_hash_bits&#39;: 26}. 
<span class=" -Color -Color-Green">[I 2021-06-25 10:28:10,905]</span> Trial 24 finished with values: [0.9163099999999842, 0.005951592612266541, 123.68359375] and parameters: {&#39;number_of_tables&#39;: 38, &#39;number_of_probes&#39;: 68, &#39;number_of_hash_bits&#39;: 13}. 
<span class=" -Color -Color-Green">[I 2021-06-25 10:28:27,555]</span> Trial 25 finished with values: [0.7608999999999929, 0.0013949713230133056, 36.1015625] and parameters: {&#39;number_of_tables&#39;: 13, &#39;number_of_probes&#39;: 23, &#39;number_of_hash_bits&#39;: 14}. 
<span class=" -Color -Color-Green">[I 2021-06-25 10:28:31,974]</span> Trial 26 finished with values: [0.34653999999998847, 0.00015223381519317627, 141.76953125] and parameters: {&#39;number_of_tables&#39;: 11, &#39;number_of_probes&#39;: 19, &#39;number_of_hash_bits&#39;: 20}. 
<span class=" -Color -Color-Green">[I 2021-06-25 10:28:49,862]</span> Trial 27 finished with values: [0.695909999999994, 0.0006918719053268433, 218.96875] and parameters: {&#39;number_of_tables&#39;: 45, &#39;number_of_probes&#39;: 71, &#39;number_of_hash_bits&#39;: 19}. 
<span class=" -Color -Color-Green">[I 2021-06-25 10:29:05,744]</span> Trial 28 finished with values: [0.7177399999999956, 0.0005888368844985962, 164.8359375] and parameters: {&#39;number_of_tables&#39;: 37, &#39;number_of_probes&#39;: 60, &#39;number_of_hash_bits&#39;: 18}. 
<span class=" -Color -Color-Green">[I 2021-06-25 10:29:12,651]</span> Trial 29 finished with values: [0.37893999999999206, 0.00018789780139923096, 254.7265625] and parameters: {&#39;number_of_tables&#39;: 17, &#39;number_of_probes&#39;: 27, &#39;number_of_hash_bits&#39;: 21}. 
<span class=" -Color -Color-Green">[I 2021-06-25 10:29:47,177]</span> Trial 30 finished with values: [0.8729599999999864, 0.0027358051776885987, 123.08984375] and parameters: {&#39;number_of_tables&#39;: 39, &#39;number_of_probes&#39;: 44, &#39;number_of_hash_bits&#39;: 14}. 
<span class=" -Color -Color-Green">[I 2021-06-25 10:29:53,145]</span> Trial 31 finished with values: [0.25388999999999207, 0.00011690478324890137, 418.47265625] and parameters: {&#39;number_of_tables&#39;: 13, &#39;number_of_probes&#39;: 16, &#39;number_of_hash_bits&#39;: 22}. 
<span class=" -Color -Color-Green">[I 2021-06-25 10:29:57,587]</span> Trial 32 finished with values: [0.3310999999999878, 0.00014544570446014404, 70.640625] and parameters: {&#39;number_of_tables&#39;: 11, &#39;number_of_probes&#39;: 17, &#39;number_of_hash_bits&#39;: 20}. 
<span class=" -Color -Color-Green">[I 2021-06-25 10:30:15,378]</span> Trial 33 finished with values: [0.39047999999999133, 0.0003522584915161133, 2557.18359375] and parameters: {&#39;number_of_tables&#39;: 37, &#39;number_of_probes&#39;: 56, &#39;number_of_hash_bits&#39;: 24}. 
<span class=" -Color -Color-Green">[I 2021-06-25 10:30:34,952]</span> Trial 34 finished with values: [0.7778799999999925, 0.0016688410997390748, -269.765625] and parameters: {&#39;number_of_tables&#39;: 14, &#39;number_of_probes&#39;: 18, &#39;number_of_hash_bits&#39;: 13}. 
<span class=" -Color -Color-Green">[I 2021-06-25 10:30:39,133]</span> Trial 35 finished with values: [0.2727299999999904, 0.00011070821285247803, 208.33984375] and parameters: {&#39;number_of_tables&#39;: 11, &#39;number_of_probes&#39;: 15, &#39;number_of_hash_bits&#39;: 21}. 
<span class=" -Color -Color-Green">[I 2021-06-25 10:30:48,154]</span> Trial 36 finished with values: [0.1564799999999973, 0.00010971610546112061, 1225.671875] and parameters: {&#39;number_of_tables&#39;: 14, &#39;number_of_probes&#39;: 17, &#39;number_of_hash_bits&#39;: 25}. 
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span><span class=" -Color -Color-Green">[I 2021-06-25 10:31:32,167]</span> Trial 37 finished with values: [0.8966099999999849, 0.003543233299255371, -490.703125] and parameters: {&#39;number_of_tables&#39;: 49, &#39;number_of_probes&#39;: 58, &#39;number_of_hash_bits&#39;: 14}. 
<span class=" -Color -Color-Green">[I 2021-06-25 10:31:54,924]</span> Trial 38 finished with values: [0.8315899999999902, 0.001818433904647827, 110.1484375] and parameters: {&#39;number_of_tables&#39;: 25, &#39;number_of_probes&#39;: 47, &#39;number_of_hash_bits&#39;: 15}. 
<span class=" -Color -Color-Green">[I 2021-06-25 10:32:14,844]</span> Trial 39 finished with values: [0.20976999999999385, 0.00020131540298461915, 3590.08984375] and parameters: {&#39;number_of_tables&#39;: 21, &#39;number_of_probes&#39;: 40, &#39;number_of_hash_bits&#39;: 26}. 
<span class=" -Color -Color-Green">[I 2021-06-25 10:32:21,519]</span> Trial 40 finished with values: [0.21414999999999368, 0.00012017819881439209, 779.625] and parameters: {&#39;number_of_tables&#39;: 14, &#39;number_of_probes&#39;: 15, &#39;number_of_hash_bits&#39;: 23}. 
<span class=" -Color -Color-Green">[I 2021-06-25 10:32:28,350]</span> Trial 41 finished with values: [0.4030499999999927, 0.0001993230104446411, -12.57421875] and parameters: {&#39;number_of_tables&#39;: 14, &#39;number_of_probes&#39;: 25, &#39;number_of_hash_bits&#39;: 20}. 
<span class=" -Color -Color-Green">[I 2021-06-25 10:32:47,883]</span> Trial 42 finished with values: [0.4036199999999926, 0.00037379658222198484, 2568.578125] and parameters: {&#39;number_of_tables&#39;: 40, &#39;number_of_probes&#39;: 59, &#39;number_of_hash_bits&#39;: 24}. 
<span class=" -Color -Color-Green">[I 2021-06-25 10:33:10,394]</span> Trial 43 finished with values: [0.8240399999999896, 0.001932988214492798, -252.359375] and parameters: {&#39;number_of_tables&#39;: 18, &#39;number_of_probes&#39;: 35, &#39;number_of_hash_bits&#39;: 14}. 
<span class=" -Color -Color-Green">[I 2021-06-25 10:33:21,861]</span> Trial 44 finished with values: [0.21251999999999385, 0.0001655750274658203, 1772.51953125] and parameters: {&#39;number_of_tables&#39;: 20, &#39;number_of_probes&#39;: 27, &#39;number_of_hash_bits&#39;: 25}. 
<span class=" -Color -Color-Green">[I 2021-06-25 10:33:38,161]</span> Trial 45 finished with values: [0.2849199999999896, 0.0002587076187133789, 1709.50390625] and parameters: {&#39;number_of_tables&#39;: 27, &#39;number_of_probes&#39;: 45, &#39;number_of_hash_bits&#39;: 25}. 
<span class=" -Color -Color-Green">[I 2021-06-25 10:34:52,425]</span> Trial 46 finished with values: [0.9225899999999821, 0.006703413605690003, -513.4140625] and parameters: {&#39;number_of_tables&#39;: 42, &#39;number_of_probes&#39;: 79, &#39;number_of_hash_bits&#39;: 13}. 
<span class=" -Color -Color-Green">[I 2021-06-25 10:35:13,134]</span> Trial 47 finished with values: [0.7708299999999901, 0.0009955607891082763, 170.828125] and parameters: {&#39;number_of_tables&#39;: 43, &#39;number_of_probes&#39;: 86, &#39;number_of_hash_bits&#39;: 18}. 
<span class=" -Color -Color-Green">[I 2021-06-25 10:35:48,713]</span> Trial 48 finished with values: [0.8452599999999911, 0.0032960205078125, 68.84375] and parameters: {&#39;number_of_tables&#39;: 13, &#39;number_of_probes&#39;: 25, &#39;number_of_hash_bits&#39;: 12}. 
<span class=" -Color -Color-Green">[I 2021-06-25 10:37:04,602]</span> Trial 49 finished with values: [0.3326099999999899, 0.0020538235902786257, 2185.73046875] and parameters: {&#39;number_of_tables&#39;: 50, &#39;number_of_probes&#39;: 79, &#39;number_of_hash_bits&#39;: 26}. 
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">lsh_motpe_df</span> <span class="o">=</span> <span class="n">cast_as_df</span><span class="p">(</span><span class="n">motpe_experiment</span><span class="p">,</span> <span class="n">experiment_type</span><span class="o">=</span><span class="s2">&quot;MOTPE&quot;</span><span class="p">,</span> <span class="n">algorithm_name</span><span class="o">=</span><span class="s2">&quot;LSH&quot;</span><span class="p">)</span>
<span class="n">lsh_random_df</span> <span class="o">=</span> <span class="n">cast_as_df</span><span class="p">(</span><span class="n">random_experiment</span><span class="p">,</span> <span class="n">experiment_type</span><span class="o">=</span><span class="s2">&quot;Random&quot;</span><span class="p">,</span> <span class="n">algorithm_name</span><span class="o">=</span><span class="s2">&quot;LSH&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">three_panel_fig</span><span class="p">([</span><span class="n">lsh_motpe_df</span><span class="p">,</span> <span class="n">lsh_random_df</span><span class="p">])</span><span class="c1">#</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/T892775_Exploring_Multi_Objective_Hyperparameter_Optimization_187_0.png" src="_images/T892775_Exploring_Multi_Objective_Hyperparameter_Optimization_187_0.png" />
</div>
</div>
</div>
<div class="section" id="id14">
<h3>Discussion time<a class="headerlink" href="#id14" title="Permalink to this headline">¶</a></h3>
<p>These figures look much the same as previous experiments: both MOTPE and random sampling more or less trace out the same region of objective space. Again, we find a few points that seem to dominate the memory space but again we caution that this is likely due to the nature in which we measure memory (in fact, we know some of these measurements are completely wrong because they’re negative!). We use the <code class="docutils literal notranslate"><span class="pre">psutil</span></code> library and pass it the process PID to measure the memory usage before and after building the index. While this works well enough for Annoy and HNSW, we suspect this LSH implementation does not always fully release memory upon a new iteration of a trial during the optimization run. This would cause the next pass through the training loop to still register memory from the previous loop, thus resulting in an incorrect measurement. We were unable to track down the exact cause – if we figure it out, we’ll fix it! (And if you spot the problem, let us know!)</p>
<p><center><img src='https://github.com/sparsh-ai/multiobjective-optimizations/blob/main/docs/_images/T892775_13.png?raw=1'></center></p><p>To be consistent, we again ran this experiment for many more trials (150 this time). While we can see Pareto frontier is very densely sampled by the MOBO strategy, random search still does a fine job of covering the space.</p>
</div>
<div class="section" id="optimizing-anns-without-mobo">
<h3>Optimizing ANNs without MOBO<a class="headerlink" href="#optimizing-anns-without-mobo" title="Permalink to this headline">¶</a></h3>
<p>We used ANNs to demonstrate a real-world, complex, data-intensive application with realistic multi-objective goals. This allowed us to explore the nuances of multi-objective optimization on more than a toy problem. But we ultimately discovered that even in such a setting, multi-objective Bayesian optimization does not typically perform significantly better than random sampling.  However, if you are building an ANN application (rather than reading along for a detailed look at MOBO) then we can do even better than random sampling.</p>
<p>In real ANN applications, often the memory budget is already known (or is cheap enough that additional resources can be easily procured). If you already know your memory budget, then there is no need to optimize over it. For most ANN algorithms, the memory budget can be set to your specifications with a little fiddling (think <code class="docutils literal notranslate"><span class="pre">M</span></code> in the case of HNSW, or <code class="docutils literal notranslate"><span class="pre">number_of_hash_tables</span></code> in LSH). Once set, perform a simple sweep over the hyperparameter that controls the latency-recall tradeoff (<code class="docutils literal notranslate"><span class="pre">search_k</span></code> in Annoy, <code class="docutils literal notranslate"><span class="pre">ef</span></code> in HNSW, <code class="docutils literal notranslate"><span class="pre">number_of_probes</span></code> in LSH). And voila! This hyperparameter sweep will inherently trace out a Pareto frontier in Recall/Query time space. However, if you do not know <em>a priori</em> what memory budget you are willing to allot for your application, the next best thing would be to perform a random sampling of the hyperparameters of your algorithm to get a feel for the trade-offs involved.</p>
<p>Another interesting axis we didn’t dig into in this post is an ANN algorithm’s <em>build time</em>. We looked closely at <em>query time</em>, which is typically more important. In this section, we artifically set the range of hyperparameter values such that cells won’t take hours or <em>days</em> to run (resulting in some rather low values of Recall, if you noticed). However, in a real application, building the search index for one of these algorithms can take a considerable amount of time when more appropriate HPs are employed and even larger datasets are used. This matters if an application will require <em>rebuilding</em> on a regular cadence. While some algorithms allow insertion of new datapoints, better results are often achieved by occasionally rebuilding the index, especially if many new items are to be added.</p>
<p>A good example is tweets. More than 500 million new tweets are added to Twitter each day. Simple insertion of these new tweets to an existing index may be suboptimal. In which case, a regular rebuilding cadence should be established and the build time for such an algorithm becomes important. Therefore, some applications may require a careful analysis of the trade-off of build time with recall, query time, and memory footprint – a 4D optimization problem! However, as with the memory footprint, an acceptable build time budget may already be known <em>a priori</em> and hyperparameters can be set such that they meet memory and build time requirements. Then, simply sweep over the latency-recall trade-off hyperparameter to identify the Pareto frontier between predictive performance and inference time.</p>
</div>
</div>
<div class="section" id="the-big-take-away">
<h2>The big take-away<a class="headerlink" href="#the-big-take-away" title="Permalink to this headline">¶</a></h2>
<p>We applied multi-objective Bayesian optimization to three Approximate Nearest Neighbor algorithms and found that the results weren’t always much better than using a random sampling. And this is not altogether unsurprising. In a <a class="reference external" href="https://arxiv.org/pdf/2012.03826.pdf">recent paper</a>, authors demonstrated that many MOBO algorithms only perform HPO a few percent better (in terms of optimizing an appropriate objective like accuracy) than random sampling on a wide range of machine learning experiments.</p>
<p>While its still unclear when MOBO might provide significant gains over random sampling, one clue may be in the relationship of the hyperparameters to the objective space. Algorithms like neural networks have many parameters with complex and non-linear relationships between them and the objective (such as accuracy or latency). The myriad hyperparameter combinations could very well map to a vast space of possible (accuracy, latency) outputs (like on the right). However, this might not hold true for all algorithms, where the actual possible output space is constrained (like on the left).</p>
<p><center><img src='https://github.com/sparsh-ai/multiobjective-optimizations/blob/main/docs/_images/T892775_14.png?raw=1'></center></p><p>We speculate that MOBO optimization techniques stand a chance of performing better when the output space is like that on the right. The sophisticated mechanisms in these techniques should be better able to identify more ideal points in the output space over a random algorithm which samples, well… randomly! But when the output space is constrained, both MOBO and random search have “less room” to sample, so random performs about as well as these more sophisticated techniques.</p>
<p><center><img src='https://github.com/sparsh-ai/multiobjective-optimizations/blob/main/docs/_images/T892775_15.png?raw=1'></center></p><p>The hyperparameters of many approximate nearest neighbor algorithms may provide their own constraints, because they often control a single objective. For example, <code class="docutils literal notranslate"><span class="pre">M</span></code> in the HNSW algorithm is almost entirely responsible for the memory footprint. This means that, regardless of the value of other hyperparmeters, the memory output is more or less fixed, constraining the possible outputs in the objective space. The same is true for the hyperparameters that control the accuracy-latency trade-off. Thus, when we plot these results we see both random and MOBO algorithms perform similarly because there simply isn’t that much output space to explore.</p>
<p>The ultimate lesson from our experiments in this and our previous sections is that random sampling is probably good enough for most applications. Using more sophisticated optimization techniques may provide added benefit when the relationship between your hyperparameters and the objective space is murky, or when you have a large compute budget. The longer these experiments are run, the more likely the MOBO algorithm is to find marginally better solutions and sometimes even a marginal improvement is worth the computational cost.</p>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "sparsh-ai/multiobjective-optimizations",
            ref: "main",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
            



<div class='prev-next-bottom'>
    
    <div id="prev">
        <a class="left-prev" href="T145475_Multi_objective_Optimization_using_Pymoo_Library.html" title="previous page">
            <i class="prevnext-label fas fa-angle-left"></i>
            <div class="prevnext-info">
                <p class="prevnext-label">previous</p>
                <p class="prevnext-title">Multi-objective Optimization using Pymoo Library</p>
            </div>
        </a>
    </div>
     <div id="next">
        <a class="right-next" href="T660394_Implicit_Hybrid_Movie_Recommender_using_Collie_Library.html" title="next page">
            <div class="prevnext-info">
                <p class="prevnext-label">next</p>
                <p class="prevnext-title">Implicit Hybrid Movie Recommender using Collie Library</p>
            </div>
            <i class="prevnext-label fas fa-angle-right"></i>
        </a>
     </div>

</div>
        
        </div>
    </div>
    <footer class="footer">
    <div class="container">
      <p>
        
          By Sparsh A.<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
  </body>
</html>